{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJMeoV8nxVxU"
      },
      "source": [
        "# Phase => 1 : Dataset preparation and clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWslGADt2d3B"
      },
      "source": [
        "Phase 1.1 => Feature Selector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBP6PAN3xNVe",
        "outputId": "7a922d30-4b93-4b62-beb1-034af2f6de62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/FeatureSelector.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/FeatureSelector.py\n",
        "# FeatureSelector.py\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "class FeatureSelector:\n",
        "    def __init__(self, cat_max_unique=20):\n",
        "        self.cat_max_unique = cat_max_unique\n",
        "        self.selected_features = []\n",
        "        self.dropped_features = []\n",
        "        self.encoders = {}\n",
        "\n",
        "    def fit(self, df: pd.DataFrame):\n",
        "        \"\"\"Decide which columns to keep/drop and build encoders for categoricals.\"\"\"\n",
        "        self.selected_features.clear()\n",
        "        self.dropped_features.clear()\n",
        "        self.encoders.clear()\n",
        "\n",
        "        for col in df.columns:\n",
        "            lower = col.lower()\n",
        "\n",
        "            # Drop IDs, dates, names\n",
        "            if \"id\" in lower or \"date\" in lower or \"name\" in lower:\n",
        "                self.dropped_features.append(col)\n",
        "                continue\n",
        "\n",
        "            # Categorical\n",
        "            if df[col].dtype == \"object\" or df[col].dtype.name == \"category\":\n",
        "                nunq = df[col].nunique(dropna=True)\n",
        "                if nunq > 0.5 * len(df) or nunq > self.cat_max_unique:\n",
        "                    self.dropped_features.append(col)\n",
        "                else:\n",
        "                    self.selected_features.append(col)\n",
        "                    self.encoders[col] = df[col].dropna().unique().tolist()\n",
        "                continue\n",
        "\n",
        "            # Numeric\n",
        "            if pd.api.types.is_numeric_dtype(df[col]):\n",
        "                self.selected_features.append(col)\n",
        "                continue\n",
        "\n",
        "            # Otherwise drop\n",
        "            self.dropped_features.append(col)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, df: pd.DataFrame):\n",
        "        \"\"\"Apply selection + one-hot encoding to dataset.\"\"\"\n",
        "        df_sel = df[self.selected_features].copy()\n",
        "\n",
        "        # Expand categoricals\n",
        "        for col, vals in self.encoders.items():\n",
        "            for v in vals:\n",
        "                df_sel[f\"{col}_{v}\"] = (df_sel[col] == v).astype(int)\n",
        "            df_sel.drop(columns=[col], inplace=True)\n",
        "\n",
        "        return df_sel\n",
        "\n",
        "    def fit_transform(self, df: pd.DataFrame):\n",
        "        return self.fit(df).transform(df)\n",
        "\n",
        "    def get_selected_features(self):\n",
        "        return list(self.selected_features)\n",
        "\n",
        "    def get_dropped_features(self):\n",
        "        return list(self.dropped_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dEm4Ln9jiDX"
      },
      "source": [
        "Phase 1.2 => DiffcultyScore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PaAZSFtXw4f",
        "outputId": "974cbdef-a0ec-4a4d-dd56-283d215033c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/DifficultyScorer.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/DifficultyScorer.py\n",
        "# DifficultyScorer.py\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "class DifficultyScorer:\n",
        "    def __init__(self,\n",
        "                 weights=None,\n",
        "                 threshold=0.5,  # FIXED: Was 0.75 → Now 0.5\n",
        "                 important_features=None,\n",
        "                 domain_factor=0.0):\n",
        "        \"\"\"\n",
        "        Computes row-wise difficulty scores + tree_type for a DataFrame.\n",
        "        \"\"\"\n",
        "        self.weights = weights or {\n",
        "            \"keyword\":   0.4,\n",
        "            \"length\":    0.3,\n",
        "            \"ambiguity\": 0.2,\n",
        "            \"domain\":    0.1\n",
        "        }\n",
        "        self.threshold = threshold\n",
        "        self.important_features = important_features or []\n",
        "        self.domain_factor = domain_factor\n",
        "\n",
        "    def fit(self, df: pd.DataFrame, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, df: pd.DataFrame):\n",
        "        records = []\n",
        "        for _, row in df.iterrows():\n",
        "            feats = row.to_dict()\n",
        "            n_total = len(feats)\n",
        "            if n_total == 0:\n",
        "                records.append({\n",
        "                    \"difficulty_score\": 0.0,\n",
        "                    \"tree_type\": \"binary\"\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            def is_filled(x):\n",
        "                return pd.notna(x) and x != \"\"\n",
        "\n",
        "            n_filled = sum(1 for v in feats.values() if is_filled(v))\n",
        "            n_missing = n_total - n_filled\n",
        "\n",
        "            # Keyword: count Vata/Pitta/Kapha mentions\n",
        "            n_keyword = sum(\n",
        "                1 for v in feats.values()\n",
        "                if isinstance(v, str) and any(k in v.lower() for k in [\"vata\", \"pitta\", \"kapha\", \"dosha\"])\n",
        "            )\n",
        "\n",
        "            # Length: long trait descriptions\n",
        "            n_length = sum(\n",
        "                1 for v in feats.values()\n",
        "                if isinstance(v, str) and len(v) > 15\n",
        "            )\n",
        "\n",
        "            # Ambiguity: \"?\" or \"or\"\n",
        "            n_ambiguous = sum(\n",
        "                1 for v in feats.values()\n",
        "                if isinstance(v, str) and (\"?\" in v or \" or \" in v.lower())\n",
        "            )\n",
        "\n",
        "            # Domain: important features filled\n",
        "            if self.important_features:\n",
        "                n_domain = sum(\n",
        "                    1 for k in self.important_features\n",
        "                    if k in feats and is_filled(feats[k])\n",
        "                )\n",
        "                K = n_domain / len(self.important_features)\n",
        "            else:\n",
        "                K = n_keyword / n_total if n_total > 0 else 0\n",
        "\n",
        "            L = n_length / n_total if n_total > 0 else 0\n",
        "            A = n_ambiguous / n_total if n_total > 0 else 0\n",
        "            D = self.domain_factor\n",
        "\n",
        "            w = self.weights\n",
        "            score = (\n",
        "                w[\"keyword\"]   * K +\n",
        "                w[\"length\"]    * L +\n",
        "                w[\"ambiguity\"] * A +\n",
        "                w[\"domain\"]    * D\n",
        "            )\n",
        "\n",
        "            tree = \"three-tree\" if score > self.threshold else \"binary\"\n",
        "            records.append({\n",
        "                \"difficulty_score\": round(score, 4),\n",
        "                \"tree_type\": tree\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(records, index=df.index)\n",
        "\n",
        "    def fit_transform(self, df: pd.DataFrame, y=None):\n",
        "        return self.fit(df, y).transform(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd7HLliAjnrP"
      },
      "source": [
        "Phase 3 => Integration Phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNe7d32tgBVW",
        "outputId": "73fbfa5e-5a7e-4377-94b9-ba64452c5850"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/Phase1Pipeline.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/Phase1Pipeline.py\n",
        "\n",
        "import pandas as pd\n",
        "from FeatureSelector import FeatureSelector\n",
        "from DifficultyScorer import DifficultyScorer # Corrected import\n",
        "\n",
        "class Phase1Pipeline:\n",
        "    def __init__(self,\n",
        "                 cat_max_unique=20,\n",
        "                 threshold=0.75,\n",
        "                 important_features=None,\n",
        "                 domain_factor=0.0):\n",
        "        self.selector = FeatureSelector(cat_max_unique=cat_max_unique)\n",
        "        self.scorer   = DifficultyScorer(\n",
        "            threshold=threshold,\n",
        "            important_features=important_features,\n",
        "            domain_factor=domain_factor\n",
        "        )\n",
        "\n",
        "    def fit(self, df: pd.DataFrame):\n",
        "        \"\"\"Fit selector on raw df, then scorer on selected features.\"\"\"\n",
        "        self.selector.fit(df)\n",
        "        selected = self.selector.transform(df)\n",
        "        self.scorer.fit(selected)\n",
        "        return self\n",
        "\n",
        "    def transform(self, df: pd.DataFrame):\n",
        "        \"\"\"Run selection + scoring and merge into one DataFrame.\"\"\"\n",
        "        X_sel = self.selector.transform(df)\n",
        "        scores = self.scorer.transform(X_sel)\n",
        "        return pd.concat(\n",
        "            [X_sel.reset_index(drop=True),\n",
        "             scores.reset_index(drop=True)],\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "    def fit_transform(self, df: pd.DataFrame):\n",
        "        return self.fit(df).transform(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDl5kvukjz_z"
      },
      "source": [
        "# Phase => 2 : Tree Construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvI8m4phj42-",
        "outputId": "f7385a21-c65e-4cbf-f8f0-3fc73561ba16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/TreeNodeV1.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/TreeNodeV1.py\n",
        "import math\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class TreeNodeV1:\n",
        "    def __init__(self,\n",
        "                 node_id=None,\n",
        "                 value=None,\n",
        "                 children=None,\n",
        "                 level: int = 0,\n",
        "                 rule=None,\n",
        "                 confidence: float = 0.0,\n",
        "                 lock_flag: str = \"unlocked\",\n",
        "                 provenance: str = None,\n",
        "                 difficulty_tag: str = None,\n",
        "                 branch_tag: str = None):\n",
        "        self.node_id = node_id\n",
        "        self.value = value\n",
        "        self.children = children or []\n",
        "        self.level = max(0, level)\n",
        "        self.rule = rule\n",
        "        self.confidence = confidence\n",
        "        self.lock_flag = lock_flag\n",
        "        self.updated_at = datetime.utcnow()\n",
        "        self.provenance = provenance\n",
        "        self.conflicts = []\n",
        "        self.difficulty_tag = difficulty_tag\n",
        "        self.branch_tag = branch_tag\n",
        "        self.cached_vector = None\n",
        "\n",
        "    def is_leaf(self) -> bool:\n",
        "        return not self.children\n",
        "\n",
        "    def add_child(self, node):\n",
        "        if node is not None:\n",
        "            self.children.append(node)\n",
        "            self.updated_at = datetime.utcnow()\n",
        "\n",
        "    def store_vector(self, vec):\n",
        "        self.cached_vector = vec\n",
        "        self.updated_at = datetime.utcnow()\n",
        "\n",
        "    def get_vector(self):\n",
        "        return self.cached_vector\n",
        "\n",
        "    def get_depth(self) -> int:\n",
        "        if self.is_leaf():\n",
        "            return 1\n",
        "        depths = [child.get_depth() for child in self.children if child is not None]\n",
        "        return 1 + max(depths) if depths else 1\n",
        "\n",
        "    def get_confidence(self) -> float:\n",
        "        depth = self.get_depth()\n",
        "        self.confidence = min(1.0, math.log(depth + 1) / 5)\n",
        "        return self.confidence\n",
        "\n",
        "    def lock(self, mode=\"soft\"):\n",
        "        self.lock_flag = mode\n",
        "        self.updated_at = datetime.utcnow()\n",
        "\n",
        "    def unlock(self):\n",
        "        self.lock_flag = \"unlocked\"\n",
        "        self.updated_at = datetime.utcnow()\n",
        "\n",
        "    def add_conflict(self, conflict_note: str):\n",
        "        self.conflicts.append(conflict_note)\n",
        "        self.updated_at = datetime.utcnow()\n",
        "\n",
        "    # --- NEW: GET ALL LEAF VALUES ---\n",
        "    def get_leaves(self) -> list:\n",
        "        \"\"\"Return list of leaf node values (strings).\"\"\"\n",
        "        leaves = []\n",
        "        if self.is_leaf():\n",
        "            if self.value and self.value != \"root\":\n",
        "                leaves.append(self.value)\n",
        "        else:\n",
        "            for child in self.children:\n",
        "                leaves.extend(child.get_leaves())\n",
        "        return leaves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SyV2QpiYgnP",
        "outputId": "985456fe-b4f0-4c3a-fe51-9f4385ddb29d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/TreeBuilderV2.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/TreeBuilderV2.py\n",
        "import logging\n",
        "import torch\n",
        "from collections import deque\n",
        "from typing import List, Tuple, Optional, Union\n",
        "from TreeNodeV1 import TreeNodeV1\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class TreeBuilderV2:\n",
        "    def __init__(self, device: str = \"cpu\", dim: int = 50, mode: str = \"binary\"):\n",
        "        \"\"\"\n",
        "        mode: 'binary' or 'three'\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        self.dim = dim\n",
        "        self.mode = mode\n",
        "        self.max_warnings = 5\n",
        "        self.failed_samples = set()\n",
        "\n",
        "    def build_tree(self, vec_pairs: List[Tuple[str, Optional[torch.Tensor]]], sample_id=\"unknown\") -> Optional[TreeNodeV1]:\n",
        "        \"\"\"Build a binary or three-tree from token-vector pairs.\"\"\"\n",
        "        if not vec_pairs:\n",
        "            logger.warning(f\"TreeBuilderV2 failed for {sample_id}: Empty vec_pairs\")\n",
        "            return None\n",
        "\n",
        "        # Root node\n",
        "        root = TreeNodeV1(node_id=sample_id, value=\"root\", level=0)\n",
        "\n",
        "        # BFS queue\n",
        "        queue = deque([(root, vec_pairs)])\n",
        "\n",
        "        while queue:\n",
        "            parent, pairs = queue.popleft()\n",
        "\n",
        "            # Decide branching factor\n",
        "            if self.mode == \"binary\":\n",
        "                chunk_size = 2\n",
        "                branch_tags = [\"L\", \"R\"]\n",
        "            else:  # three-tree\n",
        "                chunk_size = 3\n",
        "                branch_tags = [\"L\", \"M\", \"R\"]\n",
        "\n",
        "            # Split pairs into chunks\n",
        "            for i in range(0, len(pairs), chunk_size):\n",
        "                chunk = pairs[i:i+chunk_size]\n",
        "                for j, (token, vector) in enumerate(chunk):\n",
        "                    if vector is None or (vector.shape[0] != self.dim):\n",
        "                        logger.warning(f\"Skipping invalid vector for token {token}\")\n",
        "                        continue\n",
        "\n",
        "                    child = TreeNodeV1(\n",
        "                        node_id=f\"{sample_id}_{i}_{j}\",\n",
        "                        value=token,\n",
        "                        level=parent.level + 1,\n",
        "                        branch_tag=branch_tags[j % len(branch_tags)]\n",
        "                    )\n",
        "                    child.store_vector(vector.to(self.device))\n",
        "                    parent.add_child(child)\n",
        "\n",
        "                    # For now, stop at one level (can extend deeper with rules)\n",
        "                    # queue.append((child, next_pairs))  # placeholder for deeper BFS\n",
        "\n",
        "        # Compute root vector as mean of children\n",
        "        if root.children:\n",
        "            child_vectors = [c.get_vector() for c in root.children if c.get_vector() is not None]\n",
        "            if child_vectors:\n",
        "                root_vector = torch.stack(child_vectors).mean(dim=0)\n",
        "                root.store_vector(root_vector)\n",
        "\n",
        "        return root\n",
        "\n",
        "    # --- DFS tracing for explanations ---\n",
        "    def trace_dfs(self, node: TreeNodeV1, path=None) -> List[str]:\n",
        "        \"\"\"Return a list of node values in DFS order.\"\"\"\n",
        "        if path is None:\n",
        "            path = []\n",
        "        path.append(node.value)\n",
        "        for child in node.children:\n",
        "            self.trace_dfs(child, path)\n",
        "        return path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPaINPycYg2J",
        "outputId": "8ae43d5c-0c37-4540-d31b-d7b658f6acd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/LockManager.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/LockManager.py\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from TreeNodeV1 import TreeNodeV1\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class LockManager:\n",
        "    def __init__(self):\n",
        "        self.audit_log = []  # list of dicts with {node_id, action, mode, timestamp}\n",
        "\n",
        "    def lock_node(self, node: TreeNodeV1, mode=\"soft\"):\n",
        "        \"\"\"Lock a node (soft/hard).\"\"\"\n",
        "        if node.lock_flag == \"hard\":\n",
        "            logger.warning(f\"Node {node.node_id} already hard-locked, cannot override\")\n",
        "            return False\n",
        "        node.lock(mode)\n",
        "        event = {\n",
        "            \"node_id\": node.node_id,\n",
        "            \"action\": \"lock\",\n",
        "            \"mode\": mode,\n",
        "            \"timestamp\": datetime.utcnow().isoformat()\n",
        "        }\n",
        "        self.audit_log.append(event)\n",
        "        logger.info(f\"Locked node {node.node_id} with mode={mode}\")\n",
        "        return True\n",
        "\n",
        "    def unlock_node(self, node: TreeNodeV1):\n",
        "        \"\"\"Unlock a node.\"\"\"\n",
        "        if node.lock_flag == \"hard\":\n",
        "            logger.warning(f\"Node {node.node_id} is hard-locked, cannot unlock\")\n",
        "            return False\n",
        "        node.unlock()\n",
        "        event = {\n",
        "            \"node_id\": node.node_id,\n",
        "            \"action\": \"unlock\",\n",
        "            \"mode\": \"unlocked\",\n",
        "            \"timestamp\": datetime.utcnow().isoformat()\n",
        "        }\n",
        "        self.audit_log.append(event)\n",
        "        logger.info(f\"Unlocked node {node.node_id}\")\n",
        "        return True\n",
        "\n",
        "    def get_audit_log(self):\n",
        "        return self.audit_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7Bcqe3sYg_m",
        "outputId": "2d816160-0432-4f4c-e4f7-4cc02e770761"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/TreeSnapshot.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/TreeSnapshot.py\n",
        "import math\n",
        "from collections import deque\n",
        "from TreeNodeV1 import TreeNodeV1\n",
        "\n",
        "class TreeSnapshot:\n",
        "    def __init__(self, root: TreeNodeV1):\n",
        "        self.root = root\n",
        "        self.depth = 0\n",
        "        self.node_count = 0\n",
        "        self.leaf_count = 0\n",
        "        self.branching_factor = 0.0\n",
        "        self.entropy = 0.0\n",
        "        self.weak_leaves = 0\n",
        "\n",
        "        # Extras to keep EIS stable even if caller forgets to set them\n",
        "        self.branch_flip_rate = 0.0  # 0..1 (if you compute flips, overwrite upstream)\n",
        "\n",
        "        if root:\n",
        "            self._analyze()\n",
        "\n",
        "    def _analyze(self):\n",
        "        queue = deque([self.root])\n",
        "        total_children = 0\n",
        "        non_leaf_count = 0\n",
        "        leaf_confidences = []\n",
        "        max_level = 0\n",
        "        total_nodes = 0\n",
        "\n",
        "        while queue:\n",
        "            node = queue.popleft()\n",
        "            total_nodes += 1\n",
        "            max_level = max(max_level, getattr(node, \"level\", 0))\n",
        "\n",
        "            if node.is_leaf():\n",
        "                self.leaf_count += 1\n",
        "                try:\n",
        "                    leaf_confidences.append(float(node.get_confidence()))\n",
        "                except Exception:\n",
        "                    leaf_confidences.append(0.0)\n",
        "            else:\n",
        "                non_leaf_count += 1\n",
        "                c = getattr(node, \"children\", []) or []\n",
        "                total_children += len(c)\n",
        "                for child in c:\n",
        "                    queue.append(child)\n",
        "\n",
        "        self.node_count = total_nodes\n",
        "        self.depth = max_level\n",
        "\n",
        "        # Branching factor over non-leaves\n",
        "        if non_leaf_count > 0:\n",
        "            self.branching_factor = total_children / float(non_leaf_count)\n",
        "\n",
        "        # Root-level entropy over child fanouts\n",
        "        if self.root and getattr(self.root, \"children\", []):\n",
        "            counts = [len(c.children or []) for c in self.root.children]\n",
        "            total = sum(counts) if sum(counts) > 0 else 1\n",
        "            probs = [(c / total) for c in counts]\n",
        "            self.entropy = -sum(p * math.log(p + 1e-9, 2) for p in probs if p > 0)\n",
        "\n",
        "        # Weak leaves by confidence threshold\n",
        "        self.weak_leaves = sum(1 for c in leaf_confidences if c < 0.3)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            \"depth\": int(self.depth),\n",
        "            \"node_count\": int(self.node_count),\n",
        "            \"leaf_count\": int(self.leaf_count),\n",
        "            \"branching_factor\": round(float(self.branching_factor), 3),\n",
        "            \"entropy\": round(float(self.entropy), 4),\n",
        "            \"weak_leaves\": int(self.weak_leaves),\n",
        "\n",
        "            # keep EIS fields present to avoid KeyErrors upstream\n",
        "            \"branch_flip_rate\": 0.0,\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pOomCyjZcfz",
        "outputId": "d2746e90-f3b1-414e-d58b-fab9cd9da496"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/RowBatchSummary.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/RowBatchSummary.py\n",
        "# RowBatchSummary.py\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "\n",
        "class RowBatchSummary:\n",
        "    def __init__(self, batch_results: list):\n",
        "        \"\"\"\n",
        "        Robust aggregator over a batch of row-level tree results.\n",
        "\n",
        "        Expected item format (keys are optional; handled defensively):\n",
        "        {\n",
        "          \"row_index\": int,\n",
        "          \"features\": dict,                # row feature map (can be mixed types)\n",
        "          \"path\": list[str],               # DFS path (leaf last)\n",
        "          \"prev_path\": list[str],          # previous path (for flip detection)\n",
        "          \"snapshot\": dict                 # optional structural stats\n",
        "        }\n",
        "        \"\"\"\n",
        "        self.batch_results = batch_results or []\n",
        "        self.branch_flip_rate = 0.0\n",
        "        self.stability_score = 0.0\n",
        "        self.feature_stats = {}\n",
        "        self.coverage = {}\n",
        "        self._analyze()\n",
        "\n",
        "    def _analyze(self):\n",
        "        flips = 0\n",
        "        total = len(self.batch_results)\n",
        "        branch_counts = Counter()\n",
        "        feature_accum = defaultdict(list)\n",
        "\n",
        "        for row in self.batch_results:\n",
        "            # Defensive access\n",
        "            prev_path = row.get(\"prev_path\")\n",
        "            path = row.get(\"path\")\n",
        "            features = row.get(\"features\", {})\n",
        "\n",
        "            # Flip detection\n",
        "            if isinstance(prev_path, list) and isinstance(path, list) and prev_path != path:\n",
        "                flips += 1\n",
        "\n",
        "            # Coverage (count leaf occurrences)\n",
        "            if isinstance(path, list) and len(path) > 0:\n",
        "                leaf_id = path[-1]\n",
        "                branch_counts[leaf_id] += 1\n",
        "\n",
        "            # Feature stats (numeric-only, robust casting)\n",
        "            if isinstance(features, dict):\n",
        "                for k, v in features.items():\n",
        "                    # Try to coerce to float; skip non-numeric\n",
        "                    val = None\n",
        "                    try:\n",
        "                        # Handle numpy scalars, booleans, ints, floats, numeric strings\n",
        "                        if isinstance(v, (bool, int, float, np.number)):\n",
        "                            val = float(v)\n",
        "                        elif isinstance(v, str):\n",
        "                            # Attempt to parse numeric strings; skip categorical text\n",
        "                            val = float(v)\n",
        "                        # Else: unsupported type → skip\n",
        "                    except Exception:\n",
        "                        val = None\n",
        "\n",
        "                    # Accept only finite numbers\n",
        "                    if val is not None and np.isfinite(val):\n",
        "                        feature_accum[k].append(val)\n",
        "\n",
        "        # Flip rate and stability\n",
        "        self.branch_flip_rate = (flips / total) if total > 0 else 0.0\n",
        "        self.stability_score = 1.0 - self.branch_flip_rate\n",
        "\n",
        "        # Means over accumulated numeric features\n",
        "        self.feature_stats = {\n",
        "            k: float(np.mean(vals))\n",
        "            for k, vals in feature_accum.items()\n",
        "            if isinstance(vals, list) and len(vals) > 0\n",
        "        }\n",
        "\n",
        "        # Coverage distribution of leaves\n",
        "        self.coverage = dict(branch_counts)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            \"branch_flip_rate\": round(self.branch_flip_rate, 3),\n",
        "            \"stability_score\": round(self.stability_score, 3),\n",
        "            \"feature_stats\": self.feature_stats,\n",
        "            \"coverage\": self.coverage,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxdHOYSbXt33"
      },
      "source": [
        "# Phase => 3 : Neural Intergation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYGCB9hhzhpl",
        "outputId": "1dac37c9-16d7-4d19-8577-7e19f6a21b1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing anchor_extractor.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile anchor_extractor.py\n",
        "\"\"\"\n",
        "Tree-first Anchor Extractor.\n",
        "Emits anchors tied to tree nodes based on node features and snapshot stats.\n",
        "\"\"\"\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Anchor:\n",
        "    anchor_id: str\n",
        "    node_id: str\n",
        "    anchor_type: str\n",
        "    features: Dict\n",
        "    provenance: str = \"tree_based\"\n",
        "    span: Optional[Dict] = None  # {\"start\": int, \"end\": int}\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class AnchorConfig:\n",
        "    branching_threshold: int = 2\n",
        "    entropy_threshold: float = 0.6\n",
        "    leaf_token_threshold: int = 8\n",
        "\n",
        "\n",
        "def _has_digits(tokens: List[str]) -> bool:\n",
        "    for t in tokens or []:\n",
        "        if any(ch.isdigit() for ch in t):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _has_symbols(tokens: List[str]) -> bool:\n",
        "    SYMBOLS = set(\"=+-*/^%\")\n",
        "    for t in tokens or []:\n",
        "        if any(ch in SYMBOLS for ch in t):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def extract_anchors(snapshot: Dict, config: Optional[AnchorConfig] = None) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Extract anchors from a TreeSnapshot.\n",
        "\n",
        "    Expected minimal snapshot format:\n",
        "      snapshot = {\n",
        "        \"nodes\": {\n",
        "          node_id: {\n",
        "            \"depth\": int,\n",
        "            \"entropy\": float,\n",
        "            \"tokens\": List[str],\n",
        "            \"is_leaf\": bool,\n",
        "            \"children\": List[str],\n",
        "            \"locked\": bool,\n",
        "            \"weak\": bool,               # optional\n",
        "            \"is_heading\": bool,         # optional\n",
        "            \"is_code_block\": bool,      # optional\n",
        "            \"span\": {\"start\": int, \"end\": int}  # optional\n",
        "          },\n",
        "          ...\n",
        "        },\n",
        "        \"root_id\": str\n",
        "      }\n",
        "\n",
        "    Returns: List[Anchor as plain dict]\n",
        "    \"\"\"\n",
        "    cfg = config or AnchorConfig()\n",
        "    nodes = snapshot.get(\"nodes\", {})\n",
        "    root_id = snapshot.get(\"root_id\")\n",
        "\n",
        "    anchors: List[Anchor] = []\n",
        "\n",
        "    for node_id, meta in nodes.items():\n",
        "        depth = meta.get(\"depth\", 0)\n",
        "        entropy = float(meta.get(\"entropy\", 0.0))\n",
        "        tokens = meta.get(\"tokens\", []) or []\n",
        "        is_leaf = bool(meta.get(\"is_leaf\", False))\n",
        "        children = meta.get(\"children\", []) or []\n",
        "        branching_factor = len(children)\n",
        "        locked = bool(meta.get(\"locked\", False))\n",
        "        weak = bool(meta.get(\"weak\", False))\n",
        "        span = meta.get(\"span\")\n",
        "\n",
        "        features = {\n",
        "            \"depth\": depth,\n",
        "            \"entropy\": entropy,\n",
        "            \"token_count\": len(tokens),\n",
        "            \"is_leaf\": is_leaf,\n",
        "            \"branching_factor\": branching_factor,\n",
        "            \"locked\": locked,\n",
        "        }\n",
        "\n",
        "        def emit(anchor_type: str):\n",
        "            anchors.append(Anchor(\n",
        "                anchor_id=f\"{node_id}::{anchor_type}\",\n",
        "                node_id=node_id,\n",
        "                anchor_type=anchor_type,\n",
        "                features=features,\n",
        "                span=span\n",
        "            ))\n",
        "\n",
        "        # Root\n",
        "        if node_id == root_id:\n",
        "            emit(\"root_anchor\")\n",
        "\n",
        "        # Structure rules\n",
        "        if branching_factor >= cfg.branching_threshold:\n",
        "            emit(\"branching_point\")\n",
        "        if entropy >= cfg.entropy_threshold:\n",
        "            emit(\"unstable_branch\")\n",
        "        if is_leaf and len(tokens) >= cfg.leaf_token_threshold:\n",
        "            emit(\"leaf_dense\")\n",
        "\n",
        "        # Content rules (tree-derived)\n",
        "        if is_leaf and _has_digits(tokens):\n",
        "            emit(\"number_leaf\")\n",
        "        if is_leaf and _has_symbols(tokens):\n",
        "            emit(\"symbol_leaf\")\n",
        "\n",
        "        # State rules\n",
        "        if locked:\n",
        "            emit(\"locked_node\")\n",
        "        if weak:\n",
        "            emit(\"weak_leaf\")\n",
        "\n",
        "        # Optional: heading/code-like flags\n",
        "        if meta.get(\"is_heading\", False):\n",
        "            emit(\"text_heading_like\")\n",
        "        if meta.get(\"is_code_block\", False):\n",
        "            emit(\"code_block_like\")\n",
        "\n",
        "    # Deduplicate by anchor_id (determinism)\n",
        "    unique = {a.anchor_id: a for a in anchors}\n",
        "\n",
        "    # Convert dataclasses to plain dicts\n",
        "    return [vars(a) for a in unique.values()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU92R28h0NbF",
        "outputId": "92941e82-883e-481e-d18b-08a91902c851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing embedding_index.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile embedding_index.py\n",
        "\"\"\"\n",
        "Lightweight embedding index for retrieval (TF-IDF + cosine).\n",
        "Domain-agnostic and deterministic.\n",
        "\"\"\"\n",
        "\n",
        "from typing import List, Dict, Optional\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "class EmbeddingIndex:\n",
        "    def __init__(self):\n",
        "        self.fragments: List[str] = []\n",
        "        self.vectorizer: Optional[TfidfVectorizer] = None\n",
        "        self.embeddings = None\n",
        "\n",
        "    def build_index(self, fragments: List[str]):\n",
        "        \"\"\"\n",
        "        Build index from list of text fragments.\n",
        "        \"\"\"\n",
        "        self.fragments = fragments or []\n",
        "        # TF-IDF with unigrams+bigrams for better matching\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            lowercase=True,\n",
        "            analyzer=\"word\",\n",
        "            ngram_range=(1, 2),\n",
        "            min_df=1,\n",
        "            max_df=1.0\n",
        "        )\n",
        "        if self.fragments:\n",
        "            self.embeddings = self.vectorizer.fit_transform(self.fragments)\n",
        "        else:\n",
        "            self.embeddings = None\n",
        "\n",
        "    def query(self, text: str, k: int = 5) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Query top-k similar fragments.\n",
        "        Returns list of dicts:\n",
        "          {\n",
        "            \"fragmentid\": str,\n",
        "            \"fragmenttext\": str,\n",
        "            \"embeddingrank\": int,\n",
        "            \"retrievalconfidence\": float\n",
        "          }\n",
        "        \"\"\"\n",
        "        if not self.fragments or self.embeddings is None or self.vectorizer is None:\n",
        "            return []\n",
        "        q_vec = self.vectorizer.transform([text])\n",
        "        sims = cosine_similarity(q_vec, self.embeddings)[0]  # shape: (N,)\n",
        "        ranked = sorted(\n",
        "            [(i, float(sims[i])) for i in range(len(self.fragments))],\n",
        "            key=lambda x: x[1],\n",
        "            reverse=True\n",
        "        )\n",
        "        top = ranked[:max(1, k)]\n",
        "        results: List[Dict] = []\n",
        "        for rank, (idx, score) in enumerate(top, start=1):\n",
        "            results.append({\n",
        "                \"fragmentid\": f\"frag{idx}\",\n",
        "                \"fragmenttext\": self.fragments[idx],\n",
        "                \"embeddingrank\": rank,\n",
        "                \"retrievalconfidence\": round(score, 6)\n",
        "            })\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85-3qnIF50rT",
        "outputId": "4fd5b30b-3b6b-4696-fecb-1e4c51edb686"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing embedding_matching.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile embedding_matching.py\n",
        "# ===========================================================\n",
        "# embedding_matching.py (Final Clean Version)\n",
        "# Fresh index, clean retrieval, correct IDs, stable embeddings\n",
        "# ===========================================================\n",
        "\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "from tokenizer_and_embedding import TokenEmbedding, universal_tokenizer\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# GLOBAL INDEX (in RAM)\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "_index_fragments = []       # list[str]\n",
        "_index_vectors = None       # np.ndarray (N, dim)\n",
        "_embedder = TokenEmbedding(vocab=[], dim=50)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# INTERNAL SENTENCE EMBEDDING\n",
        "# Uses token averages (since TokenEmbedding supports only lookup)\n",
        "# -----------------------------------------------------------\n",
        "def _embed(text: str):\n",
        "    text = str(text)\n",
        "    toks = universal_tokenizer(text.lower())\n",
        "\n",
        "    if not toks:\n",
        "        return np.zeros(_embedder.dim, dtype=np.float32)\n",
        "\n",
        "    vecs = []\n",
        "    for t in toks:\n",
        "        v = _embedder.lookup(t)\n",
        "\n",
        "        # Convert torch tensor or numpy to numpy array\n",
        "        if hasattr(v, \"detach\"):\n",
        "            v = v.detach().cpu().numpy()\n",
        "        else:\n",
        "            v = np.array(v)\n",
        "\n",
        "        vecs.append(v)\n",
        "\n",
        "    return np.mean(vecs, axis=0).astype(np.float32)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# BUILD INDEX (ALWAYS REBUILDS)\n",
        "# -----------------------------------------------------------\n",
        "def build_fragments(fragments: List[str]):\n",
        "    \"\"\"\n",
        "    Build a fresh embedding index from scratch.\n",
        "    fragments: list of fragment strings\n",
        "    \"\"\"\n",
        "    global _index_fragments, _index_vectors\n",
        "\n",
        "    print(\">>> Building fresh embedding index...\")\n",
        "\n",
        "    _index_fragments = fragments[:]  # store raw text\n",
        "\n",
        "    vecs = []\n",
        "    for text in fragments:\n",
        "        vecs.append(_embed(text))\n",
        "\n",
        "    _index_vectors = np.vstack(vecs)\n",
        "    print(\">>> Embedding index built! Total vectors:\", len(_index_fragments))\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# RETRIEVE (cosine similarity)\n",
        "# -----------------------------------------------------------\n",
        "def retrieve(query: str, k: int = 5) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Retrieve top-k fragments using cosine similarity.\n",
        "    ID pattern matches your all_fragments: \"row_col\" (0_0)\n",
        "    \"\"\"\n",
        "    if _index_vectors is None or len(_index_vectors) == 0:\n",
        "        return []\n",
        "\n",
        "    q_vec = _embed(query)\n",
        "    q_vec = q_vec / (np.linalg.norm(q_vec) + 1e-12)\n",
        "\n",
        "    sims = _index_vectors @ q_vec\n",
        "    idxs = np.argsort(-sims)[:k]\n",
        "\n",
        "    results = []\n",
        "    for rank, idx in enumerate(idxs):\n",
        "        # correct consistent ID structure for fragments\n",
        "        frag_id = f\"{idx//6}_{idx%6}\"\n",
        "\n",
        "        results.append({\n",
        "            \"fragmentid\": frag_id,\n",
        "            \"fragmenttext\": _index_fragments[idx],\n",
        "            \"embeddingrank\": rank + 1,\n",
        "            \"retrievalconfidence\": float(sims[idx])\n",
        "        })\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtHkGsHf6nC4",
        "outputId": "34022399-4ec1-4f17-efad-f6d2e0285aa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing rl_critic.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile rl_critic.py\n",
        "\"\"\"\n",
        "RL Critic: quick scoring estimates for candidate actions.\n",
        "Heuristic-based, deterministic, tree-feature aware.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict\n",
        "\n",
        "# Domain-specific modifiers (tunable)\n",
        "DOMAIN_MODIFIERS = {\n",
        "    \"math\": 0.05,\n",
        "    \"code\": 0.02,\n",
        "    \"science\": 0.00,\n",
        "    \"chess\": -0.01\n",
        "}\n",
        "\n",
        "def score_candidate(state: Dict, action: Dict) -> Dict:\n",
        "    \"\"\"\n",
        "    Estimate pos/neg/quality scores for a candidate.\n",
        "\n",
        "    Args:\n",
        "        state: dict with features like\n",
        "            {\"depth\": int, \"entropy\": float, \"token_count\": int,\n",
        "             \"branching_factor\": int, \"is_leaf\": bool,\n",
        "             \"coverage_balance\": float, \"domain\": str}\n",
        "        action: dict like\n",
        "            {\"type\": \"split\"|\"lock\"|\"promote\", \"target\": node_id, \"anchor_type\": str}\n",
        "\n",
        "    Returns:\n",
        "        {\n",
        "            \"est_pos\": float,\n",
        "            \"est_neg\": float,\n",
        "            \"est_quality\": float\n",
        "        }\n",
        "    \"\"\"\n",
        "    depth = float(state.get(\"depth\", 0))\n",
        "    entropy = float(state.get(\"entropy\", 0.0))\n",
        "    token_count = int(state.get(\"token_count\", 0))\n",
        "    branching = int(state.get(\"branching_factor\", 0))\n",
        "    is_leaf = bool(state.get(\"is_leaf\", False))\n",
        "    coverage_balance = float(state.get(\"coverage_balance\", 0.0))\n",
        "    domain = (state.get(\"domain\") or \"\").lower()\n",
        "\n",
        "    action_type = (action.get(\"type\") or \"\").lower()\n",
        "    anchor_type = (action.get(\"anchor_type\") or \"\").lower()\n",
        "\n",
        "    # Base positive signal: entropy, branching, token richness\n",
        "    pos = 0.0\n",
        "    pos += min(1.0, entropy) * 0.5\n",
        "    pos += min(1.0, branching / 3.0) * 0.3\n",
        "    pos += min(1.0, token_count / 12.0) * 0.2\n",
        "\n",
        "    # Coverage balance bonus\n",
        "    pos += min(0.2, coverage_balance * 0.2)\n",
        "\n",
        "    # Action alignment boosts\n",
        "    if action_type == \"split\":\n",
        "        pos += 0.2 if (entropy >= 0.6 or branching >= 2) else 0.0\n",
        "    if action_type == \"lock\":\n",
        "        pos += 0.15 if anchor_type in (\"number_leaf\", \"text_heading_like\") else 0.0\n",
        "    if action_type == \"promote\":\n",
        "        pos += 0.15 if (depth >= 1 and not is_leaf) else 0.0\n",
        "\n",
        "    # Negative signals\n",
        "    neg = 0.0\n",
        "    if is_leaf and entropy >= 0.7 and token_count < 3:\n",
        "        neg += 0.3\n",
        "    if action_type == \"lock\" and anchor_type not in (\"number_leaf\", \"symbol_leaf\", \"text_heading_like\"):\n",
        "        neg += 0.25\n",
        "    if action_type == \"split\" and branching == 0:\n",
        "        neg += 0.2\n",
        "\n",
        "    # Quality = positive minus weighted negative\n",
        "    quality = pos - 0.5 * neg\n",
        "\n",
        "    # Apply domain modifier\n",
        "    quality += DOMAIN_MODIFIERS.get(domain, 0.0)\n",
        "\n",
        "    # Clamp to [0,1]\n",
        "    quality = max(0.0, min(1.0, quality))\n",
        "\n",
        "    return {\n",
        "        \"est_pos\": round(pos, 4),\n",
        "        \"est_neg\": round(neg, 4),\n",
        "        \"est_quality\": round(quality, 4)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoSoXmWu7O6o",
        "outputId": "f26a8e05-2764-4a2e-d847-b4c1c5946a9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing decoder.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile decoder.py\n",
        "\"\"\"\n",
        "Decoder: generate short explanation text from snapshot + retrieval context.\n",
        "Deterministic, template-based.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, List\n",
        "\n",
        "def decode_snapshot(snapshot: Dict, anchors: List[Dict], retrievals: List[Dict]) -> str:\n",
        "    \"\"\"\n",
        "    Produce explanation text for reviewer clarity.\n",
        "    Chooses first anchor and first retrieval for a concise rationale.\n",
        "    \"\"\"\n",
        "    if not anchors:\n",
        "        return \"No anchors found for this snapshot.\"\n",
        "    anchor = anchors[0]\n",
        "    node_id = anchor.get(\"node_id\")\n",
        "    a_type = anchor.get(\"anchor_type\")\n",
        "    entropy = anchor.get(\"features\", {}).get(\"entropy\", 0.0)\n",
        "    branching = anchor.get(\"features\", {}).get(\"branching_factor\", 0)\n",
        "\n",
        "    base = f\"Node {node_id} anchored as {a_type}; entropy {entropy:.2f}, branching {branching}.\"\n",
        "    if retrievals:\n",
        "        r = retrievals[0]\n",
        "        frag = r.get(\"fragmenttext\", \"\")\n",
        "        conf = r.get(\"retrievalconfidence\", 0.0)\n",
        "        base += f\" Retrieved: '{frag}' (confidence {conf:.2f}).\"\n",
        "\n",
        "    return base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6A18bI_7T4b",
        "outputId": "28187ab7-c1a2-4eaf-942f-c09507e91a23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing smoother.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile smoother.py\n",
        "\"\"\"\n",
        "Smoother: polish explanation text for readability.\n",
        "Simple rule-based cleanup.\n",
        "\"\"\"\n",
        "\n",
        "def smooth_text(text: str) -> str:\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    cleaned = text.strip()\n",
        "    # Capitalize first letter\n",
        "    cleaned = cleaned[0].upper() + cleaned[1:]\n",
        "    # Ensure trailing period\n",
        "    if cleaned[-1] not in \".!?\":\n",
        "        cleaned += \".\"\n",
        "    # Compact extra spaces\n",
        "    cleaned = \" \".join(cleaned.split())\n",
        "    return cleaned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm9FLOywmpMR"
      },
      "source": [
        "# Phase 3.2 => Dual‑Valence RL‑lite Learner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz3Q3M67da-G",
        "outputId": "bb410192-7d6d-4540-8030-28baf1f74fa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/tokenizer_and_embedding.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/tokenizer_and_embedding.py\n",
        "import torch, re, logging\n",
        "from typing import List\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class TokenEmbedding:\n",
        "    def __init__(self, vocab: List[str], dim:int=50, device:str='cpu'):\n",
        "        self.dim, self.device = dim, device\n",
        "        self.vocab = ['<unk>'] + vocab\n",
        "        self.word2idx = {w:i for i,w in enumerate(self.vocab)}\n",
        "        self.embeddings = torch.randn(len(self.vocab), dim, device=device) / (dim**0.5)\n",
        "        logger.info(f\"TokenEmbedding: vocab_size={len(self.vocab)} dim={dim}\")\n",
        "\n",
        "    def lookup(self, token:str) -> torch.Tensor:\n",
        "        idx = self.word2idx.get(token, 0)\n",
        "        return self.embeddings[idx]\n",
        "\n",
        "def universal_tokenizer(text: str) -> List[str]:\n",
        "    if not text:\n",
        "        return []\n",
        "    # split numbers, identifiers, symbols\n",
        "    return re.findall(r'\\d+\\.\\d+|\\d+|[A-Za-z]+|[+\\-*/^=():]', text)\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def build_trait_embedding_index(trait_vpk_table, embedder):\n",
        "    index = {}\n",
        "    for trait in trait_vpk_table.keys():\n",
        "        tokens = universal_tokenizer(trait)\n",
        "        if not tokens:\n",
        "            continue\n",
        "        vec = sum(embedder.lookup(tok) for tok in tokens) / len(tokens)\n",
        "        index[trait] = vec / (vec.norm() + 1e-9)  # normalized\n",
        "    return index\n",
        "\n",
        "\n",
        "def semantic_match_traits(query, trait_index, embedder, top_k=8):\n",
        "    tokens = universal_tokenizer(query.lower())\n",
        "    if not tokens:\n",
        "        return []\n",
        "    q_vec = sum(embedder.lookup(tok) for tok in tokens) / len(tokens)\n",
        "    q_vec = q_vec / (q_vec.norm() + 1e-9)\n",
        "\n",
        "    sims = []\n",
        "    for trait, t_vec in trait_index.items():\n",
        "        s = F.cosine_similarity(q_vec.unsqueeze(0), t_vec.unsqueeze(0)).item()\n",
        "        sims.append((trait, s))\n",
        "\n",
        "    sims.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    strong = [t for t, s in sims if s >= 0.72][:top_k]     # strong signals\n",
        "    medium = [t for t, s in sims if 0.55 <= s < 0.72][:top_k - len(strong)]\n",
        "\n",
        "    return strong + medium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rk7zgcXox92u",
        "outputId": "4c86380a-15d5-4155-a034-23009f0b69cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/TokenEmbedding.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/TokenEmbedding.py\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import logging\n",
        "from TreeNodeV1 import TreeNodeV1\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class TokenEmbedding:\n",
        "    def __init__(self, model_name=\"all-MiniLM-L6-v2\", device=\"cpu\"):\n",
        "        \"\"\"\n",
        "        CPU-first, lightweight embeddings.\n",
        "        model_name: 80MB, ~100ms per query on CPU\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        self.model = SentenceTransformer(model_name).to(self.device)\n",
        "        self.model.eval()\n",
        "        logger.info(f\"TokenEmbedding loaded: {model_name} on {device}\")\n",
        "\n",
        "    def embed_leaves(self, trees: list) -> dict:\n",
        "        \"\"\"\n",
        "        Input: list of TreeNodeV1\n",
        "        Output: {leaf_text: vector}\n",
        "        \"\"\"\n",
        "        all_leaves = []\n",
        "        leaf_to_tree = {}\n",
        "        for tree in trees:\n",
        "            leaves = tree.get_leaves()\n",
        "            for leaf in leaves:\n",
        "                all_leaves.append(leaf)\n",
        "                leaf_to_tree[leaf] = tree\n",
        "\n",
        "        if not all_leaves:\n",
        "            return {}\n",
        "\n",
        "        # Batch encode\n",
        "        with torch.no_grad():\n",
        "            embeddings = self.model.encode(\n",
        "                all_leaves,\n",
        "                batch_size=32,\n",
        "                show_progress_bar=False,\n",
        "                convert_to_tensor=True\n",
        "            ).cpu()\n",
        "\n",
        "        # Map back\n",
        "        embedding_map = {}\n",
        "        for leaf, vec in zip(all_leaves, embeddings):\n",
        "            embedding_map[leaf] = vec\n",
        "            # Store in tree node\n",
        "            node = self._find_node(leaf_to_tree[leaf], leaf)\n",
        "            if node:\n",
        "                node.store_vector(vec)\n",
        "\n",
        "        return embedding_map\n",
        "\n",
        "    def _find_node(self, root: TreeNodeV1, value: str):\n",
        "        \"\"\"DFS find node with value.\"\"\"\n",
        "        if root.value == value:\n",
        "            return root\n",
        "        for child in root.children:\n",
        "            found = self._find_node(child, value)\n",
        "            if found:\n",
        "                return found\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxXQYso0dbIQ",
        "outputId": "a1f5161e-2bac-4107-dd23-a19489da0a7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/model_utils.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/model_utils.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import logging\n",
        "\n",
        "# Setup logger\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class InputProjector(nn.Module):\n",
        "    def __init__(self, input_dim, target_dim):\n",
        "        super().__init__()\n",
        "        self.project = nn.Linear(input_dim, target_dim)\n",
        "        logger.info(f\"Initialized InputProjector: {input_dim} -> {target_dim}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.project(x)\n",
        "\n",
        "def patch_model_input(model, input_vector, expected_dim):\n",
        "    actual_dim = input_vector.shape[-1]\n",
        "    if actual_dim != expected_dim:\n",
        "        if hasattr(model, 'input_projector'):\n",
        "            logger.warning(f\"Model already has input_projector, skipping patch\")\n",
        "            return\n",
        "        logger.info(f\"Auto-patching model input: {actual_dim} -> {expected_dim}\")\n",
        "        projector = InputProjector(actual_dim, expected_dim).to(input_vector.device)\n",
        "        old_forward = model.forward\n",
        "\n",
        "        def new_forward(x):\n",
        "            x_proj = projector(x)\n",
        "            return old_forward(x_proj)\n",
        "\n",
        "        model.forward = new_forward\n",
        "        model.input_projector = projector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "X0FTioIiTEmM",
        "outputId": "8b38a8c2-8ff7-4889-8df6-268235d421f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing Phase2Env.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile Phase2Env.py\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "\n",
        "from TreeSnapshot import TreeSnapshot\n",
        "from check_action_allowed import check_action_allowed\n",
        "from apply_action import apply_action\n",
        "from compute_reward import compute_reward\n",
        "from generative_decision_loop_safe import generative_decision_loop as generative_decision_loop_safe\n",
        "\n",
        "\n",
        "class Phase2Env:\n",
        "    def __init__(self, builder, data, feature_vectors, max_edits=20):\n",
        "        self.builder = builder\n",
        "        self.data = data\n",
        "        self.feature_vectors = feature_vectors\n",
        "        self.current_idx = None\n",
        "        self.tree = None\n",
        "        self.snapshot = None\n",
        "        self.steps = 0\n",
        "        self.max_edits = max_edits\n",
        "\n",
        "        # Economy state\n",
        "        self.tickets = {\"G\": 0, \"B\": 0, \"Y\": 0, \"R\": 0, \"P\": 0}\n",
        "        self.failure_memory = {}\n",
        "        self.temp_tickets = {\"G\": 0, \"B\": 0, \"Y\": 0, \"R\": 0}\n",
        "        self.decay_queue = []\n",
        "\n",
        "        # Adaptive coeffs container (not directly used by compute_reward;\n",
        "        # you can surface them to UI or curriculum later)\n",
        "        self.coeffs = {\"alpha\": 1.0, \"beta\": 1.0, \"gamma\": 1.0, \"delta\": 1.0}\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # Reset (supports global prakriti tree pointer)\n",
        "    # --------------------------------------------------------------------------\n",
        "    def reset(self, idx: int = None, use_global_tree=False, global_tree=None):\n",
        "        if use_global_tree and global_tree is not None:\n",
        "            self.tree = global_tree\n",
        "            self.snapshot = TreeSnapshot(self.tree).to_dict()\n",
        "            self.steps = 0\n",
        "\n",
        "            # reset economy\n",
        "            self.tickets = {\"G\": 0, \"B\": 0, \"Y\": 0, \"R\": 0, \"P\": 0}\n",
        "            self.failure_memory = {}\n",
        "            self.temp_tickets = {\"G\": 0, \"B\": 0, \"Y\": 0, \"R\": 0}\n",
        "            self.decay_queue = []\n",
        "            self.coeffs = {\"alpha\": 1.0, \"beta\": 1.0, \"gamma\": 1.0, \"delta\": 1.0}\n",
        "            return self._snapshot_to_obs(self.snapshot)\n",
        "\n",
        "        # Local (row) tree\n",
        "        row = self.data.iloc[idx]\n",
        "        active_tokens = [c for c, v in row.items() if v == 1 and c not in [\"difficulty_score\", \"tree_type\"]]\n",
        "        vec_pairs = [(tok, self.feature_vectors[tok]) for tok in active_tokens if tok in self.feature_vectors]\n",
        "\n",
        "        self.tree = self.builder.build_tree(vec_pairs, sample_id=f\"row{idx}\")\n",
        "        self.snapshot = TreeSnapshot(self.tree).to_dict()\n",
        "        self.steps = 0\n",
        "\n",
        "        self.tickets = {\"G\": 0, \"B\": 0, \"Y\": 0, \"R\": 0, \"P\": 0}\n",
        "        self.failure_memory = {}\n",
        "        self.temp_tickets = {\"G\": 0, \"B\": 0, \"Y\": 0, \"R\": 0}\n",
        "        self.decay_queue = []\n",
        "        self.coeffs = {\"alpha\": 1.0, \"beta\": 1.0, \"gamma\": 1.0, \"delta\": 1.0}\n",
        "\n",
        "        # small structural budget by default\n",
        "        self.tickets[\"B\"] = max(self.tickets.get(\"B\", 0), 12)\n",
        "        return self._snapshot_to_obs(self.snapshot)\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    def _normalize_action(self, action):\n",
        "        if isinstance(action, dict):\n",
        "            op = action.get(\"type\") or action.get(\"op_type\")\n",
        "            tgt = action.get(\"target\") or action.get(\"target_id\") or action.get(\"node_id\")\n",
        "            extra = action.get(\"extra\") if \"extra\" in action else action.get(\"payload\", None)\n",
        "            return (op, tgt, extra)\n",
        "        if isinstance(action, (list, tuple)) and len(action) >= 2:\n",
        "            op, tgt = action[0], action[1]\n",
        "            extra = action[2] if len(action) > 2 else None\n",
        "            return (op, tgt, extra)\n",
        "        return (None, None, None)\n",
        "\n",
        "    def _validate_snapshot(self, snap):\n",
        "        if not isinstance(snap, dict):\n",
        "            return False\n",
        "        required = [\"depth\", \"node_count\", \"leaf_count\", \"branching_factor\", \"entropy\", \"weak_leaves\"]\n",
        "        return all(k in snap for k in required)\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    def step(self, action, config=None,\n",
        "             generator=None, scorer=None, sim_engine=None,\n",
        "             verifier=None, policy_module=None,\n",
        "             apply_engine=None, provenance_logger=None,\n",
        "             fallback_module=None):\n",
        "\n",
        "        action = self._normalize_action(action)\n",
        "\n",
        "        if not self._validate_snapshot(self.snapshot):\n",
        "            return np.zeros(6, dtype=np.float32), -1.0, True, {\"log\": {\"error\": \"invalid_snapshot\"}}\n",
        "\n",
        "        prev_snap = dict(self.snapshot)\n",
        "\n",
        "        # EIS (for generative trigger only; the authoritative EIS is recomputed inside compute_reward)\n",
        "        entropy = self.snapshot.get(\"entropy\", 0.0)\n",
        "        stability = 1.0 - self.snapshot.get(\"branch_flip_rate\", 0.0)\n",
        "        depth_ratio = self.snapshot.get(\"depth\", 1) / max(1, self.snapshot.get(\"node_count\", 1))\n",
        "        EIS = 0.4 * entropy + 0.3 * (1 - stability) + 0.3 * depth_ratio\n",
        "\n",
        "        # -------------------- Generative path --------------------\n",
        "        if config and EIS >= config.get(\"generative_threshold\", 0.7):\n",
        "            if generator is None or provenance_logger is None:\n",
        "                return self._snapshot_to_obs(self.snapshot), -0.5, True, {\n",
        "                    \"log\": {\"status\": \"generative_blocked\", \"reason\": \"missing_generator_or_logger\"}\n",
        "                }\n",
        "\n",
        "            new_snapshot, status = generative_decision_loop_safe(\n",
        "                self.snapshot,\n",
        "                self.tickets,\n",
        "                self.failure_memory,\n",
        "                config,\n",
        "                generator,\n",
        "                scorer,\n",
        "                sim_engine,\n",
        "                verifier,\n",
        "                policy_module,\n",
        "                apply_engine,\n",
        "                provenance_logger,\n",
        "                fallback_module\n",
        "            )\n",
        "            self.snapshot = new_snapshot\n",
        "\n",
        "            reward, self.tickets, self.failure_memory, log_entry = compute_reward(\n",
        "                prev_snap, self.snapshot, self.builder.mode,\n",
        "                self.tickets, self.failure_memory,\n",
        "                self.temp_tickets, self.decay_queue,\n",
        "                proposed_fix=None\n",
        "            )\n",
        "\n",
        "            self.steps += 1\n",
        "            done = (self.steps >= self.max_edits) or (reward < -5.0)\n",
        "            return self._snapshot_to_obs(self.snapshot), reward, done, {\n",
        "                \"log\": {\"mode\": \"generative\", **status, **log_entry}\n",
        "            }\n",
        "\n",
        "        # -------------------- Structural path --------------------\n",
        "        allowed, cost, reason = check_action_allowed(action, self.tickets)\n",
        "        if not allowed:\n",
        "            return self._snapshot_to_obs(self.snapshot), -1.0, True, {\n",
        "                \"log\": {\"status\": \"blocked\", \"reason\": reason}\n",
        "            }\n",
        "\n",
        "        # ticket spend\n",
        "        for k, v in (cost or {}).items():\n",
        "            self.tickets[k] = max(0, self.tickets.get(k, 0) - int(v))\n",
        "\n",
        "        # apply edit\n",
        "        log = apply_action(self.tree, action)\n",
        "\n",
        "        # refresh snapshot + add shaping\n",
        "        self.snapshot = TreeSnapshot(self.tree).to_dict()\n",
        "        cur = self.snapshot\n",
        "\n",
        "        # compute deltas for shaping\n",
        "        d_ent    = float(prev_snap.get(\"entropy\", 0.0) - cur.get(\"entropy\", 0.0))\n",
        "        d_weak   = float(prev_snap.get(\"weak_leaves\", 0) - cur.get(\"weak_leaves\", 0))\n",
        "        d_br     = float(prev_snap.get(\"branching_factor\", 0.0) - cur.get(\"branching_factor\", 0.0))\n",
        "\n",
        "        pos = 0.20 * max(0.0, d_ent) + 0.15 * max(0.0, d_weak) + 0.07 * max(0.0, d_br)\n",
        "        neg = 0.20 * max(0.0, -d_ent) + 0.10 * max(0.0, -d_br)\n",
        "\n",
        "        # anti-gaming cap\n",
        "        cur[\"pos_score\"] = float(min(pos, 5.0))\n",
        "        cur[\"neg_score\"] = float(min(neg, 5.0))\n",
        "\n",
        "        # outcome hint (reward will still re-infer robustly if absent)\n",
        "        if d_ent > 0 or d_weak > 0:\n",
        "            cur[\"outcome\"] = \"success\"\n",
        "        elif neg > pos:\n",
        "            cur[\"outcome\"] = \"repeat_fail\"\n",
        "        else:\n",
        "            cur[\"outcome\"] = \"neutral\"\n",
        "\n",
        "        self.snapshot.setdefault(\"pos_score\",0.0)\n",
        "        self.snapshot.setdefault(\"neg_score\",0.0)\n",
        "        # compute reward\n",
        "        reward, self.tickets, self.failure_memory, log_entry = compute_reward(\n",
        "            prev_snap, self.snapshot, self.builder.mode,\n",
        "            self.tickets, self.failure_memory,\n",
        "            self.temp_tickets, self.decay_queue,\n",
        "            proposed_fix=None\n",
        "        )\n",
        "\n",
        "        self.steps += 1\n",
        "        done = (self.steps >= self.max_edits) or (reward < -5.0)\n",
        "\n",
        "        obs = self._snapshot_to_obs(self.snapshot)\n",
        "        return obs, reward, done, {\"log\": {**(log or {}), **log_entry}}\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    def _snapshot_to_obs(self, snap):\n",
        "        return np.array([\n",
        "            snap[\"depth\"],\n",
        "            snap[\"node_count\"],\n",
        "            snap[\"leaf_count\"],\n",
        "            snap[\"branching_factor\"],\n",
        "            snap[\"entropy\"],\n",
        "            snap[\"weak_leaves\"],\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    def update_feedback(self, episode_stats):\n",
        "        # optional curriculum hooks (you already had these; preserved)\n",
        "        if episode_stats.get(\"avg_red\", 0) > 0.4:\n",
        "            self.coeffs[\"alpha\"] *= 0.9\n",
        "            self.coeffs[\"beta\"] *= 0.9\n",
        "\n",
        "        if episode_stats.get(\"avg_success\", 0) > 0.8:\n",
        "            self.coeffs[\"alpha\"] *= 1.1\n",
        "            self.coeffs[\"beta\"] *= 1.1\n",
        "\n",
        "        if episode_stats.get(\"entropy_plateau\", False):\n",
        "            self.coeffs[\"gamma\"] += 0.1\n",
        "\n",
        "        if episode_stats.get(\"purple_conversions\", 0) > 0.5:\n",
        "            self.coeffs[\"delta\"] += 0.1\n",
        "\n",
        "        if episode_stats.get(\"loan_tickets\", 0) > 3:\n",
        "            self.coeffs[\"alpha\"] *= 0.8\n",
        "            self.coeffs[\"beta\"] *= 0.8\n",
        "\n",
        "        return {\"coeffs\": dict(self.coeffs), \"adjustments\": episode_stats}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJbPf9Q8vab_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# Null-Shims + Safe Generative Wrapper (Phase 2 - A)\n",
        "# Paste this BELOW the Phase2Env class\n",
        "# ============================================================\n",
        "\n",
        "class NullProvLogger:\n",
        "    def log_request(self, *a, **k): pass\n",
        "    def log_candidates(self, *a, **k): pass\n",
        "    def log_scores(self, *a, **k): pass\n",
        "    def log_accept(self, *a, **k): pass\n",
        "    def log_fallback(self, *a, **k): pass\n",
        "\n",
        "class NullGenerator:\n",
        "    def generate(self, request, k=1):\n",
        "        # returns empty candidate list (safe no-op)\n",
        "        return []\n",
        "\n",
        "class NullScorer:\n",
        "    def score_batch(self, candidates, snapshot):\n",
        "        # assign default neutral scores\n",
        "        for c in candidates:\n",
        "            c[\"gen_confidence\"] = c.get(\"gen_confidence\", 0.5)\n",
        "            c[\"novelty_score\"] = c.get(\"novelty_score\", 0.0)\n",
        "        return candidates\n",
        "    def select_top_k(self, cands, k=3):\n",
        "        return cands[:k]\n",
        "\n",
        "class NullSimEngine:\n",
        "    def dual_valence(self, snapshot, candidate):\n",
        "        # pos, neg, entropy_delta, stability_delta\n",
        "        return 0.1, 0.0, 0.0, 0.0\n",
        "\n",
        "class NullVerifier:\n",
        "    def check(self, snapshot, candidate):\n",
        "        return True, \"null\"\n",
        "\n",
        "class NullPolicy:\n",
        "    def apply_ticket_penalties(self, scored, tickets, costs):\n",
        "        return scored\n",
        "    def select(self, scored, policy=\"greedy\", epsilon=0.1, temperature=1.0):\n",
        "        if not scored:\n",
        "            return None\n",
        "        return max(scored, key=lambda s: s.get(\"final_score\", 0.0))\n",
        "\n",
        "class NullApplyEngine:\n",
        "    # Return snapshot unchanged — \"no structural effect\"\n",
        "    def apply(self, *args, **kwargs):\n",
        "        snap = args[1]  # snapshot reference\n",
        "        return snap, snap, False, \"noop\"\n",
        "\n",
        "class NullFallback:\n",
        "    def choose(self, snapshot, scored, extras, failure_memory):\n",
        "        return {\"type\": \"noop\", \"reason\": \"no candidate\"}\n",
        "\n",
        "# ---- Safe Generative Loop Wrapper ----\n",
        "\n",
        "def generative_decision_loop_safe(snapshot,\n",
        "                                  tickets,\n",
        "                                  failure_memory,\n",
        "                                  config,\n",
        "                                  generator=None,\n",
        "                                  scorer=None,\n",
        "                                  sim_engine=None,\n",
        "                                  verifier=None,\n",
        "                                  policy_module=None,\n",
        "                                  apply_engine=None,\n",
        "                                  provenance_logger=None,\n",
        "                                  fallback_module=None):\n",
        "\n",
        "    # Fill missing modules with null shims\n",
        "    generator = generator or NullGenerator()\n",
        "    scorer = scorer or NullScorer()\n",
        "    sim_engine = sim_engine or NullSimEngine()\n",
        "    verifier = verifier or NullVerifier()\n",
        "    policy_module = policy_module or NullPolicy()\n",
        "    apply_engine = apply_engine or NullApplyEngine()\n",
        "    provenance_logger = provenance_logger or NullProvLogger()\n",
        "    fallback_module = fallback_module or NullFallback()\n",
        "\n",
        "    try:\n",
        "        return generative_decision_loop(\n",
        "            snapshot,\n",
        "            tickets,\n",
        "            failure_memory,\n",
        "            config,\n",
        "            generator,\n",
        "            scorer,\n",
        "            sim_engine,\n",
        "            verifier,\n",
        "            policy_module,\n",
        "            apply_engine,\n",
        "            provenance_logger,\n",
        "            fallback_module\n",
        "        )\n",
        "    except Exception as e:\n",
        "        # Safe fallback (no crash)\n",
        "        return snapshot, {\"status\": \"safe_fallback\", \"reason\": str(e)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgXUGmBbbTAx"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ui7jqYMJDqsF",
        "outputId": "ef5ec762-9be9-4fea-b500-2aabadcfdbdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing TLiteComponents.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile TLiteComponents.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import logging\n",
        "import numpy as np\n",
        "\n",
        "# Optional niceties\n",
        "try:\n",
        "    import nltk\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "except Exception:\n",
        "    # not fatal - tokenizer/embedding code may handle missing resources at runtime\n",
        "    pass\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "if not logger.handlers:\n",
        "    handler = logging.StreamHandler()\n",
        "    handler.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"))\n",
        "    logger.addHandler(handler)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Small utility / missing pieces\n",
        "# -------------------------\n",
        "class InputProjector(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple linear projector used to match unexpected input dims to model.expected_dim.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim: int, out_dim: int):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(in_dim, out_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.linear(x)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Token embedding wrapper (expects TokenEmbedding in your project)\n",
        "# -------------------------\n",
        "class EnhancedTokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab, dim, device='cpu'):\n",
        "        super().__init__()\n",
        "        # `TokenEmbedding` is assumed to be provided elsewhere in your repo\n",
        "        try:\n",
        "            from tokenizer_and_embedding import TokenEmbedding\n",
        "            tokens = list(vocab) if isinstance(vocab, (list, tuple, dict)) else vocab\n",
        "            self.embedding = TokenEmbedding(tokens, dim, device)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"TokenEmbedding unavailable or failed to import: {e}\")\n",
        "            self.embedding = None\n",
        "        self.device = device\n",
        "        self.dim = dim\n",
        "        self.oov_vector = nn.Parameter(torch.randn(dim, device=device) * 0.01)\n",
        "        logger.info(f\"Initialized EnhancedTokenEmbedding (dim={dim}) on device={device}\")\n",
        "\n",
        "    def lookup(self, token: str) -> torch.Tensor:\n",
        "        if self.embedding is None:\n",
        "            return self.oov_vector\n",
        "        try:\n",
        "            vec = self.embedding.lookup(token)\n",
        "            if isinstance(vec, torch.Tensor) and vec.dim() == 1:\n",
        "                return vec.to(self.device)\n",
        "            # if returned shape is (1, dim) or similar\n",
        "            if isinstance(vec, torch.Tensor) and vec.dim() > 1:\n",
        "                return vec.squeeze(0).to(self.device)\n",
        "            return self.oov_vector\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Token {token} lookup failed, using OOV: {e}\")\n",
        "            return self.oov_vector\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Tree encoder with attention\n",
        "# -------------------------\n",
        "class TreeEncoderWithAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Encodes a tree structure by recursively encoding children and running self-attention\n",
        "    over the child vectors. Returns a vector of size `dim`.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int, num_heads: int = 4, device: str = 'cpu'):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.device = device\n",
        "        self.projectors = {}  # cache for per-input-dim linear projectors\n",
        "\n",
        "        # normalize and MHA\n",
        "        # MultiheadAttention in PyTorch expects embed_dim divisible by num_heads\n",
        "        if dim % num_heads != 0:\n",
        "            # choose nearest divisor-friendly heads\n",
        "            orig_heads = num_heads\n",
        "            num_heads = max(1, dim // (dim // max(1, num_heads)))\n",
        "            logger.warning(f\"Adjusted num_heads {orig_heads} -> {num_heads} for embed_dim={dim}\")\n",
        "\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.to(device)\n",
        "        logger.info(f\"Initialized TreeEncoderWithAttention dim={dim}, heads={num_heads} on {device}\")\n",
        "\n",
        "    def _ensure_dim(self, vec: torch.Tensor) -> torch.Tensor:\n",
        "        vec = vec.to(self.device)\n",
        "        if vec.shape[-1] == self.dim:\n",
        "            return vec\n",
        "        input_dim = vec.shape[-1]\n",
        "        key = f\"{input_dim}>{self.dim}\"\n",
        "        if key not in self.projectors:\n",
        "            logger.info(f\"Auto-projecting leaf vector: {input_dim} -> {self.dim}\")\n",
        "            self.projectors[key] = nn.Linear(input_dim, self.dim).to(self.device)\n",
        "        return self.projectors[key](vec)\n",
        "\n",
        "    def encode(self, node, get_vector_fn):\n",
        "        \"\"\"\n",
        "        Recursively encode node. node.is_leaf() and node.get_vector() are expected.\n",
        "        Returns torch.Tensor shape (dim,)\n",
        "        \"\"\"\n",
        "        # defensive checks\n",
        "        if node is None:\n",
        "            return torch.zeros(self.dim, device=self.device)\n",
        "\n",
        "        if getattr(node, \"is_leaf\", None) and node.is_leaf():\n",
        "            vec = get_vector_fn(node)\n",
        "            if vec is None:\n",
        "                return torch.zeros(self.dim, device=self.device)\n",
        "            if not isinstance(vec, torch.Tensor):\n",
        "                vec = torch.tensor(np.asarray(vec), dtype=torch.float32, device=self.device)\n",
        "            return self._ensure_dim(vec)\n",
        "\n",
        "        # gather child vectors\n",
        "        vectors = []\n",
        "        for child in getattr(node, \"children\", []):\n",
        "            try:\n",
        "                vec = self.encode(child, get_vector_fn)\n",
        "            except Exception as e:\n",
        "                logger.debug(f\"child encode failed: {e}\")\n",
        "                vec = torch.zeros(self.dim, device=self.device)\n",
        "            if vec is not None:\n",
        "                vectors.append(vec)\n",
        "\n",
        "        if not vectors:\n",
        "            return torch.zeros(self.dim, device=self.device)\n",
        "\n",
        "        # stack into shape (batch=1, seq_len, embed_dim) for batch_first attention\n",
        "        stacked = torch.stack(vectors, dim=0).unsqueeze(0)  # [1, seq_len, dim]\n",
        "        attn_output, _ = self.attention(stacked, stacked, stacked)  # returns [1, seq_len, dim]\n",
        "        pooled = attn_output.mean(dim=1).squeeze(0)  # [dim]\n",
        "        return self.norm(pooled)\n",
        "\n",
        "    def forward(self, node, get_vector_fn=lambda n: n.get_vector()):\n",
        "        return self.encode(node, get_vector_fn)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# TLite modules (V4, V5, V6)\n",
        "# -------------------------\n",
        "class TLiteV4_SearchEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Small MLP that reduces embedding to a non-negative score.\n",
        "    Expects input vector size `expected_dim` (if not, will project).\n",
        "    \"\"\"\n",
        "    def __init__(self, dim=50, hidden_dim=256, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.expected_dim = dim\n",
        "        self.device = device\n",
        "        self.projector = None\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Softplus()\n",
        "        )\n",
        "        self.to(device)\n",
        "        logger.info(f\"Initialized TLiteV4_SearchEncoder dim={dim} on {device}\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = x.to(self.device)\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(0)  # make batch\n",
        "        if x.shape[-1] != self.expected_dim:\n",
        "            if self.projector is None:\n",
        "                self.projector = InputProjector(x.shape[-1], self.expected_dim).to(self.device)\n",
        "                logger.info(f\"Created projector for TLiteV4: {x.shape[-1]} -> {self.expected_dim}\")\n",
        "            x = self.projector(x)\n",
        "        out = self.encoder(x)  # [batch, 1]\n",
        "        return out.squeeze(-1)  # [batch]\n",
        "\n",
        "\n",
        "class TLiteV5_ReasoningModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual stack reasoning head: input -> depth of residual blocks -> scalar head.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim=50, hidden_dim=128, depth=4, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.expected_dim = dim\n",
        "        self.device = device\n",
        "        self.projector = None\n",
        "        layers = []\n",
        "        for _ in range(depth):\n",
        "            layers.append(nn.Sequential(\n",
        "                nn.LayerNorm(dim),\n",
        "                nn.Linear(dim, hidden_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(hidden_dim, dim)\n",
        "            ))\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "        self.final_norm = nn.LayerNorm(dim)\n",
        "        self.head = nn.Sequential(nn.Linear(dim, 1), nn.Softplus())\n",
        "        self.to(device)\n",
        "        logger.info(f\"Initialized TLiteV5_ReasoningModule dim={dim}, depth={depth} on {device}\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = x.to(self.device)\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "        if x.shape[-1] != self.expected_dim:\n",
        "            if self.projector is None:\n",
        "                self.projector = InputProjector(x.shape[-1], self.expected_dim).to(self.device)\n",
        "                logger.info(f\"Created projector for TLiteV5: {x.shape[-1]} -> {self.expected_dim}\")\n",
        "            x = self.projector(x)\n",
        "        # residual stack\n",
        "        for layer in self.layers:\n",
        "            x = x + layer(x)\n",
        "        x = self.final_norm(x)\n",
        "        out = self.head(x)\n",
        "        return out.squeeze(-1)  # [batch]\n",
        "\n",
        "\n",
        "class TLiteExpert(nn.Module):\n",
        "    def __init__(self, dim=50, hidden_dim=64, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
        "        self.to(device)\n",
        "        logger.info(f\"Initialized TLiteExpert dim={dim} on {device}\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = x.to(self.device)\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "            squeezed = True\n",
        "        else:\n",
        "            squeezed = False\n",
        "        y = self.norm(x)\n",
        "        y = F.gelu(self.fc1(y))\n",
        "        y = self.fc2(y)\n",
        "        if squeezed:\n",
        "            return y.squeeze(0)\n",
        "        return y\n",
        "\n",
        "\n",
        "class TLiteRouter(nn.Module):\n",
        "    def __init__(self, dim=50, num_experts=8, top_k=2, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.gate = nn.Linear(dim, num_experts)\n",
        "        self.num_experts = num_experts\n",
        "        self.top_k = top_k\n",
        "        self.device = device\n",
        "        self.to(device)\n",
        "        logger.info(f\"Initialized TLiteRouter num_experts={num_experts}, top_k={top_k} on {device}\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        x: [batch, dim]\n",
        "        returns topk_indices [batch, top_k], topk_weights [batch, top_k]\n",
        "        \"\"\"\n",
        "        x = x.to(self.device)\n",
        "        logits = self.gate(x)  # [batch, num_experts]\n",
        "        topk_scores, topk_indices = torch.topk(logits, k=self.top_k, dim=-1)  # both [batch, top_k]\n",
        "        topk_weights = F.softmax(topk_scores, dim=-1)  # [batch, top_k]\n",
        "        return topk_indices, topk_weights\n",
        "\n",
        "\n",
        "class TLiteV6(nn.Module):\n",
        "    \"\"\"\n",
        "    Mixture of small experts. Input: [batch, dim] -> produces scalar per batch.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim=48, hidden_dim=64, num_experts=8, top_k=2, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.num_experts = num_experts\n",
        "        self.top_k = top_k\n",
        "        # experts return [batch, dim]\n",
        "        self.experts = nn.ModuleList([TLiteExpert(dim, hidden_dim, device) for _ in range(num_experts)])\n",
        "        self.router = TLiteRouter(dim, num_experts, top_k, device)\n",
        "        self.final_head = nn.Sequential(nn.Linear(dim, 1), nn.Softplus())\n",
        "        self.to(device)\n",
        "        logger.info(f\"Initialized TLiteV6 num_experts={num_experts} on {device}\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        x: [batch, dim] or [dim] (will be batched)\n",
        "        returns: [batch] scalar scores\n",
        "        \"\"\"\n",
        "        x = x.to(self.device)\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "            squeezed = True\n",
        "        else:\n",
        "            squeezed = False\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        topk_indices, topk_weights = self.router(x)  # [batch, top_k], [batch, top_k]\n",
        "\n",
        "        # aggregate expert outputs\n",
        "        # prepare a container with same shape as x to accumulate weighted expert outputs\n",
        "        out = torch.zeros_like(x, device=self.device)  # [batch, dim]\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for i in range(self.top_k):\n",
        "                idx = int(topk_indices[b, i].item())\n",
        "                weight = topk_weights[b, i]\n",
        "                expert_out = self.experts[idx](x[b])  # expert_out: [dim] or [1,dim] squeezed to [dim]\n",
        "                if expert_out.dim() == 1:\n",
        "                    expert_vec = expert_out\n",
        "                else:\n",
        "                    expert_vec = expert_out.squeeze(0)\n",
        "                out[b] += weight * expert_vec\n",
        "\n",
        "        final = self.final_head(out)  # [batch,1]\n",
        "        final = final.squeeze(-1)  # [batch]\n",
        "        if squeezed:\n",
        "            return final.squeeze(0)\n",
        "        return final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VwtZUTMbZbM",
        "outputId": "3d65ee39-7cf1-462c-a7b8-a72f86f99b70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/DecoderV1.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/DecoderV1.py\n",
        "import logging\n",
        "from typing import Dict, Any, List, Tuple, Optional\n",
        "\n",
        "from TreeBuilderV2 import TreeBuilderV2\n",
        "from TLiteComponents import TreeEncoderWithAttention, TLiteV6\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class DecoderV1:\n",
        "    \"\"\"\n",
        "    DecoderV1\n",
        "    ----------\n",
        "    • Takes a reasoning tree and produces a user-facing Ayurvedic explanation\n",
        "    • Adaptive:\n",
        "        - Simple queries -> short summary\n",
        "        - Complex / multi-symptom -> more detailed explanation\n",
        "    • Can use extra follow-up context:\n",
        "        context = {\n",
        "            \"sleep\": \"light\" | \"disturbed\" | \"deep\" | \"normal\" | None,\n",
        "            \"appetite\": \"strong\" | \"normal\" | \"low\" | None,\n",
        "            \"weather\": \"worse_cold\" | \"worse_heat\" | \"worse_damp\" | \"none\" | None\n",
        "        }\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim=48, device=\"cpu\", use_smoother=True):\n",
        "        self.encoder = TreeEncoderWithAttention(dim=dim, device=device)\n",
        "        self.smoother = TLiteV6(dim=dim, device=device) if use_smoother else None\n",
        "        self.device = device\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # Helper: Dosha-specific recommendations (short, safe)\n",
        "    # ---------------------------------------------------------\n",
        "    def _dosha_recommendation(self, main_dosha: Optional[str]) -> str:\n",
        "        d = (main_dosha or \"\").lower()\n",
        "\n",
        "        if d == \"vata\":\n",
        "            return (\n",
        "                \"Favour warm, freshly cooked meals, regular routines, and calming practices \"\n",
        "                \"like gentle stretching, slow breathing, or quiet time to help settle vata.\"\n",
        "            )\n",
        "        if d == \"pitta\":\n",
        "            return (\n",
        "                \"Cooling, less oily and less spicy meals, staying well hydrated, and taking small \"\n",
        "                \"pauses to relax can help settle pitta.\"\n",
        "            )\n",
        "        if d == \"kapha\":\n",
        "            return (\n",
        "                \"Light, warm meals, gentle daily movement, and avoiding very heavy, cold, or overly sweet \"\n",
        "                \"foods can help reduce kapha heaviness.\"\n",
        "            )\n",
        "\n",
        "        # generic fallback\n",
        "        return (\n",
        "            \"A balanced, freshly cooked diet, a steady daily routine, and simple calming practices \"\n",
        "            \"are generally helpful to support balance.\"\n",
        "        )\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # Micro-suggestions based on symptoms + context\n",
        "    # ---------------------------------------------------------\n",
        "    def _micro_suggestions(\n",
        "        self,\n",
        "        flags: Dict[str, bool],\n",
        "        main_dosha: Optional[str],\n",
        "        context: Dict[str, Optional[str]],\n",
        "    ) -> List[str]:\n",
        "        out: List[str] = []\n",
        "\n",
        "        # Symptom-based\n",
        "        if flags.get(\"has_dryness\"):\n",
        "            out.append(\"Gentle oil application before bath (such as sesame or coconut oil) may help dryness.\")\n",
        "        if flags.get(\"has_heaviness\") or flags.get(\"has_slowness\"):\n",
        "            out.append(\"Prefer lighter meals at night and include a short walk after eating.\")\n",
        "        if flags.get(\"has_bloating\") or flags.get(\"has_constip\"):\n",
        "            out.append(\"Warm water in the morning and including cooked vegetables and lentils may support digestion.\")\n",
        "        if flags.get(\"has_acidity\") or flags.get(\"has_heat\") or flags.get(\"has_fever\"):\n",
        "            out.append(\"Reduce very spicy, oily, or deep-fried foods and avoid lying down immediately after meals.\")\n",
        "        if flags.get(\"has_anxiety\"):\n",
        "            out.append(\"Short calming practices like slow breathing or a brief walk in fresh air can help settle the mind.\")\n",
        "\n",
        "        # Context-based\n",
        "        sleep = (context.get(\"sleep\") or \"\").lower()\n",
        "        if sleep in (\"light\", \"disturbed\"):\n",
        "            out.append(\"Try a simple evening wind-down with less screen time and light, warm dinner to support sleep.\")\n",
        "        elif sleep == \"deep\":\n",
        "            # deep sleep → often kapha; no need for special warning\n",
        "            pass\n",
        "\n",
        "        appetite = (context.get(\"appetite\") or \"\").lower()\n",
        "        if appetite == \"low\":\n",
        "            out.append(\"Small, warm, mildly spiced meals like simple moong dal can be easier to digest.\")\n",
        "        elif appetite == \"strong\":\n",
        "            out.append(\"Even with strong appetite, avoid overeating and prefer freshly cooked, not very spicy food.\")\n",
        "\n",
        "        weather = (context.get(\"weather\") or \"\").lower()\n",
        "        if weather == \"worse_cold\":\n",
        "            out.append(\"Keep warm, avoid excess cold or raw foods, and protect yourself from cold wind.\")\n",
        "        elif weather == \"worse_heat\":\n",
        "            out.append(\"Stay hydrated, avoid direct midday sun, and reduce very heating foods.\")\n",
        "        elif weather == \"worse_damp\":\n",
        "            out.append(\"In damp weather, prefer warm, dry foods and gentle movement to reduce heaviness.\")\n",
        "\n",
        "        # Dosha-generic if nothing else\n",
        "        if not out:\n",
        "            out.append(self._dosha_recommendation(main_dosha))\n",
        "\n",
        "        # de-duplicate while preserving order\n",
        "        seen = set()\n",
        "        uniq = []\n",
        "        for s in out:\n",
        "            if s in seen:\n",
        "                continue\n",
        "            seen.add(s)\n",
        "            uniq.append(s)\n",
        "        return uniq[:6]  # keep it concise\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # Clarity, severity, etc.\n",
        "    # ---------------------------------------------------------\n",
        "    def _severity_from_scores(self, symptom_count: int, max_dosha_score: int) -> str:\n",
        "        if symptom_count >= 4 or max_dosha_score >= 6:\n",
        "            return \"High\"\n",
        "        if symptom_count >= 2 or max_dosha_score >= 3:\n",
        "            return \"Moderate\"\n",
        "        if symptom_count >= 1:\n",
        "            return \"Mild\"\n",
        "        return \"Unknown\"\n",
        "\n",
        "    def _clarity_score(self, symptom_count: int, main_dosha: Optional[str], severity: str) -> int:\n",
        "        \"\"\"\n",
        "        Approx 0..10. Higher = clearer pattern.\n",
        "        We keep it strict so vague single-line things tend to get follow-up.\n",
        "        \"\"\"\n",
        "        score = 0\n",
        "\n",
        "        if main_dosha:\n",
        "            score += 3\n",
        "\n",
        "        if symptom_count >= 1:\n",
        "            score += 1\n",
        "        if symptom_count >= 3:\n",
        "            score += 2\n",
        "\n",
        "        if severity == \"Moderate\":\n",
        "            score += 1\n",
        "        elif severity == \"High\":\n",
        "            score += 2\n",
        "\n",
        "        return max(0, min(10, score))\n",
        "\n",
        "    def _default_followups(self) -> List[str]:\n",
        "        return [\n",
        "            \"How is your sleep—light, disturbed, or deep?\",\n",
        "            \"How is your appetite—strong, normal, or low?\",\n",
        "            \"Do your symptoms feel worse in cold, heat, or damp weather?\",\n",
        "        ]\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # Core analysis with optional context\n",
        "    # ---------------------------------------------------------\n",
        "    def _analyze_tree(\n",
        "        self,\n",
        "        root,\n",
        "        context: Optional[Dict[str, Optional[str]]] = None,\n",
        "    ) -> Tuple[str, Dict[str, Any]]:\n",
        "        if context is None:\n",
        "            context = {}\n",
        "        ctx_sleep = (context.get(\"sleep\") or \"\").lower()\n",
        "        ctx_app = (context.get(\"appetite\") or \"\").lower()\n",
        "        ctx_weather = (context.get(\"weather\") or \"\").lower()\n",
        "\n",
        "        # flatten path to tokens\n",
        "        builder = TreeBuilderV2(mode=\"binary\")\n",
        "        path = builder.trace_dfs(root)\n",
        "        tokens = [str(x).lower() for x in path]\n",
        "        tset = set(tokens)\n",
        "\n",
        "        # ---- Symptom flags from text ----\n",
        "        has_dryness   = any(w in tset for w in [\"dry\", \"dryness\", \"rough\"])\n",
        "        has_skin      = \"skin\" in tset\n",
        "        has_joints    = any(w in tset for w in [\"joint\", \"joints\", \"knee\", \"elbow\", \"pain\"])\n",
        "        has_heaviness = any(w in tset for w in [\"heavy\", \"heaviness\"])\n",
        "        has_slowness  = any(w in tset for w in [\"slow\", \"slowness\", \"sluggish\", \"lazy\"])\n",
        "        has_fever     = any(w in tset for w in [\"fever\", \"feverish\", \"temperature\"])\n",
        "        has_heat      = any(w in tset for w in [\"heat\", \"burning\", \"hot\"])\n",
        "        has_acidity   = any(w in tset for w in [\"acidity\", \"acid\", \"heartburn\", \"reflux\"])\n",
        "        has_bloating  = any(w in tset for w in [\"bloating\", \"gas\", \"gassy\"])\n",
        "        has_constip   = any(w in tset for w in [\"constipation\", \"constipated\"])\n",
        "        has_anxiety   = any(w in tset for w in [\"anxious\", \"anxiety\", \"worry\", \"overthinking\", \"restless\"])\n",
        "        has_sleepy    = any(w in tset for w in [\"sleepy\", \"drowsy\", \"tired\"])\n",
        "        has_digestion = any(w in tset for w in [\"digestion\", \"digest\", \"stomach\", \"appetite\", \"hungry\"])\n",
        "        has_energy    = any(w in tset for w in [\"energy\", \"weak\", \"fatigue\", \"tired\"])\n",
        "\n",
        "        flags = {\n",
        "            \"has_dryness\": has_dryness,\n",
        "            \"has_skin\": has_skin,\n",
        "            \"has_joints\": has_joints,\n",
        "            \"has_heaviness\": has_heaviness,\n",
        "            \"has_slowness\": has_slowness,\n",
        "            \"has_fever\": has_fever,\n",
        "            \"has_heat\": has_heat,\n",
        "            \"has_acidity\": has_acidity,\n",
        "            \"has_bloating\": has_bloating,\n",
        "            \"has_constip\": has_constip,\n",
        "            \"has_anxiety\": has_anxiety,\n",
        "            \"has_sleepy\": has_sleepy,\n",
        "            \"has_digestion\": has_digestion,\n",
        "            \"has_energy\": has_energy,\n",
        "        }\n",
        "\n",
        "        # ---- Dosha scores base ----\n",
        "        vata = 0\n",
        "        pitta = 0\n",
        "        kapha = 0\n",
        "\n",
        "        # Vata\n",
        "        if has_dryness: vata += 2\n",
        "        if has_joints: vata += 1\n",
        "        if has_constip: vata += 2\n",
        "        if has_anxiety: vata += 2\n",
        "        if has_bloating: vata += 1\n",
        "        if has_energy and \"weak\" in tset: vata += 1\n",
        "\n",
        "        # Pitta\n",
        "        if has_heat or has_fever: pitta += 2\n",
        "        if has_acidity: pitta += 2\n",
        "        if has_anxiety and not has_constip: pitta += 1  # intensity\n",
        "\n",
        "        # Kapha\n",
        "        if has_heaviness: kapha += 2\n",
        "        if has_slowness: kapha += 1\n",
        "        if has_sleepy: kapha += 2\n",
        "        if has_bloating: kapha += 1\n",
        "        if has_energy and \"low\" in tset: kapha += 1\n",
        "\n",
        "        # ---- Apply context influence ----\n",
        "        if ctx_sleep in (\"light\", \"disturbed\"):\n",
        "            vata += 1\n",
        "        elif ctx_sleep == \"deep\":\n",
        "            kapha += 1\n",
        "\n",
        "        if ctx_app == \"low\":\n",
        "            vata += 1\n",
        "            kapha += 1\n",
        "        elif ctx_app == \"strong\":\n",
        "            pitta += 1\n",
        "\n",
        "        if ctx_weather == \"worse_cold\":\n",
        "            vata += 1\n",
        "        elif ctx_weather == \"worse_heat\":\n",
        "            pitta += 1\n",
        "        elif ctx_weather == \"worse_damp\":\n",
        "            kapha += 1\n",
        "\n",
        "        scores = {\"Vata\": vata, \"Pitta\": pitta, \"Kapha\": kapha}\n",
        "        ranked = sorted(\n",
        "            [d for d in scores.items() if d[1] > 0],\n",
        "            key=lambda x: x[1],\n",
        "            reverse=True,\n",
        "        )\n",
        "        main_dosha = ranked[0][0] if ranked else None\n",
        "        second_dosha = ranked[1][0] if len(ranked) > 1 and ranked[1][1] > 0 else None\n",
        "        max_score = ranked[0][1] if ranked else 0\n",
        "\n",
        "        # ---- Build symptom phrases ----\n",
        "        symptoms: List[str] = []\n",
        "\n",
        "        if has_dryness and has_skin and not has_joints:\n",
        "            symptoms.append(\"dryness of the skin\")\n",
        "        if has_dryness and has_skin and has_joints:\n",
        "            symptoms.append(\"dry skin with joint discomfort\")\n",
        "        elif has_joints and not has_dryness:\n",
        "            symptoms.append(\"discomfort or cracking in the joints\")\n",
        "\n",
        "        if has_heaviness and has_digestion and (has_slowness or \"slow\" in tset):\n",
        "            symptoms.append(\"heaviness with slow or sluggish digestion\")\n",
        "        elif has_heaviness and has_digestion:\n",
        "            symptoms.append(\"heaviness around digestion or after eating\")\n",
        "        elif has_heaviness:\n",
        "            symptoms.append(\"a general feeling of heaviness in the body\")\n",
        "\n",
        "        if has_fever:\n",
        "            symptoms.append(\"a feverish or heated sensation\")\n",
        "        if has_heat and not has_fever:\n",
        "            symptoms.append(\"a warm or burning sensation in the body or chest\")\n",
        "\n",
        "        if has_bloating:\n",
        "            symptoms.append(\"bloating or gas\")\n",
        "        if has_constip:\n",
        "            symptoms.append(\"a tendency toward constipation\")\n",
        "        if has_anxiety:\n",
        "            symptoms.append(\"an anxious or overactive mind\")\n",
        "        if has_sleepy:\n",
        "            symptoms.append(\"daytime sleepiness or low alertness\")\n",
        "        if has_energy:\n",
        "            symptoms.append(\"low or fluctuating energy\")\n",
        "\n",
        "        # Context-based symptoms\n",
        "        if ctx_sleep in (\"light\", \"disturbed\"):\n",
        "            symptoms.append(\"light or disturbed sleep\")\n",
        "        elif ctx_sleep == \"deep\":\n",
        "            symptoms.append(\"deep or heavy sleep\")\n",
        "\n",
        "        if ctx_app == \"low\":\n",
        "            symptoms.append(\"reduced or weaker appetite\")\n",
        "        elif ctx_app == \"strong\":\n",
        "            symptoms.append(\"strong or sharp appetite\")\n",
        "\n",
        "        if ctx_weather == \"worse_cold\":\n",
        "            symptoms.append(\"symptoms that feel worse in cold\")\n",
        "        elif ctx_weather == \"worse_heat\":\n",
        "            symptoms.append(\"symptoms that feel worse in heat\")\n",
        "        elif ctx_weather == \"worse_damp\":\n",
        "            symptoms.append(\"symptoms that feel worse in damp or humid weather\")\n",
        "\n",
        "        # Count major categories (excluding purely contextual ones if needed)\n",
        "        symptom_count = len(symptoms)\n",
        "\n",
        "        # ---- Severity + clarity ----\n",
        "        severity = self._severity_from_scores(symptom_count, max_score)\n",
        "        clarity = self._clarity_score(symptom_count, main_dosha, severity)\n",
        "\n",
        "        # ---- Build explanation text (simple vs complex) ----\n",
        "        simple_mode = symptom_count <= 2\n",
        "\n",
        "        lines: List[str] = []\n",
        "\n",
        "        # Symptom narrative\n",
        "        if symptoms:\n",
        "            if simple_mode:\n",
        "                lines.append(\n",
        "                    \"From your description, you are experiencing \" +\n",
        "                    \", \".join(symptoms) + \".\"\n",
        "                )\n",
        "            else:\n",
        "                lines.append(\n",
        "                    \"You are experiencing multiple symptoms, including \" +\n",
        "                    \", \".join(symptoms) + \".\"\n",
        "                )\n",
        "\n",
        "        # Dosha explanation\n",
        "        if main_dosha:\n",
        "            if simple_mode:\n",
        "                if second_dosha:\n",
        "                    lines.append(\n",
        "                        f\"These features are most consistent with a {main_dosha.lower()} imbalance, \"\n",
        "                        f\"with some influence of {second_dosha.lower()}.\"\n",
        "                    )\n",
        "                else:\n",
        "                    lines.append(\n",
        "                        f\"These features are most consistent with a {main_dosha.lower()} dosha imbalance.\"\n",
        "                    )\n",
        "            else:\n",
        "                extra_clause = \"\"\n",
        "                pair = (main_dosha, second_dosha)\n",
        "                if second_dosha:\n",
        "                    if pair in ((\"Vata\", \"Pitta\"), (\"Pitta\", \"Vata\")):\n",
        "                        extra_clause = \" combining dryness or nervousness with some heat or intensity.\"\n",
        "                    elif pair in ((\"Pitta\", \"Kapha\"), (\"Kapha\", \"Pitta\")):\n",
        "                        extra_clause = \" often appearing as heat together with heaviness or sluggishness.\"\n",
        "                    elif pair in ((\"Vata\", \"Kapha\"), (\"Kapha\", \"Vata\")):\n",
        "                        extra_clause = \" mixing light, changeable qualities with heavier, slower ones.\"\n",
        "                    lines.append(\n",
        "                        f\"Taken together, these signs point mainly towards a {main_dosha.lower()} imbalance \"\n",
        "                        f\"with additional influence of {second_dosha.lower()}{extra_clause}\".rstrip(\".\") + \".\"\n",
        "                    )\n",
        "                else:\n",
        "                    lines.append(\n",
        "                        f\"Taken together, these signs point mainly towards a {main_dosha.lower()} dosha imbalance.\"\n",
        "                    )\n",
        "        else:\n",
        "            lines.append(\n",
        "                \"These symptoms do not clearly match a single dosha pattern, \"\n",
        "                \"but they can still be supported with simple diet and lifestyle adjustments.\"\n",
        "            )\n",
        "\n",
        "        # Generic dosha-specific recommendation\n",
        "        lines.append(self._dosha_recommendation(main_dosha))\n",
        "\n",
        "        if not any(symptoms):\n",
        "            # fully vague case fallback\n",
        "            text = (\n",
        "                \"These symptoms do not clearly match a single dosha pattern. \"\n",
        "                \"A balanced, freshly cooked diet, a steady daily routine, and simple calming practices \"\n",
        "                \"are generally helpful to support balance.\"\n",
        "            )\n",
        "        else:\n",
        "            text = \" \".join(lines)\n",
        "\n",
        "        # micro suggestions\n",
        "        micro = self._micro_suggestions(flags, main_dosha, context)\n",
        "\n",
        "        meta = {\n",
        "            \"primary_dosha\": main_dosha,\n",
        "            \"secondary_dosha\": second_dosha,\n",
        "            \"severity\": severity,\n",
        "            \"symptoms\": symptoms,\n",
        "            \"micro_suggestions\": micro,\n",
        "            \"followups\": self._default_followups(),\n",
        "            \"clarity_score\": clarity,\n",
        "        }\n",
        "\n",
        "        # cleanup text\n",
        "        text = \" \".join(text.split()).strip()\n",
        "        if not text.endswith(\".\"):\n",
        "            text += \".\"\n",
        "\n",
        "        return text, meta\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # Simple text-only interface\n",
        "    # ---------------------------------------------------------\n",
        "    def explain(self, root_node, context: Optional[Dict[str, Optional[str]]] = None) -> str:\n",
        "        if root_node is None:\n",
        "            return \"I could not analyze your symptoms clearly.\"\n",
        "        text, _ = self._analyze_tree(root_node, context=context)\n",
        "        return text\n",
        "\n",
        "    def decode_tree(self, root_node, context: Optional[Dict[str, Optional[str]]] = None) -> str:\n",
        "        return self.explain(root_node, context=context)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y60sPq2KTCSN"
      },
      "source": [
        "# Phase-4 : RLTeacher prototype\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEaZAEhvUSB_"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class RLTeacher:\n",
        "    def __init__(self):\n",
        "        self.history = []\n",
        "\n",
        "    # Step 1: Propose candidate actions\n",
        "    def propose_actions(self, anchors):\n",
        "        actions = []\n",
        "        for anchor in anchors:\n",
        "            actions.append({\n",
        "                \"type\": \"split\",\n",
        "                \"target\": anchor[\"id\"],\n",
        "                \"anchor_type\": anchor[\"type\"]\n",
        "            })\n",
        "            actions.append({\n",
        "                \"type\": \"merge\",\n",
        "                \"target\": anchor[\"id\"],\n",
        "                \"anchor_type\": anchor[\"type\"]\n",
        "            })\n",
        "        return actions\n",
        "\n",
        "    # Step 2: Evaluate actions with a simple critic\n",
        "    def evaluate_actions(self, state, actions):\n",
        "        scored = []\n",
        "        for action in actions:\n",
        "            # Dummy critic: random quality for now\n",
        "            est_quality = random.uniform(0, 1)\n",
        "            score = {\n",
        "                \"est_pos\": est_quality,\n",
        "                \"est_neg\": 1 - est_quality,\n",
        "                \"est_quality\": est_quality\n",
        "            }\n",
        "            scored.append({\"action\": action, \"score\": score})\n",
        "        return scored\n",
        "\n",
        "    # Step 3: Select best action (greedy)\n",
        "    def select_action(self, scored_actions):\n",
        "        if not scored_actions:\n",
        "            return None, None\n",
        "        sorted_actions = sorted(\n",
        "            scored_actions,\n",
        "            key=lambda x: x[\"score\"].get(\"est_quality\", 0.0),\n",
        "            reverse=True\n",
        "        )\n",
        "        best = sorted_actions[0]\n",
        "        return best[\"action\"], best[\"score\"]\n",
        "\n",
        "    # Step 4: Apply action (stub)\n",
        "    def apply_action(self, state, action):\n",
        "        new_state = state.copy()\n",
        "        new_state[\"applied_action\"] = action[\"type\"]\n",
        "        new_state[\"applied_target\"] = action[\"target\"]\n",
        "        return new_state\n",
        "\n",
        "    # Step 5: Log decision\n",
        "    def log_decision(self, state, action, score, explanation):\n",
        "        self.history.append({\n",
        "            \"state\": state,\n",
        "            \"action\": action,\n",
        "            \"score\": score,\n",
        "            \"explanation\": explanation\n",
        "        })\n",
        "\n",
        "    # Step 6: Adaptive run loop\n",
        "    def run(self, snapshot, anchors, retrievals,\n",
        "            base_steps=2, max_steps=6, success_threshold=0.7):\n",
        "        steps_remaining = base_steps\n",
        "        current_state = snapshot.copy()\n",
        "\n",
        "        while steps_remaining > 0 and len(self.history) < max_steps:\n",
        "            actions = self.propose_actions(anchors)\n",
        "            scored = self.evaluate_actions(current_state, actions)\n",
        "            action, score = self.select_action(scored)\n",
        "\n",
        "            if not action:\n",
        "                break\n",
        "\n",
        "            new_state = self.apply_action(current_state, action)\n",
        "\n",
        "            # Simple explanation (stub)\n",
        "            explanation = (\n",
        "                f\"Applied {action['type']} on {action['target']} \"\n",
        "                f\"with quality {score['est_quality']:.2f}\"\n",
        "            )\n",
        "\n",
        "            self.log_decision(new_state, action, score, explanation)\n",
        "\n",
        "            # Adaptive adjustment\n",
        "            if score[\"est_quality\"] >= success_threshold:\n",
        "                steps_remaining += 1\n",
        "            else:\n",
        "                steps_remaining -= 1\n",
        "\n",
        "            current_state = new_state\n",
        "\n",
        "        return self.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrzNPX2o3Cxe"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FailureMemory:\n",
        "    \"\"\"Persistent memory for clustered failures and their solutions.\"\"\"\n",
        "    def __init__(self, filepath=\"failure_memory.json\"):\n",
        "        self.filepath = filepath\n",
        "        self.memory = self._load()\n",
        "\n",
        "    def _load(self):\n",
        "        if os.path.exists(self.filepath):\n",
        "            with open(self.filepath, \"r\") as f:\n",
        "                return json.load(f)\n",
        "        return {}\n",
        "\n",
        "    def _save(self):\n",
        "        with open(self.filepath, \"w\") as f:\n",
        "            json.dump(self.memory, f, indent=2)\n",
        "\n",
        "    def record_failure(self, signature, snapshot_dict, details):\n",
        "        \"\"\"\n",
        "        Record a failure into its cluster.\n",
        "        signature: str (stable hash of failure pattern)\n",
        "        snapshot_dict: dict from TreeSnapshot.to_dict()\n",
        "        details: dict with error_type, node_id, etc.\n",
        "        \"\"\"\n",
        "        cluster = self.memory.get(signature, {\n",
        "            \"signature\": signature,\n",
        "            \"problems\": [],\n",
        "            \"repeat_count\": 0,\n",
        "            \"impact_score\": 0.0,\n",
        "            \"solution\": None,\n",
        "            \"stats\": {\"last_seen\": None, \"resolved\": False, \"uses\": 0}\n",
        "        })\n",
        "\n",
        "        cluster[\"problems\"].append({\n",
        "            \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
        "            \"details\": details\n",
        "        })\n",
        "        cluster[\"repeat_count\"] += 1\n",
        "        cluster[\"stats\"][\"last_seen\"] = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
        "\n",
        "        # Compute impact score using snapshot metrics\n",
        "        eis = snapshot_dict.get(\"entropy\", 0.0)\n",
        "        depth = snapshot_dict.get(\"depth\", 1)\n",
        "        max_depth = max(1, snapshot_dict.get(\"node_count\", 1))\n",
        "        cluster[\"impact_score\"] = 0.5*(1 - depth/max_depth) + 0.3*math.log(1+cluster[\"repeat_count\"]) + 0.2*eis\n",
        "\n",
        "        self.memory[signature] = cluster\n",
        "        self._save()\n",
        "        return cluster\n",
        "\n",
        "    def update_solution(self, signature, natural_text, patch, quality, provenance):\n",
        "        \"\"\"Attach a verified solution to a cluster.\"\"\"\n",
        "        cluster = self.memory.get(signature)\n",
        "        if not cluster:\n",
        "            return None\n",
        "        cluster[\"solution\"] = {\n",
        "            \"natural_text\": natural_text,\n",
        "            \"patch\": patch,\n",
        "            \"quality\": quality,\n",
        "            \"provenance\": provenance\n",
        "        }\n",
        "        cluster[\"stats\"][\"resolved\"] = quality >= 0.6\n",
        "        self.memory[signature] = cluster\n",
        "        self._save()\n",
        "        return cluster\n",
        "\n",
        "    def reuse_solution(self, signature):\n",
        "        \"\"\"Fetch a stored solution if available.\"\"\"\n",
        "        cluster = self.memory.get(signature)\n",
        "        if cluster and cluster.get(\"solution\"):\n",
        "            cluster[\"stats\"][\"uses\"] += 1\n",
        "            self._save()\n",
        "            return cluster[\"solution\"]\n",
        "        return None\n",
        "\n",
        "    def get_penalty(self, signature):\n",
        "        \"\"\"Penalty scales with impact score instead of flat count.\"\"\"\n",
        "        cluster = self.memory.get(signature)\n",
        "        if not cluster:\n",
        "            return 0.0\n",
        "        return 0.5 * cluster[\"impact_score\"]  # tune factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlnQgQ4fNcrJ",
        "outputId": "952f1626-5109-429a-c444-86bcca4f1d75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing update_failure_memory.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile update_failure_memory.py\n",
        "import math\n",
        "import time\n",
        "\n",
        "def update_failure_memory(failure_memory, failure_sig, eis, proposed_fix=None):\n",
        "    \"\"\"\n",
        "    Update or create a failure cluster entry.\n",
        "    Returns: (updated_failure_memory, outcome, lesson_flag, insight_flag)\n",
        "    \"\"\"\n",
        "\n",
        "    now = time.time()\n",
        "    lesson_flag, insight_flag = 0, 0\n",
        "\n",
        "    if failure_sig in failure_memory:\n",
        "        # --- Existing cluster ---\n",
        "        cluster = failure_memory[failure_sig]\n",
        "        cluster[\"repeat_count\"] += 1\n",
        "        cluster[\"stats\"][\"last_seen\"] = now\n",
        "\n",
        "        # Compute Impact Score (IS)\n",
        "        is_val = 0.5*(1 - cluster.get(\"tree_score\", 0.5)) \\\n",
        "                 + 0.3*math.log(1+cluster[\"repeat_count\"]) \\\n",
        "                 + 0.2*eis\n",
        "        cluster[\"impact_score\"] = round(is_val, 3)\n",
        "\n",
        "        # Try to resolve with proposed fix\n",
        "        if proposed_fix:\n",
        "            quality = proposed_fix.get(\"quality\", 0.0)\n",
        "            thresh = max(0.5, 0.7 - 0.2*eis)\n",
        "            if quality > thresh:\n",
        "                cluster[\"solution\"] = proposed_fix\n",
        "                cluster[\"resolved\"] = True\n",
        "                insight_flag = 1   # Purple ticket trigger\n",
        "                outcome = \"resolved_cluster\"\n",
        "            else:\n",
        "                outcome = \"repeat_fail\"\n",
        "                lesson_flag = 1    # Yellow ticket trigger\n",
        "        else:\n",
        "            outcome = \"repeat_fail\"\n",
        "            lesson_flag = 1\n",
        "\n",
        "    else:\n",
        "        # --- New cluster ---\n",
        "        failure_memory[failure_sig] = {\n",
        "            \"signature\": failure_sig,\n",
        "            \"repeat_count\": 1,\n",
        "            \"impact_score\": 0.2*eis,\n",
        "            \"solution\": None,\n",
        "            \"resolved\": False,\n",
        "            \"stats\": {\"created\": now, \"last_seen\": now}\n",
        "        }\n",
        "        outcome = \"unique_fail\"\n",
        "        lesson_flag = 1\n",
        "\n",
        "    return failure_memory, outcome, lesson_flag, insight_flag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UVMQIvNK5ExF"
      },
      "outputs": [],
      "source": [
        "\n",
        "from rl_critic import score_candidate\n",
        "from TreeSnapshot import TreeSnapshot\n",
        "\n",
        "class DualValenceCritic:\n",
        "    \"\"\"\n",
        "    Dual‑valence critic that uses TreeSnapshot features + rl_critic.score_candidate.\n",
        "    Adjusts quality with failure memory penalties.\n",
        "    \"\"\"\n",
        "    def __init__(self, failure_memory, pos_weight=1.0, neg_weight=1.0):\n",
        "        self.failure_memory = failure_memory\n",
        "        self.pos_weight = pos_weight\n",
        "        self.neg_weight = neg_weight\n",
        "\n",
        "    def optimistic_sim(self, snapshot_dict, action):\n",
        "        \"\"\"\n",
        "        Optimistic sim: use rl_critic.score_candidate with normal weights.\n",
        "        \"\"\"\n",
        "        return score_candidate(snapshot_dict, action)[\"est_pos\"]\n",
        "\n",
        "    def pessimistic_sim(self, snapshot_dict, action):\n",
        "        \"\"\"\n",
        "        Pessimistic sim: invert perspective by treating negatives more heavily.\n",
        "        \"\"\"\n",
        "        return score_candidate(snapshot_dict, action)[\"est_neg\"]\n",
        "\n",
        "    def evaluate(self, tree, action):\n",
        "        \"\"\"\n",
        "        Evaluate an action on a given tree snapshot.\n",
        "        \"\"\"\n",
        "        # Build snapshot features\n",
        "        snapshot = TreeSnapshot(tree).to_dict()\n",
        "\n",
        "        # Run dual‑valence sims\n",
        "        pos = self.optimistic_sim(snapshot, action)\n",
        "        neg = self.pessimistic_sim(snapshot, action)\n",
        "        raw_quality = self.pos_weight * pos - self.neg_weight * neg\n",
        "\n",
        "        # Action signature for memory\n",
        "        signature = f\"{action.get('type','unknown')}_{action.get('target','na')}\"\n",
        "        penalty = self.failure_memory.get_penalty(signature)\n",
        "        adjusted_quality = raw_quality - penalty\n",
        "\n",
        "        return {\n",
        "            \"action_signature\": signature,\n",
        "            \"pos_score\": pos,\n",
        "            \"neg_score\": neg,\n",
        "            \"raw_quality\": raw_quality,\n",
        "            \"penalty\": penalty,\n",
        "            \"adjusted_quality\": adjusted_quality,\n",
        "            \"snapshot\": snapshot\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUet8Xmk3DfW"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "\n",
        "class PolicyModule:\n",
        "    \"\"\"\n",
        "    Chooses actions based on critic scores, curriculum thresholds, and exploration.\n",
        "    - Curriculum gating: structural always; rule/generative unlocked per domain thresholds.\n",
        "    - Curriculum pruning: cap at max_actions (default 32).\n",
        "    - Selection: greedy by max pos_score, then adjusted_quality; ε-greedy optional.\n",
        "    - Rationale includes thresholds, failure-memory effects, and domain modifiers.\n",
        "    \"\"\"\n",
        "    def __init__(self, domain_thresholds, epsilon=0.0, max_actions=32, domain_modifiers=None):\n",
        "        self.domain_thresholds = domain_thresholds\n",
        "        self.epsilon = epsilon\n",
        "        self.max_actions = max_actions\n",
        "        # Domain modifiers: small adjustments to adjusted_quality\n",
        "        # e.g. {\"math\": +0.05, \"code\": +0.02, \"science\": 0.0, \"chess\": -0.01}\n",
        "        self.domain_modifiers = domain_modifiers or {}\n",
        "\n",
        "    def _curriculum_allowed(self, domain, stability_score, action_type):\n",
        "        th = self.domain_thresholds.get(domain, {})\n",
        "        if action_type == \"structural\":\n",
        "            return True\n",
        "        if action_type == \"rule\":\n",
        "            return stability_score >= th.get(\"rule\", 1.0)\n",
        "        if action_type == \"generative\":\n",
        "            return stability_score >= th.get(\"generative\", 1.0)\n",
        "        return False\n",
        "\n",
        "    def filter_by_curriculum(self, domain, stability_score, scored_candidates):\n",
        "        allowed, locked_out = [], []\n",
        "        for c in scored_candidates:\n",
        "            a_type = c[\"action\"].get(\"type\", \"structural\")\n",
        "            if self._curriculum_allowed(domain, stability_score, a_type):\n",
        "                allowed.append(c)\n",
        "            else:\n",
        "                locked_out.append(c)\n",
        "\n",
        "        # 🔒 Curriculum pruning: cap at max_actions\n",
        "        if len(allowed) > self.max_actions:\n",
        "            allowed = sorted(\n",
        "                allowed,\n",
        "                key=lambda c: (c[\"scores\"][\"pos_score\"], c[\"scores\"][\"adjusted_quality\"]),\n",
        "                reverse=True\n",
        "            )[:self.max_actions]\n",
        "\n",
        "        return allowed, locked_out\n",
        "\n",
        "    def _apply_domain_modifier(self, domain, score):\n",
        "        \"\"\"Apply small domain-specific adjustment to adjusted_quality.\"\"\"\n",
        "        return score + self.domain_modifiers.get(domain, 0.0)\n",
        "\n",
        "    def choose_action(self, domain, stability_score, scored_candidates):\n",
        "        allowed, locked_out = self.filter_by_curriculum(domain, stability_score, scored_candidates)\n",
        "        if not allowed:\n",
        "            rationale = f\"No unlocked actions for domain={domain} at stability={stability_score:.2f}\"\n",
        "            return None, rationale\n",
        "\n",
        "        # ε-greedy exploration\n",
        "        if random.random() < self.epsilon:\n",
        "            chosen = random.choice(allowed)\n",
        "            rationale = (\n",
        "                f\"Exploration (ε={self.epsilon}): sampled among unlocked actions. \"\n",
        "                f\"Stability={stability_score:.2f}, domain={domain}.\"\n",
        "            )\n",
        "            return chosen, rationale\n",
        "\n",
        "        # Greedy: sort by (pos_score, adjusted_quality + domain_modifier), descending\n",
        "        chosen = sorted(\n",
        "            allowed,\n",
        "            key=lambda c: (\n",
        "                c[\"scores\"][\"pos_score\"],\n",
        "                self._apply_domain_modifier(domain, c[\"scores\"][\"adjusted_quality\"])\n",
        "            ),\n",
        "            reverse=True\n",
        "        )[0]\n",
        "\n",
        "        # Build rationale including curriculum, failure-memory, and domain modifier\n",
        "        s = chosen[\"scores\"]\n",
        "        a = chosen[\"action\"]\n",
        "        fm_influence = (\"yes\" if (s.get(\"penalty\", 0.0) or 0.0) > 0 else \"no\")\n",
        "        dom_mod = self.domain_modifiers.get(domain, 0.0)\n",
        "        rationale = (\n",
        "            f\"Greedy selection: max pos_score then adjusted_quality. \"\n",
        "            f\"Action={a.get('type','?')} on target={a.get('target','?')}. \"\n",
        "            f\"Curriculum: unlocked for domain={domain} at stability={stability_score:.2f}. \"\n",
        "            f\"Failure-memory influence={fm_influence} (penalty={s.get('penalty',0.0):.3f}). \"\n",
        "            f\"Domain modifier applied={dom_mod:+.3f}.\"\n",
        "        )\n",
        "        return chosen, rationale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0veng40_AcsU"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ Define thresholds outside the class\n",
        "domain_thresholds = {\n",
        "    \"text\": {\"rule\": 0.70, \"generative\": 0.90},\n",
        "    \"code\": {\"rule\": 0.60, \"generative\": 0.80},\n",
        "    \"math\": {\"rule\": 0.65, \"generative\": 0.85}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7c6bRNG6av0",
        "outputId": "79edd2f1-ab05-45c6-cef8-99543e41a2b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing check_action_allowed.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile check_action_allowed.py\n",
        "ROOT_IDS = {\"prakriti_global\"}  # do not allow structural edits on root\n",
        "\n",
        "def check_action_allowed(action, tickets):\n",
        "    \"\"\"\n",
        "    Enforce ticket gating rules before executing an action.\n",
        "    Returns (allowed: bool, cost: dict, reason: str)\n",
        "    \"\"\"\n",
        "    op_type, target_id, _ = action\n",
        "\n",
        "    # --- hard protection on root ---\n",
        "    if target_id in ROOT_IDS and op_type in (\"prune\", \"split\", \"reorder\"):\n",
        "        return False, {}, \"Root is protected\"\n",
        "\n",
        "    cost = {}\n",
        "    reason = \"ok\"\n",
        "\n",
        "    if op_type in (\"prune\", \"split\", \"reorder\"):\n",
        "        # Structural actions\n",
        "        if tickets.get(\"B\", 0) >= 1:\n",
        "            cost = {\"B\": 1}\n",
        "        else:\n",
        "            return False, {}, \"Insufficient Blue tickets\"\n",
        "\n",
        "    elif op_type in (\"lock\", \"unlock\"):\n",
        "        # Rule actions\n",
        "        if tickets.get(\"G\", 0) >= 5:\n",
        "            cost = {\"G\": 2}\n",
        "        else:\n",
        "            return False, {}, \"Need ≥5 Green tickets\"\n",
        "\n",
        "    elif op_type in (\"retrieve\", \"generate\"):\n",
        "        # Generative actions\n",
        "        if (tickets.get(\"G\", 0) >= 10 and tickets.get(\"Y\", 0) >= 1):\n",
        "            cost = {\"G\": 3, \"Y\": 1}\n",
        "        elif tickets.get(\"P\", 0) >= 2:\n",
        "            cost = {\"P\": 2}  # Purple shortcut\n",
        "        else:\n",
        "            return False, {}, \"Not enough tickets for generative action\"\n",
        "\n",
        "    else:\n",
        "        return False, {}, \"Unknown action\"\n",
        "\n",
        "    return True, cost, reason"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg4GL7013DnJ",
        "outputId": "bc0c4d91-8ccb-4883-ceca-966eb3116738"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing apply_action.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile apply_action.py\n",
        "from LockManager import LockManager\n",
        "from TreeNodeV1 import TreeNodeV1\n",
        "\n",
        "# protect root from destructive structural edits\n",
        "ROOT_IDS = {\"prakriti_global\"}\n",
        "\n",
        "lock_manager = LockManager()\n",
        "\n",
        "def apply_action(tree, action, embedding_index=None):\n",
        "    \"\"\"\n",
        "    Apply an action to the tree.\n",
        "    Supports structural (split, prune, lock/unlock, reorder) and generative (retrieve, generate).\n",
        "\n",
        "    Args:\n",
        "        tree: TreeNodeV1 root\n",
        "        action: tuple (op_type, target_id, extra)\n",
        "        embedding_index: optional EmbeddingIndex for retrieval\n",
        "    \"\"\"\n",
        "    op_type, target_id, extra = action\n",
        "    log = {\"action\": op_type, \"target\": target_id, \"extra\": extra, \"status\": \"ok\"}\n",
        "\n",
        "    # --- hard protection: never mutate the root ---\n",
        "    if target_id in ROOT_IDS and op_type in (\"prune\", \"split\", \"reorder\"):\n",
        "        log[\"status\"] = \"blocked_root\"\n",
        "        return log\n",
        "\n",
        "    # --- Structural actions ---\n",
        "    if op_type == \"prune\":\n",
        "        # remove children of target node\n",
        "        node = find_node(tree, target_id)\n",
        "        if node:\n",
        "            node.children = []\n",
        "        else:\n",
        "            log[\"status\"] = \"fail\"\n",
        "\n",
        "    elif op_type == \"split\":\n",
        "        # placeholder: split node into two children\n",
        "        node = find_node(tree, target_id)\n",
        "        if node:\n",
        "            left = TreeNodeV1(node_id=f\"{target_id}_L\", value=\"split_left\", level=node.level+1)\n",
        "            right = TreeNodeV1(node_id=f\"{target_id}_R\", value=\"split_right\", level=node.level+1)\n",
        "            node.children = [left, right]\n",
        "        else:\n",
        "            log[\"status\"] = \"fail\"\n",
        "\n",
        "    elif op_type == \"reorder\":\n",
        "        node = find_node(tree, target_id)\n",
        "        if node and len(node.children) > 1:\n",
        "            # simple deterministic rotate (stable)\n",
        "            node.children = node.children[1:] + node.children[:1]\n",
        "        else:\n",
        "            log[\"status\"] = \"fail\"\n",
        "\n",
        "    elif op_type == \"lock\":\n",
        "        node = find_node(tree, target_id)\n",
        "        if node:\n",
        "            node.lock(mode=extra or \"soft\")\n",
        "        else:\n",
        "            log[\"status\"] = \"fail\"\n",
        "\n",
        "    elif op_type == \"unlock\":\n",
        "        node = find_node(tree, target_id)\n",
        "        if node:\n",
        "            node.unlock()\n",
        "        else:\n",
        "            log[\"status\"] = \"fail\"\n",
        "\n",
        "    # --- Generative actions ---\n",
        "    elif op_type == \"retrieve\":\n",
        "        if embedding_index and isinstance(extra, str):\n",
        "            results = embedding_index.query(extra, k=3)\n",
        "            log[\"retrievals\"] = results\n",
        "        else:\n",
        "            log[\"status\"] = \"fail\"\n",
        "\n",
        "    elif op_type == \"generate\":\n",
        "        # (keep as-is if your workspace has the decoder pieces wired)\n",
        "        try:\n",
        "            snapshot = TreeSnapshot(tree).to_dict()\n",
        "            anchors = extract_anchors(snapshot)\n",
        "            retrievals = []\n",
        "            if embedding_index and extra:\n",
        "                retrievals = embedding_index.query(extra, k=1)\n",
        "            explanation = decode_snapshot(snapshot, anchors, retrievals)\n",
        "            explanation = smooth_text(explanation)\n",
        "            node = find_node(tree, target_id)\n",
        "            if node:\n",
        "                node.value = f\"{node.value} :: {explanation}\"\n",
        "                log[\"generated\"] = explanation\n",
        "            else:\n",
        "                log[\"status\"] = \"fail\"\n",
        "        except Exception as e:\n",
        "            log[\"status\"] = f\"generate_fail: {e}\"\n",
        "\n",
        "    else:\n",
        "        log[\"status\"] = \"unknown_action\"\n",
        "\n",
        "    return log\n",
        "\n",
        "\n",
        "def find_node(root, node_id):\n",
        "    \"\"\"DFS search for a node by id.\"\"\"\n",
        "    if getattr(root, \"node_id\", None) == node_id:\n",
        "        return root\n",
        "    for child in getattr(root, \"children\", []) or []:\n",
        "        found = find_node(child, node_id)\n",
        "        if found:\n",
        "            return found\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXXKS-I1_Nx5"
      },
      "outputs": [],
      "source": [
        "\n",
        "import time\n",
        "import json\n",
        "\n",
        "class Logger:\n",
        "    \"\"\"\n",
        "    Logs each decision with scores, rationale, curriculum gating,\n",
        "    failure memory, and provenance.\n",
        "    Outputs JSONL records for easy review.\n",
        "    \"\"\"\n",
        "    def __init__(self, filepath=\"rlteacher_log.jsonl\"):\n",
        "        self.filepath = filepath\n",
        "\n",
        "    def log_decision(self, step_idx, state, action, scores, rationale,\n",
        "                     curriculum_status=None, failure_memory_penalty=None,\n",
        "                     provenance=None, bestaction_flag=True):\n",
        "        record = {\n",
        "            \"timestamp\": time.time(),\n",
        "            \"step\": step_idx,\n",
        "            \"state\": state,              # snapshot features\n",
        "            \"action\": action,            # chosen action\n",
        "            \"scores\": scores,            # pos, neg, raw, adjusted\n",
        "            \"rationale\": rationale,      # why chosen\n",
        "            \"curriculum_status\": curriculum_status or {},\n",
        "            \"failure_memory_penalty\": failure_memory_penalty or 0.0,\n",
        "            \"provenance\": provenance or {},\n",
        "            \"bestaction_flag\": bool(bestaction_flag)  # ✅ new field\n",
        "        }\n",
        "        with open(self.filepath, \"a\") as f:\n",
        "            f.write(json.dumps(record) + \"\\n\")\n",
        "        return record\n",
        "\n",
        "def explain_choice(chosen, rationale, curriculum_status, fm_penalty):\n",
        "    \"\"\"\n",
        "    Generate a reviewer-friendly explanation string.\n",
        "    \"\"\"\n",
        "    sig = chosen[\"scores\"][\"action_signature\"]\n",
        "    pos = round(chosen[\"scores\"][\"pos_score\"], 3)\n",
        "    neg = round(chosen[\"scores\"][\"neg_score\"], 3)\n",
        "    adj = round(chosen[\"scores\"][\"adjusted_quality\"], 3)\n",
        "    return (\n",
        "        f\"Action {sig} chosen with pos={pos}, neg={neg}, adj={adj}. \"\n",
        "        f\"Reason: {rationale}. \"\n",
        "        f\"Curriculum: {curriculum_status}. \"\n",
        "        f\"Failure memory penalty={fm_penalty:.3f}. \"\n",
        "        f\"BestActionFlag=True\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHeS9NCD66AV"
      },
      "outputs": [],
      "source": [
        "def normalize_action_type(a_type: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize raw action types into canonical categories:\n",
        "    - split/merge → structural\n",
        "    - lock/unlock → rule\n",
        "    - generative → generative\n",
        "    - anything else → structural (default fallback)\n",
        "    \"\"\"\n",
        "    if not a_type:\n",
        "        return \"structural\"\n",
        "    a_type = a_type.lower()\n",
        "    if a_type in [\"split\", \"merge\"]:\n",
        "        return \"structural\"\n",
        "    if a_type in [\"lock\", \"unlock\"]:\n",
        "        return \"rule\"\n",
        "    if a_type == \"generative\":\n",
        "        return \"generative\"\n",
        "    return \"structural\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2eh3nNW6dTI"
      },
      "outputs": [],
      "source": [
        "\n",
        "from decoder import decode_snapshot   # TLiteV6 Decoder\n",
        "from smoother import smooth_text      # TLiteV6 Language Polisher\n",
        "\n",
        "def build_explanation(snapshot, anchors, retrievals=None):\n",
        "    \"\"\"\n",
        "    Convert tree snapshot → structured reasoning → polished natural language.\n",
        "    snapshot: dict from TreeSnapshot(root).to_dict()\n",
        "    anchors: dict anchor group metadata\n",
        "    retrievals: optional retrieval evidence (list)\n",
        "    \"\"\"\n",
        "    if retrievals is None:\n",
        "        retrievals = []\n",
        "\n",
        "    # Step 1 — Convert symbolic structure -> rough explanation\n",
        "    raw_text = decode_snapshot(snapshot, anchors, retrievals)\n",
        "\n",
        "    # Step 2 — Smooth grammar + clarity (does not change meaning)\n",
        "    final_text = smooth_text(raw_text)\n",
        "\n",
        "    return final_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceJNZ3MnNMRm"
      },
      "outputs": [],
      "source": [
        "\n",
        "def coeff_a(eis: float) -> float:\n",
        "    \"\"\"Positive weight a(EIS) = 1 + 0.3*EIS\"\"\"\n",
        "    return 1.0 + 0.3 * eis\n",
        "\n",
        "def coeff_b(eis: float) -> float:\n",
        "    \"\"\"Negative weight β(EIS) = 1 - 0.2*EIS\"\"\"\n",
        "    return 1.0 - 0.2 * eis\n",
        "\n",
        "def coeff_gamma(eis: float) -> float:\n",
        "    \"\"\"Lesson weight γ(EIS) = 1 + 0.5*EIS\"\"\"\n",
        "    return 1.0 + 0.5 * eis\n",
        "\n",
        "def coeff_delta(eis: float) -> float:\n",
        "    \"\"\"Insight weight δ(EIS) = 1 + 0.6*EIS\"\"\"\n",
        "    return 1.0 + 0.6 * eis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLkHhlY-NMXP"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math\n",
        "\n",
        "def compute_impact_score(atree_score: float,\n",
        "                         max_depth: int,\n",
        "                         repeat_count: int,\n",
        "                         eis: float) -> float:\n",
        "    \"\"\"\n",
        "    Compute Impact Score (IS) for a failure cluster.\n",
        "    IS = 0.5*(1 - ATreeScore/max_depth)\n",
        "       + 0.3*log(1 + repeat_count)\n",
        "       + 0.2*EIS\n",
        "    \"\"\"\n",
        "    depth_term = 1.0 - (atree_score / max(1, max_depth))\n",
        "    repeat_term = math.log(1 + repeat_count)\n",
        "    eis_term = eis\n",
        "\n",
        "    is_val = 0.5 * depth_term + 0.3 * repeat_term + 0.2 * eis\n",
        "    return round(is_val, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzrcNny6AfU9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpASA-uq5VQM",
        "outputId": "7d8c44fc-4896-4181-934d-6d2b4d938176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing compute_reward.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile compute_reward.py\n",
        "import math, time\n",
        "import numpy as np\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def _safe_float(x, default=0.0):\n",
        "    try:\n",
        "        return float(x)\n",
        "    except Exception:\n",
        "        try:\n",
        "            return float(default)\n",
        "        except Exception:\n",
        "            return 0.0\n",
        "\n",
        "def _clip(x, lo, hi):\n",
        "    x = _safe_float(x, 0.0)\n",
        "    return max(lo, min(hi, x))\n",
        "\n",
        "def _sanitize_snapshot(snap):\n",
        "    \"\"\"Ensure every field compute_reward relies on is a real number.\"\"\"\n",
        "    if snap is None: snap = {}\n",
        "    s = dict(snap)\n",
        "\n",
        "    # structural\n",
        "    s[\"entropy\"]           = _clip(s.get(\"entropy\", 0.0), 0.0, 32.0)\n",
        "    s[\"branch_flip_rate\"]  = _clip(s.get(\"branch_flip_rate\", 0.0) or 0.0, 0.0, 1.0)\n",
        "    s[\"depth\"]             = max(0, int(_safe_float(s.get(\"depth\", 0), 0)))\n",
        "    s[\"node_count\"]        = max(1, int(_safe_float(s.get(\"node_count\", 1), 1)))\n",
        "    s[\"leaf_count\"]        = max(0, int(_safe_float(s.get(\"leaf_count\", 0), 0)))\n",
        "    s[\"branching_factor\"]  = _clip(s.get(\"branching_factor\", 0.0) or 0.0, 0.0, 64.0)\n",
        "\n",
        "    # shaping (optional; default to 0.0)\n",
        "    s[\"pos_score\"]         = _safe_float(s.get(\"pos_score\", 0.0), 0.0)\n",
        "    s[\"neg_score\"]         = _safe_float(s.get(\"neg_score\", 0.0), 0.0)\n",
        "\n",
        "    # outcome flags (optional)\n",
        "    if \"outcome\" not in s:\n",
        "        s[\"outcome\"] = \"neutral\"\n",
        "\n",
        "    return s\n",
        "\n",
        "def _sanitize_tickets(t):\n",
        "    base = {\"G\":0,\"B\":0,\"Y\":0,\"R\":0,\"P\":0}\n",
        "    if not isinstance(t, dict): return base\n",
        "    out = {}\n",
        "    for k in base:\n",
        "        try: out[k] = int(t.get(k, 0))\n",
        "        except Exception: out[k] = 0\n",
        "    return out\n",
        "\n",
        "def _sanitize_temp_tickets(t):\n",
        "    base = {\"G\":0,\"B\":0,\"Y\":0,\"R\":0}\n",
        "    if not isinstance(t, dict): return base\n",
        "    out = {}\n",
        "    for k in base:\n",
        "        try: out[k] = int(t.get(k, 0))\n",
        "        except Exception: out[k] = 0\n",
        "    return out\n",
        "\n",
        "# ---------- main ----------\n",
        "def compute_reward(prev_snap, new_snap, mode,\n",
        "                   tickets, failure_memory,\n",
        "                   temp_tickets, decay_queue,\n",
        "                   proposed_fix=None):\n",
        "    \"\"\"\n",
        "    A-CRES v1.3 with failure-memory + economy + shaping.\n",
        "    Returns: (reward, tickets, failure_memory, log)\n",
        "    \"\"\"\n",
        "\n",
        "    # 0) sanitize inputs\n",
        "    prev = _sanitize_snapshot(prev_snap)\n",
        "    new  = _sanitize_snapshot(new_snap)\n",
        "    tickets       = _sanitize_tickets(tickets)\n",
        "    temp_tickets  = _sanitize_temp_tickets(temp_tickets)\n",
        "    if decay_queue is None: decay_queue = []\n",
        "    if failure_memory is None: failure_memory = {}\n",
        "\n",
        "    # 1) EIS\n",
        "    entropy = new[\"entropy\"]\n",
        "    stability = 1.0 - new[\"branch_flip_rate\"]\n",
        "    depth_ratio = new[\"depth\"] / max(1, new[\"node_count\"])\n",
        "    EIS = 0.4*entropy + 0.3*(1.0 - stability) + 0.3*depth_ratio\n",
        "    EIS = float(_clip(EIS, 0.0, 4.0))\n",
        "\n",
        "    # 2) Failure memory\n",
        "    failure_sig = new.get(\"failure_sig\", None)\n",
        "    lesson_flag, insight_flag = 0, 0\n",
        "    outcome = new.get(\"outcome\", \"neutral\")\n",
        "\n",
        "    IS = 0.0\n",
        "    if failure_sig:\n",
        "        cluster = failure_memory.get(failure_sig)\n",
        "        if cluster:\n",
        "            cluster[\"repeat_count\"] = int(cluster.get(\"repeat_count\", 0)) + 1\n",
        "            stats = cluster.setdefault(\"stats\", {})\n",
        "            stats[\"last_seen\"] = time.time()\n",
        "\n",
        "            # Impact Score\n",
        "            denom = max(1, new[\"depth\"])\n",
        "            IS = 0.5*(1.0 - stability/denom) + 0.3*math.log(1 + cluster[\"repeat_count\"]) + 0.2*EIS\n",
        "            IS = float(_clip(IS, 0.0, 10.0))\n",
        "            cluster[\"impact_score\"] = round(IS, 3)\n",
        "\n",
        "            # proposed fix path\n",
        "            if proposed_fix:\n",
        "                quality = _safe_float(proposed_fix.get(\"quality\", 0.0), 0.0)\n",
        "                thresh = max(0.5, 0.7 - 0.2*EIS)\n",
        "                if quality > thresh:\n",
        "                    cluster[\"solution\"] = proposed_fix\n",
        "                    cluster[\"resolved\"] = True\n",
        "                    outcome = \"resolved_cluster\"\n",
        "                    insight_flag = 1\n",
        "                else:\n",
        "                    outcome = \"repeat_fail\"\n",
        "                    lesson_flag = 1\n",
        "            else:\n",
        "                outcome = \"repeat_fail\"\n",
        "                lesson_flag = 1\n",
        "        else:\n",
        "            failure_memory[failure_sig] = {\n",
        "                \"signature\": failure_sig,\n",
        "                \"repeat_count\": 1,\n",
        "                \"impact_score\": round(0.2*EIS, 3),\n",
        "                \"solution\": None,\n",
        "                \"resolved\": False,\n",
        "                \"stats\": {\"created\": time.time(), \"last_seen\": time.time()}\n",
        "            }\n",
        "            outcome = \"unique_fail\"\n",
        "            lesson_flag = 1\n",
        "    else:\n",
        "        IS = float(round(0.2*EIS, 3))\n",
        "\n",
        "    # 3) Adaptive coeffs\n",
        "    alpha = 1 + 0.3*EIS\n",
        "    beta  = 1 - 0.2*EIS\n",
        "    gamma = 1 + 0.5*EIS\n",
        "    delta = 1 + 0.6*EIS\n",
        "\n",
        "    # 4) Ticket updates\n",
        "    if outcome == \"success\":\n",
        "        tickets[\"G\"] += int(round(3 * alpha))\n",
        "    elif outcome == \"neutral\":\n",
        "        tickets[\"B\"] += max(1, int(round(2 * max(0.1, beta))))\n",
        "    elif outcome == \"unique_fail\":\n",
        "        tickets[\"Y\"] += int(round(1 * gamma))\n",
        "    elif outcome == \"repeat_fail\":\n",
        "        tickets[\"R\"] += int(round(max(1, beta)))\n",
        "    elif outcome == \"resolved_cluster\":\n",
        "        tickets[\"P\"] += 1\n",
        "\n",
        "    # Purple conversions/loans (optional flags in new snapshot)\n",
        "    if tickets.get(\"P\",0) >= 1 and new.get(\"convert_purple\", False):\n",
        "        tickets[\"P\"] -= 1\n",
        "        target = new.get(\"convert_target\", \"G\")\n",
        "        tickets[target] = tickets.get(target, 0) + 3\n",
        "\n",
        "    if tickets.get(\"P\",0) >= 1 and new.get(\"loan_purple\", False):\n",
        "        tickets[\"P\"] -= 1\n",
        "        target = new.get(\"loan_target\", \"G\")\n",
        "        temp_tickets[target] = temp_tickets.get(target, 0) + 1\n",
        "        decay_queue.append(target)\n",
        "\n",
        "    # Decay one temporary ticket per step (if any)\n",
        "    if decay_queue:\n",
        "        expired = decay_queue.pop(0)\n",
        "        temp_tickets[expired] = max(0, temp_tickets.get(expired, 0) - 1)\n",
        "\n",
        "    # 5) Reward (use shaping from new snapshot; may be 0.0 if absent)\n",
        "    pos = _safe_float(new.get(\"pos_score\", 0.0), 0.0)\n",
        "    neg = _safe_float(new.get(\"neg_score\", 0.0), 0.0)\n",
        "    reward = alpha*pos - beta*neg + gamma*lesson_flag + delta*insight_flag - 2.0*IS\n",
        "    reward = float(round(reward, 6))\n",
        "\n",
        "    # 6) Log\n",
        "    log_entry = {\n",
        "        \"EIS\": round(EIS, 3),\n",
        "        \"IS\": round(IS, 3),\n",
        "        \"coeffs\": {\"alpha\": round(alpha,3), \"beta\": round(beta,3),\n",
        "                   \"gamma\": round(gamma,3), \"delta\": round(delta,3)},\n",
        "        \"tickets\": dict(tickets),\n",
        "        \"temp_tickets\": dict(temp_tickets),\n",
        "        \"outcome\": outcome,\n",
        "        \"pos\": round(pos, 6), \"neg\": round(neg, 6),\n",
        "        \"lesson\": int(lesson_flag), \"insight\": int(insight_flag),\n",
        "        \"reward\": round(reward, 6),\n",
        "    }\n",
        "    return reward, tickets, failure_memory, log_entry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BfktnAmANKr"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "import time\n",
        "from typing import Dict, List\n",
        "\n",
        "class GeneratorAdapter:\n",
        "    def __init__(self, model_name=\"tlite-v4.1\", backend=None):\n",
        "        self.model_name = model_name\n",
        "        self.backend = backend  # hook to TLite or HF model\n",
        "\n",
        "    def build_prompt(self, request: Dict) -> str:\n",
        "        \"\"\"Format prompt based on request type.\"\"\"\n",
        "        pt = request.get(\"prompt_type\", \"subtree_expand\")\n",
        "        snapshot = request.get(\"snapshot\", {})\n",
        "        anchors = snapshot.get(\"anchors\", [])\n",
        "        constraints = request.get(\"constraints\", {})\n",
        "\n",
        "        if pt == \"subtree_expand\":\n",
        "            return f\"\"\"CONTEXT:\n",
        "{snapshot.get('tree','')}\n",
        "ANCHOR: {anchors[0] if anchors else 'root'}\n",
        "CONSTRAINTS: {constraints}\n",
        "TASK: Propose up to {request.get('k',3)} valid subtree expansions that reduce entropy and improve stability.\n",
        "Provide both: (a) structured subtree JSON, (b) natural explanation.\"\"\"\n",
        "\n",
        "        elif pt == \"code_fix\":\n",
        "            return f\"\"\"CONTEXT:\n",
        "{snapshot.get('tree','')}\n",
        "ERROR: {constraints.get('error_trace','')}\n",
        "TASK: Suggest a minimal patch (<=10 lines) to fix the bug and explain reasoning in 2 sentences.\n",
        "Provide patch as unified diff and as AST JSON.\"\"\"\n",
        "\n",
        "        elif pt == \"rule_synth\":\n",
        "            return f\"\"\"CONTEXT:\n",
        "{constraints.get('example_pairs','')}\n",
        "TASK: Propose a concise rule (pseudo-code) that generalizes the transformation.\n",
        "Format: rule_name, pattern, replacement, guard_conditions, cost.\"\"\"\n",
        "\n",
        "        else:\n",
        "            return f\"Default prompt: expand snapshot {snapshot.get('id','unknown')}\"\n",
        "\n",
        "    def generate(self, request: Dict, k: int = 3) -> List[Dict]:\n",
        "        \"\"\"Generate candidates using backend model.\"\"\"\n",
        "        prompt = self.build_prompt(request)\n",
        "        start = time.time()\n",
        "\n",
        "        # --- Call backend (stubbed here) ---\n",
        "        # Replace with actual model call: self.backend.generate(prompt, k=k)\n",
        "        outputs = [f\"Mock output {i} for {prompt[:40]}...\" for i in range(k)]\n",
        "\n",
        "        latency = int((time.time() - start) * 1000)\n",
        "        candidates = []\n",
        "        for i, out in enumerate(outputs):\n",
        "            candidates.append({\n",
        "                \"candidate_id\": f\"cand-{uuid.uuid4().hex[:6]}\",\n",
        "                \"request_id\": request[\"request_id\"],\n",
        "                \"generated_subtree\": {\"node_id\": f\"gen_{i}\", \"value\": \"mock\"},\n",
        "                \"natural_text\": out,\n",
        "                \"gen_confidence\": 0.7 + 0.05*i,\n",
        "                \"generator_version\": self.model_name,\n",
        "                \"generation_latency_ms\": latency\n",
        "            })\n",
        "        return candidates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPoUP-y0ANiD"
      },
      "outputs": [],
      "source": [
        "\n",
        "import hashlib\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "\n",
        "class QuickScorer:\n",
        "    def __init__(self, novelty_cache=None):\n",
        "        self.novelty_cache = novelty_cache or set()\n",
        "\n",
        "    def score_batch(self, candidates: List[Dict], snapshot: Dict) -> List[Dict]:\n",
        "        scored = []\n",
        "        for cand in candidates:\n",
        "            text = cand.get(\"natural_text\", \"\")\n",
        "            # --- Novelty via hash ---\n",
        "            h = hashlib.md5(text.encode()).hexdigest()\n",
        "            novelty = 1.0 if h not in self.novelty_cache else 0.0\n",
        "            self.novelty_cache.add(h)\n",
        "\n",
        "            # --- Coverage heuristic ---\n",
        "            node_count = snapshot.get(\"node_count\", 1)\n",
        "            subtree_size = len(cand.get(\"generated_subtree\", {}).get(\"children\", []))\n",
        "            coverage = min(1.0, subtree_size / max(1, node_count))\n",
        "\n",
        "            # --- Anchor alignment (stub: reward if anchor mentioned) ---\n",
        "            anchors = snapshot.get(\"anchors\", [])\n",
        "            anchor_bonus = 0.2 if any(a[\"node_id\"] in text for a in anchors) else 0.0\n",
        "\n",
        "            cand[\"novelty_score\"] = novelty\n",
        "            cand[\"coverage_score\"] = coverage\n",
        "            cand[\"anchor_bonus\"] = anchor_bonus\n",
        "            scored.append(cand)\n",
        "        return scored\n",
        "\n",
        "    def select_top_k(self, candidates: List[Dict], k: int = 4) -> List[Dict]:\n",
        "        return sorted(candidates, key=lambda c: (\n",
        "            c.get(\"novelty_score\", 0.0) +\n",
        "            c.get(\"coverage_score\", 0.0) +\n",
        "            c.get(\"anchor_bonus\", 0.0)\n",
        "        ), reverse=True)[:k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2favjVBIANtM"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "class SimEngine:\n",
        "    def __init__(self, n_rollouts: int = 3):\n",
        "        self.n_rollouts = n_rollouts\n",
        "\n",
        "    def dual_valence(self, snapshot: Dict, candidate: Dict) -> Tuple[float,float,float,float]:\n",
        "        \"\"\"\n",
        "        Run optimistic and pessimistic rollouts.\n",
        "        Returns: (pos, neg, delta_entropy, delta_stability)\n",
        "        \"\"\"\n",
        "        entropy = snapshot.get(\"entropy\", 0.5)\n",
        "        stability = 1.0 - snapshot.get(\"branch_flip_rate\", 0.0)\n",
        "\n",
        "        pos_scores, neg_scores, dEs, dSs = [], [], [], []\n",
        "\n",
        "        for _ in range(self.n_rollouts):\n",
        "            # optimistic rollout\n",
        "            pos = max(0.0, min(1.0, entropy*0.6 + stability*0.4 + random.uniform(0,0.1)))\n",
        "            # pessimistic rollout\n",
        "            neg = max(0.0, min(1.0, (1-stability)*0.7 + entropy*0.3 + random.uniform(0,0.1)))\n",
        "\n",
        "            dE = random.uniform(-0.1, 0.1)  # entropy delta\n",
        "            dS = random.uniform(-0.1, 0.1)  # stability delta\n",
        "\n",
        "            pos_scores.append(pos); neg_scores.append(neg)\n",
        "            dEs.append(dE); dSs.append(dS)\n",
        "\n",
        "        return (\n",
        "            float(np.mean(pos_scores)),\n",
        "            float(np.mean(neg_scores)),\n",
        "            float(np.mean(dEs)),\n",
        "            float(np.mean(dSs))\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bIjPELrAN3U",
        "outputId": "9ed8c890-d63e-4d54-c96b-82f1e47e1905"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing generative_decision_loop_safe.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile generative_decision_loop_safe.py\n",
        "import uuid\n",
        "import numpy as np\n",
        "\n",
        "def generative_decision_loop(snapshot,\n",
        "                             tickets,\n",
        "                             failure_memory,\n",
        "                             config,\n",
        "                             generator,          # GeneratorAdapter\n",
        "                             scorer,             # QuickScorer\n",
        "                             sim_engine,         # SimEngine\n",
        "                             verifier,           # Verifier\n",
        "                             policy_module,      # Policy\n",
        "                             apply_engine,       # ApplyEngine\n",
        "                             provenance_logger,  # ProvenanceLogger\n",
        "                             fallback_module):   # FallbackStrategies\n",
        "    \"\"\"\n",
        "    End-to-end generative decision loop with QuickScorer + SimEngine integrated.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. Build generator request ---\n",
        "    req_id = f\"req-{uuid.uuid4().hex[:6]}\"\n",
        "    request = {\n",
        "        \"request_id\": req_id,\n",
        "        \"snapshot_id\": snapshot.get(\"id\", \"unknown\"),\n",
        "        \"snapshot\": snapshot,\n",
        "        \"prompt_type\": config.get(\"prompt_type\", \"subtree_expand\"),\n",
        "        \"constraints\": config.get(\"constraints\", {}),\n",
        "        \"ticket_budget\": dict(tickets),\n",
        "        \"k\": config.get(\"k\", 4)\n",
        "    }\n",
        "    provenance_logger.log_request(request)\n",
        "\n",
        "    # --- 2. Generate candidates ---\n",
        "    candidates = generator.generate(request, k=request[\"k\"])\n",
        "    provenance_logger.log_candidates(req_id, candidates)\n",
        "\n",
        "    # --- 3. Quick scoring + filter (novelty, coverage, anchor bonus) ---\n",
        "    candidates = scorer.score_batch(candidates, snapshot)\n",
        "    top_candidates = scorer.select_top_k(candidates, k=min(config.get(\"k_quick\", 4), len(candidates)))\n",
        "\n",
        "    scored = []\n",
        "    # --- 4. Micro-sim scoring (dual valence) ---\n",
        "    for cand in top_candidates:\n",
        "        pos, neg, dE, dS = sim_engine.dual_valence(snapshot, cand)\n",
        "        novelty = cand.get(\"novelty_score\", 0.0)\n",
        "        gconf = cand.get(\"gen_confidence\", 0.5)\n",
        "\n",
        "        # --- 5. Fuse scores ---\n",
        "        quality_raw = 1.0 * pos - 0.9 * neg + 0.8 * dE + 0.6 * dS\n",
        "        fused = 0.7 * quality_raw + 0.2 * gconf + 0.1 * novelty\n",
        "        final = 1 / (1 + np.exp(-fused))  # sigmoid squash\n",
        "\n",
        "        ok, reason = verifier.check(snapshot, cand)\n",
        "\n",
        "        scored.append({\n",
        "            \"candidate\": cand,\n",
        "            \"pos\": pos,\n",
        "            \"neg\": neg,\n",
        "            \"delta_entropy\": dE,\n",
        "            \"delta_stability\": dS,\n",
        "            \"novelty\": novelty,\n",
        "            \"final_score\": final,\n",
        "            \"verifier_ok\": ok,\n",
        "            \"verifier_reason\": reason\n",
        "        })\n",
        "    provenance_logger.log_scores(req_id, scored)\n",
        "\n",
        "    # --- 6. Policy selection (ticket-aware) ---\n",
        "    scored = policy_module.apply_ticket_penalties(\n",
        "        scored,\n",
        "        tickets,\n",
        "        config.get(\"ticket_costs\", {})\n",
        "    )\n",
        "    chosen = policy_module.select(\n",
        "        scored,\n",
        "        policy=config.get(\"policy\", \"greedy\"),\n",
        "        epsilon=config.get(\"epsilon\", 0.1),\n",
        "        temperature=config.get(\"temperature\", 1.0)\n",
        "    )\n",
        "\n",
        "    # --- 7. Verify + Apply or Fallback ---\n",
        "    if chosen and chosen[\"verifier_ok\"] and chosen[\"final_score\"] >= config.get(\"min_accept_score\", 0.6):\n",
        "        pre_snap, post_snap, success, reason = apply_engine.apply(\n",
        "            None, snapshot, chosen[\"candidate\"], verifier\n",
        "        )\n",
        "        if success:\n",
        "            provenance_logger.log_accept(request, chosen, post_snap)\n",
        "            return post_snap, {\"status\": \"accepted\", \"reason\": reason,\n",
        "                               \"ticket_penalty\": chosen.get(\"ticket_penalty_factor\", 1.0)}\n",
        "        else:\n",
        "            provenance_logger.log_fallback(request, chosen, pre_snap)\n",
        "            return pre_snap, {\"status\": \"apply_failed\", \"reason\": reason}\n",
        "    else:\n",
        "        fallback = fallback_module.choose(snapshot, scored, [], failure_memory)\n",
        "        provenance_logger.log_fallback(request, fallback, snapshot)\n",
        "        return snapshot, {\"status\": \"fallback\", \"reason\": \"no valid candidate\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qpj2DBQI5ZZG"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import time\n",
        "from typing import Dict, Tuple, Optional\n",
        "\n",
        "def _now():\n",
        "    return time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
        "\n",
        "def make_signature(details: dict, node_id: str, context_id: str) -> str:\n",
        "    \"\"\"\n",
        "    Build a stable cluster signature from salient failure features.\n",
        "    Replace fields to match your domain (e.g., rule_id, error_type, anchor path).\n",
        "    \"\"\"\n",
        "    key = f\"{node_id}|{details.get('error_type','unknown')}|{details.get('anchor','')}\"\n",
        "    return hashlib.sha1(key.encode(\"utf-8\")).hexdigest()[:12]\n",
        "\n",
        "def compute_cluster_is(tree_depth: int, max_depth: int, repeat_count: int, eis: float) -> float:\n",
        "    \"\"\"\n",
        "    Mirror spec: IS = 0.5*(1 - ATreeScore/max_depth) + 0.3*log(1+repeat_count) + 0.2*EIS\n",
        "    Use tree_depth as proxy for ATreeScore if you don’t have it yet.\n",
        "    \"\"\"\n",
        "    import math\n",
        "    atree_score = max(1, tree_depth)\n",
        "    return 0.5*(1 - atree_score/max(1, max_depth)) + 0.3*math.log(1+repeat_count) + 0.2*eis\n",
        "\n",
        "def record_failure(failure_memory: Dict[str, dict],\n",
        "                   context_id: str, node_id: str, details: dict,\n",
        "                   tree_depth: int, max_depth: int, eis: float) -> Tuple[str, dict]:\n",
        "    \"\"\"\n",
        "    Upsert a failure into its cluster and update impact metrics.\n",
        "    Returns (signature, cluster_ref).\n",
        "    \"\"\"\n",
        "    sig = make_signature(details, node_id, context_id)\n",
        "    cluster = failure_memory.get(sig)\n",
        "    if cluster is None:\n",
        "        cluster = {\n",
        "            \"signature\": sig,\n",
        "            \"problems\": [],\n",
        "            \"repeat_count\": 0,\n",
        "            \"impact_score\": 0.0,\n",
        "            \"cluster_weight\": 1.0,\n",
        "            \"solution\": None,\n",
        "            \"stats\": {\"last_seen\": _now(), \"resolved\": False, \"uses\": 0}\n",
        "        }\n",
        "        failure_memory[sig] = cluster\n",
        "\n",
        "    cluster[\"problems\"].append({\n",
        "        \"timestamp\": _now(),\n",
        "        \"context_id\": context_id,\n",
        "        \"node_id\": node_id,\n",
        "        \"details\": details\n",
        "    })\n",
        "    cluster[\"repeat_count\"] += 1\n",
        "    cluster[\"stats\"][\"last_seen\"] = _now()\n",
        "\n",
        "    # Update metrics\n",
        "    cluster[\"impact_score\"] = compute_cluster_is(tree_depth, max_depth, cluster[\"repeat_count\"], eis)\n",
        "    # Optional: scale cluster weight with repeat_count to reflect severity\n",
        "    cluster[\"cluster_weight\"] = min(5.0, 1.0 + 0.25 * cluster[\"repeat_count\"])\n",
        "\n",
        "    return sig, cluster\n",
        "\n",
        "def update_cluster_solution(failure_memory: Dict[str, dict],\n",
        "                            signature: str,\n",
        "                            natural_text: str,\n",
        "                            patch: dict,\n",
        "                            quality: float,\n",
        "                            provenance: dict) -> Optional[dict]:\n",
        "    \"\"\"\n",
        "    Attach a verified solution to a cluster; mark resolved if quality passes threshold.\n",
        "    \"\"\"\n",
        "    cluster = failure_memory.get(signature)\n",
        "    if not cluster:\n",
        "        return None\n",
        "    cluster[\"solution\"] = {\n",
        "        \"natural_text\": natural_text,\n",
        "        \"patch\": patch,\n",
        "        \"quality\": quality,\n",
        "        \"provenance\": provenance\n",
        "    }\n",
        "    cluster[\"stats\"][\"resolved\"] = quality >= 0.6   # tune threshold to your verifier\n",
        "    return cluster\n",
        "\n",
        "def reuse_solution_if_available(failure_memory: Dict[str, dict],\n",
        "                               signature: str) -> Optional[dict]:\n",
        "    \"\"\"\n",
        "    Fetch a stored solution for reapplication.\n",
        "    \"\"\"\n",
        "    cluster = failure_memory.get(signature)\n",
        "    if not cluster or not cluster.get(\"solution\"):\n",
        "        return None\n",
        "    cluster[\"stats\"][\"uses\"] += 1\n",
        "    return cluster[\"solution\"]\n",
        "\n",
        "def prune_clusters(failure_memory: Dict[str, dict],\n",
        "                   max_clusters: int = 50) -> None:\n",
        "    \"\"\"\n",
        "    Keep memory bounded; drop the lowest-impact clusters if above cap.\n",
        "    \"\"\"\n",
        "    if len(failure_memory) <= max_clusters:\n",
        "        return\n",
        "    # Sort by impact_score descending; keep top max_clusters\n",
        "    sorted_items = sorted(failure_memory.items(), key=lambda kv: kv[1].get(\"impact_score\", 0.0), reverse=True)\n",
        "    to_keep = dict(sorted_items[:max_clusters])\n",
        "    failure_memory.clear()\n",
        "    failure_memory.update(to_keep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86FDj2YHBG5R"
      },
      "outputs": [],
      "source": [
        "\n",
        "def award_purple_on_resolution(tickets: Dict[str, int], cluster: dict) -> None:\n",
        "    \"\"\"\n",
        "    If cluster is resolved with acceptable quality, grant Purple insight.\n",
        "    \"\"\"\n",
        "    if cluster.get(\"stats\", {}).get(\"resolved\", False):\n",
        "        tickets[\"P\"] = tickets.get(\"P\", 0) + 1\n",
        "\n",
        "def convert_purple(tickets: Dict[str, int],\n",
        "                   temp_tickets: Dict[str, int],\n",
        "                   decay_queue: list,\n",
        "                   target_color: str = \"G\",\n",
        "                   loan: bool = False) -> str:\n",
        "    \"\"\"\n",
        "    v1.3 conversion: 1P -> 3 any OR 1 loan ticket (expires next cycle).\n",
        "    \"\"\"\n",
        "    if tickets.get(\"P\", 0) < 1:\n",
        "        return \"Insufficient Purple\"\n",
        "    tickets[\"P\"] -= 1\n",
        "    if loan:\n",
        "        temp_tickets[target_color] = temp_tickets.get(target_color, 0) + 1\n",
        "        decay_queue.append(target_color)\n",
        "        return f\"Loaned 1{target_color}, expires next cycle\"\n",
        "    else:\n",
        "        tickets[target_color] = tickets.get(target_color, 0) + 3\n",
        "        return f\"Converted 1P -> 3{target_color}\"\n",
        "\n",
        "def decay_loans(temp_tickets: Dict[str, int], tickets: Dict[str, int], decay_queue: list) -> None:\n",
        "    \"\"\"\n",
        "    Apply loan decay at episode boundary.\n",
        "    \"\"\"\n",
        "    while decay_queue:\n",
        "        color = decay_queue.pop(0)\n",
        "        temp_tickets[color] = max(0, temp_tickets.get(color, 0) - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN2_t1HcDxJQ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "class Verifier:\n",
        "    def __init__(self, max_nodes: int = 50):\n",
        "        self.max_nodes = max_nodes\n",
        "\n",
        "    def check(self, snapshot: Dict, candidate: Dict) -> Tuple[bool, str]:\n",
        "        \"\"\"\n",
        "        Verify candidate before applying.\n",
        "        Returns (pass: bool, reason: str).\n",
        "        \"\"\"\n",
        "        # --- 1. Structural checks ---\n",
        "        subtree = candidate.get(\"generated_subtree\", {})\n",
        "        if not isinstance(subtree, dict):\n",
        "            return False, \"Invalid subtree format\"\n",
        "\n",
        "        # --- 2. Lock checks ---\n",
        "        locked_nodes = snapshot.get(\"locked_nodes\", [])\n",
        "        must_preserve = candidate.get(\"constraints\", {}).get(\"must_preserve_nodes\", [])\n",
        "        for nid in must_preserve:\n",
        "            if nid in locked_nodes:\n",
        "                return False, f\"Lock violation on node {nid}\"\n",
        "\n",
        "        # --- 3. Complexity cap ---\n",
        "        node_count = snapshot.get(\"node_count\", 0)\n",
        "        added_nodes = len(subtree.get(\"children\", []))\n",
        "        if node_count + added_nodes > self.max_nodes:\n",
        "            return False, \"Complexity cap exceeded\"\n",
        "\n",
        "        # --- 4. Domain-specific checks ---\n",
        "        natxt = candidate.get(\"natural_text\", \"\")\n",
        "        if self._contains_profanity(natxt):\n",
        "            return False, \"Prohibited language\"\n",
        "\n",
        "        if snapshot.get(\"domain\") == \"code\":\n",
        "            if not self._lint_code(natxt):\n",
        "                return False, \"Code lint failed\"\n",
        "\n",
        "        if snapshot.get(\"domain\") == \"math\":\n",
        "            if not self._math_simplifiable(natxt):\n",
        "                return False, \"Math expression invalid\"\n",
        "\n",
        "        # --- 5. Passed all checks ---\n",
        "        return True, \"Verifier passed\"\n",
        "\n",
        "    # --- Helpers ---\n",
        "    def _contains_profanity(self, text: str) -> bool:\n",
        "        banned = [\"badword1\", \"badword2\"]  # extend as needed\n",
        "        return any(b in text.lower() for b in banned)\n",
        "\n",
        "    def _lint_code(self, code: str) -> bool:\n",
        "        # Placeholder: could integrate flake8/pylint\n",
        "        return \"import os\" not in code  # trivial sandbox rule\n",
        "\n",
        "    def _math_simplifiable(self, expr: str) -> bool:\n",
        "        # Placeholder: integrate sympy.simplify\n",
        "        return bool(expr and re.match(r\"^[0-9x+\\-*/^() ]+$\", expr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQjHccCRDxWN"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import random\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "class Policy:\n",
        "    @staticmethod\n",
        "    def apply_ticket_penalties(scored: List[Dict], tickets: Dict, ticket_costs: Dict) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Apply ticket-aware penalties to candidate scores.\n",
        "        If candidate requires more tickets than available, scale score down.\n",
        "        \"\"\"\n",
        "        adjusted = []\n",
        "        for cand in scored:\n",
        "            action_type = cand[\"candidate\"].get(\"action_type\", \"generative\")\n",
        "            cost = ticket_costs.get(action_type, {\"G\": 10, \"Y\": 1})  # default\n",
        "            available = sum(tickets.get(k, 0) for k in cost.keys())\n",
        "\n",
        "            required = sum(cost.values())\n",
        "            if required == 0:\n",
        "                factor = 1.0\n",
        "            else:\n",
        "                factor = min(1.0, (available + 1e-6) / (required + 1e-6))\n",
        "\n",
        "            cand[\"final_score\"] *= factor\n",
        "            cand[\"ticket_blocked\"] = (factor < 1.0)\n",
        "            cand[\"ticket_penalty_factor\"] = round(factor, 3)\n",
        "            adjusted.append(cand)\n",
        "        return adjusted\n",
        "\n",
        "    @staticmethod\n",
        "    def select(scored: List[Dict],\n",
        "               policy: str = \"greedy\",\n",
        "               epsilon: float = 0.1,\n",
        "               temperature: float = 1.0) -> Optional[Dict]:\n",
        "        \"\"\"\n",
        "        Select candidate based on policy.\n",
        "        \"\"\"\n",
        "        valid = [c for c in scored if c[\"final_score\"] > 0]  # allow penalized but nonzero\n",
        "        if not valid:\n",
        "            return None\n",
        "\n",
        "        if policy == \"greedy\":\n",
        "            return max(valid, key=lambda c: c[\"final_score\"])\n",
        "\n",
        "        elif policy == \"epsilon_greedy\":\n",
        "            if random.random() < epsilon:\n",
        "                return random.choice(valid)\n",
        "            return max(valid, key=lambda c: c[\"final_score\"])\n",
        "\n",
        "        elif policy == \"softmax\":\n",
        "            scores = np.array([c[\"final_score\"] for c in valid])\n",
        "            exp_scores = np.exp(scores / max(temperature, 1e-6))\n",
        "            probs = exp_scores / exp_scores.sum()\n",
        "            return np.random.choice(valid, p=probs)\n",
        "\n",
        "        else:\n",
        "            # Default fallback: greedy\n",
        "            return max(valid, key=lambda c: c[\"final_score\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOm72UHQDxgl"
      },
      "outputs": [],
      "source": [
        "\n",
        "import json\n",
        "import time\n",
        "from typing import Dict, List\n",
        "\n",
        "class ProvenanceLogger:\n",
        "    def __init__(self, filepath=\"provenance_log.jsonl\"):\n",
        "        self.filepath = filepath\n",
        "\n",
        "    def _write(self, record: Dict):\n",
        "        record[\"timestamp\"] = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
        "        with open(self.filepath, \"a\") as f:\n",
        "            f.write(json.dumps(record) + \"\\n\")\n",
        "\n",
        "    def log_request(self, request: Dict):\n",
        "        self._write({\n",
        "            \"event\": \"request\",\n",
        "            \"request\": request\n",
        "        })\n",
        "\n",
        "    def log_candidates(self, request_id: str, candidates: List[Dict]):\n",
        "        self._write({\n",
        "            \"event\": \"candidates\",\n",
        "            \"request_id\": request_id,\n",
        "            \"candidates\": candidates\n",
        "        })\n",
        "\n",
        "    def log_scores(self, request_id: str, scores: List[Dict]):\n",
        "        self._write({\n",
        "            \"event\": \"scores\",\n",
        "            \"request_id\": request_id,\n",
        "            \"scores\": scores\n",
        "        })\n",
        "\n",
        "    def log_accept(self, request: Dict, chosen: Dict, snapshot: Dict):\n",
        "        self._write({\n",
        "            \"event\": \"accept\",\n",
        "            \"request_id\": request[\"request_id\"],\n",
        "            \"chosen\": chosen,\n",
        "            \"snapshot_post\": snapshot\n",
        "        })\n",
        "\n",
        "    def log_fallback(self, request: Dict, fallback: Dict, snapshot: Dict):\n",
        "        self._write({\n",
        "            \"event\": \"fallback\",\n",
        "            \"request_id\": request[\"request_id\"],\n",
        "            \"fallback\": fallback,\n",
        "            \"snapshot_post\": snapshot\n",
        "        })\n",
        "\n",
        "    def log_reject(self, request: Dict, rejected: List[Dict], snapshot: Dict):\n",
        "        self._write({\n",
        "            \"event\": \"reject\",\n",
        "            \"request_id\": request[\"request_id\"],\n",
        "            \"rejected\": rejected,\n",
        "            \"snapshot_post\": snapshot\n",
        "        })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdtJ3aL_DxqA"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import time\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "class ApplyEngine:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def apply(self, tree, snapshot: Dict, candidate: Dict, verifier) -> Tuple[Dict, Dict, bool, str]:\n",
        "        \"\"\"\n",
        "        Apply a candidate edit to the tree atomically.\n",
        "        Returns (pre_snapshot, post_snapshot, success, reason).\n",
        "        \"\"\"\n",
        "        # --- 1. Save pre-snapshot ---\n",
        "        pre_snapshot = copy.deepcopy(snapshot)\n",
        "\n",
        "        # --- 2. Verify candidate before apply ---\n",
        "        ok, reason = verifier.check(snapshot, candidate)\n",
        "        if not ok:\n",
        "            return pre_snapshot, snapshot, False, f\"Verifier failed: {reason}\"\n",
        "\n",
        "        try:\n",
        "            # --- 3. Apply candidate edit ---\n",
        "            # Placeholder: actual logic depends on your tree structure\n",
        "            # Example: add/replace nodes\n",
        "            subtree = candidate.get(\"generated_subtree\", {})\n",
        "            if subtree:\n",
        "                # naive example: attach subtree to root\n",
        "                if hasattr(tree, \"children\"):\n",
        "                    tree.children.append(subtree)\n",
        "\n",
        "            # --- 4. Build post-snapshot ---\n",
        "            post_snapshot = TreeSnapshot(tree).to_dict()\n",
        "\n",
        "            return pre_snapshot, post_snapshot, True, \"Applied successfully\"\n",
        "\n",
        "        except Exception as e:\n",
        "            # --- 5. Rollback on failure ---\n",
        "            return pre_snapshot, snapshot, False, f\"Apply failed: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5lsXw4MDxyl"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "class FallbackStrategies:\n",
        "    @staticmethod\n",
        "    def conservative_generator(snapshot: Dict) -> Dict:\n",
        "        \"\"\"Produce a minimal safe patch instead of a full subtree.\"\"\"\n",
        "        return {\n",
        "            \"candidate_id\": f\"fallback-{random.randint(1000,9999)}\",\n",
        "            \"generated_subtree\": {\"node_id\": \"hint\", \"value\": \"TODO: refine\"},\n",
        "            \"natural_text\": \"Conservative fallback: added placeholder node.\",\n",
        "            \"gen_confidence\": 0.3,\n",
        "            \"verifier_ok\": True,\n",
        "            \"verifier_reason\": \"Conservative safe patch\"\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def structural_best(snapshot: Dict, structural_actions: List[Dict]) -> Optional[Dict]:\n",
        "        \"\"\"Pick the best structural action if available.\"\"\"\n",
        "        if not structural_actions:\n",
        "            return None\n",
        "        # Assume each action has a 'score' field\n",
        "        return max(structural_actions, key=lambda a: a.get(\"score\", 0.0))\n",
        "\n",
        "    @staticmethod\n",
        "    def escalation(snapshot: Dict, failure_memory: Dict, reason: str) -> Dict:\n",
        "        \"\"\"Escalate unresolved case into failure memory and issue Yellow ticket.\"\"\"\n",
        "        sig = f\"escalation-{random.randint(1000,9999)}\"\n",
        "        failure_memory[sig] = {\n",
        "            \"signature\": sig,\n",
        "            \"problems\": [{\"timestamp\": \"now\", \"details\": {\"reason\": reason}}],\n",
        "            \"repeat_count\": 1,\n",
        "            \"impact_score\": 0.5,\n",
        "            \"solution\": None,\n",
        "            \"stats\": {\"last_seen\": \"now\", \"resolved\": False, \"uses\": 0}\n",
        "        }\n",
        "        return {\n",
        "            \"candidate_id\": sig,\n",
        "            \"natural_text\": f\"Escalated unresolved case: {reason}\",\n",
        "            \"verifier_ok\": False,\n",
        "            \"verifier_reason\": \"Escalation\"\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def choose(snapshot: Dict, scored: List[Dict], structural_actions: List[Dict], failure_memory: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        Decide which fallback to use.\n",
        "        Priority: conservative generator > structural best > escalation.\n",
        "        \"\"\"\n",
        "        if scored:\n",
        "            return FallbackStrategies.conservative_generator(snapshot)\n",
        "        elif structural_actions:\n",
        "            return FallbackStrategies.structural_best(snapshot, structural_actions)\n",
        "        else:\n",
        "            return FallbackStrategies.escalation(snapshot, failure_memory, \"No valid candidates\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycnRGmlmMaXx",
        "outputId": "1aae62af-1512-4c89-e5e6-ee6c23c4eaad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing tlite_state_action_encoder.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile tlite_state_action_encoder.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class StateActionEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Shared encoder to transform ANY feature vector (state or action)\n",
        "    → into a 64D neural embedding TLite can use.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, embed_dim=64, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(input_dim),\n",
        "            nn.Linear(input_dim, embed_dim * 2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(embed_dim * 2, embed_dim)\n",
        "        ).to(device)\n",
        "\n",
        "    def forward(self, raw_vec):\n",
        "        if not isinstance(raw_vec, torch.Tensor):\n",
        "            raw_vec = torch.tensor(raw_vec, dtype=torch.float32)\n",
        "        return self.net(raw_vec.to(self.device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbyLlsoOhvyF",
        "outputId": "4590c343-9369-4da9-c59a-a4c5529e2d0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing tlite_rl_bridge.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile tlite_rl_bridge.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# TLite scorer wrapper\n",
        "# -----------------------------------------------------------\n",
        "class TLiteActionScorer:\n",
        "    def __init__(self, model, device='cpu'):\n",
        "        \"\"\"\n",
        "        model: a torch.nn.Module with forward(x) and attribute .expected_dim\n",
        "               (combined feature dim). If dims mismatch, a projector is created.\n",
        "        \"\"\"\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "\n",
        "        # Align action embedding (50) to state embedding (64) for elementwise interaction.\n",
        "        self.align_action = nn.Linear(50, 64).to(device)\n",
        "\n",
        "        # Optional projector if model.expected_dim != computed combined dim.\n",
        "        self.projector = None\n",
        "\n",
        "    def _as_tensor1d(self, vec):\n",
        "        # Accept list/np/tensor; return shape (1, D) on device\n",
        "        if not isinstance(vec, torch.Tensor):\n",
        "            vec = torch.tensor(vec, dtype=torch.float32)\n",
        "        return vec.to(self.device).unsqueeze(0)\n",
        "\n",
        "    def score_actions(self, state_vector, action_vectors):\n",
        "        \"\"\"\n",
        "        state_vector: 1D np.ndarray / list / torch.Tensor (length 64)\n",
        "        action_vectors: list of 1D vectors (each length 50)\n",
        "        returns: list[float] scores\n",
        "        \"\"\"\n",
        "        state_rep = self._as_tensor1d(state_vector)  # (1, 64)\n",
        "\n",
        "        scores = []\n",
        "        for act_vec in action_vectors:\n",
        "            act_rep = self._as_tensor1d(act_vec)     # (1, 50)\n",
        "            act_rep = self.align_action(act_rep)     # (1, 64) <- aligned to state\n",
        "\n",
        "            # Elementwise interaction now safe (both 64)\n",
        "            interaction = state_rep * act_rep        # (1, 64)\n",
        "\n",
        "            # Combined features: [state, action_aligned, interaction] = 64+64+64 = 192\n",
        "            combined = torch.cat([state_rep, act_rep, interaction], dim=-1)  # (1, 192)\n",
        "\n",
        "            # Match model expected_dim if provided\n",
        "            if hasattr(self.model, \"expected_dim\"):\n",
        "                expected = int(self.model.expected_dim)\n",
        "                if combined.shape[-1] != expected:\n",
        "                    if self.projector is None:\n",
        "                        self.projector = nn.Linear(combined.shape[-1], expected).to(self.device)\n",
        "                    combined = self.projector(combined)\n",
        "\n",
        "            score = self.model(combined).item()\n",
        "            scores.append(score)\n",
        "\n",
        "        return scores\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# FEATURE EXTRACTION\n",
        "# -----------------------------------------------------------\n",
        "STATE_KEYS_ORDER = [\n",
        "    \"depth\",\n",
        "    \"node_count\",\n",
        "    \"leaf_count\",\n",
        "    \"branching_factor\",\n",
        "    \"entropy\",\n",
        "    \"weak_leaves\",\n",
        "]\n",
        "\n",
        "ACTION_TYPES = [\"split\", \"prune\", \"reorder\", \"lock\", \"unlock\"]\n",
        "\n",
        "\n",
        "def extract_features_from_state(state):\n",
        "    \"\"\"\n",
        "    Accepts:\n",
        "      - dict snapshot with keys in STATE_KEYS_ORDER\n",
        "      - list/tuple/np.ndarray numeric of length 6 (already vectorized)\n",
        "    Returns:\n",
        "      - list[float] of length 6\n",
        "    \"\"\"\n",
        "    if isinstance(state, dict):\n",
        "        out = []\n",
        "        for k in STATE_KEYS_ORDER:\n",
        "            v = state.get(k, 0.0)\n",
        "            try:\n",
        "                out.append(float(v))\n",
        "            except Exception:\n",
        "                out.append(0.0)\n",
        "        return out\n",
        "\n",
        "    if isinstance(state, (list, tuple, np.ndarray)):\n",
        "        arr = np.asarray(state, dtype=np.float32).flatten()\n",
        "        if arr.shape[0] < 6:\n",
        "            arr = np.concatenate([arr, np.zeros(6 - arr.shape[0], dtype=np.float32)], axis=0)\n",
        "        elif arr.shape[0] > 6:\n",
        "            arr = arr[:6]\n",
        "        return arr.tolist()\n",
        "\n",
        "    return [0.0] * 6\n",
        "\n",
        "\n",
        "def extract_features_from_action(action):\n",
        "    \"\"\"\n",
        "    Accepts:\n",
        "      - tuple/list: (op_type, target_id, extra)\n",
        "      - dict: {\"type\": \"...\", ...}\n",
        "    Returns:\n",
        "      - one-hot list[float] over ACTION_TYPES (len=5)\n",
        "    \"\"\"\n",
        "    op = None\n",
        "    if isinstance(action, (list, tuple)) and len(action) >= 1:\n",
        "        op = action[0]\n",
        "    elif isinstance(action, dict):\n",
        "        op = action.get(\"type\") or action.get(\"op_type\")\n",
        "\n",
        "    return [1.0 if op == t else 0.0 for t in ACTION_TYPES]\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# SHARED ENCODERS (separate for state/action to avoid dim clash)\n",
        "# -----------------------------------------------------------\n",
        "from tlite_state_action_encoder import StateActionEncoder\n",
        "\n",
        "STATE_EMBED_DIM = 64\n",
        "ACTION_EMBED_DIM = 50\n",
        "\n",
        "_state_encoder = None\n",
        "_action_encoder = None\n",
        "\n",
        "\n",
        "def encode_state_to_vector(state):\n",
        "    \"\"\"\n",
        "    Returns a 1D numpy vector of length STATE_EMBED_DIM (64).\n",
        "    \"\"\"\n",
        "    global _state_encoder\n",
        "    raw_list = extract_features_from_state(state)\n",
        "    raw = torch.tensor(raw_list, dtype=torch.float32)\n",
        "\n",
        "    if _state_encoder is None:\n",
        "        _state_encoder = StateActionEncoder(input_dim=raw.shape[-1], embed_dim=STATE_EMBED_DIM).to(raw.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        emb = _state_encoder(raw)  # (64,)\n",
        "    return emb.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def encode_action_to_vector(action):\n",
        "    \"\"\"\n",
        "    Returns a 1D numpy vector of length ACTION_EMBED_DIM (50).\n",
        "    \"\"\"\n",
        "    global _action_encoder\n",
        "    raw_list = extract_features_from_action(action)\n",
        "    raw = torch.tensor(raw_list, dtype=torch.float32)\n",
        "\n",
        "    if _action_encoder is None:\n",
        "        _action_encoder = StateActionEncoder(input_dim=raw.shape[-1], embed_dim=ACTION_EMBED_DIM).to(raw.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        emb = _action_encoder(raw)  # (50,)\n",
        "    return emb.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# OPTIONAL: tiny smoke-test if run as script\n",
        "# -----------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    class TinyModel(nn.Module):\n",
        "        # combined = state(64) + aligned_action(64) + interaction(64) = 192\n",
        "        def __init__(self, expected_dim=192):\n",
        "            super().__init__()\n",
        "            self.expected_dim = expected_dim\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Linear(self.expected_dim, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, 1)\n",
        "            )\n",
        "\n",
        "        def forward(self, x):\n",
        "            return self.net(x)\n",
        "\n",
        "    # Fake snapshot (dict or 6-list is fine)\n",
        "    snap = {\n",
        "        \"depth\": 3,\n",
        "        \"node_count\": 100,\n",
        "        \"leaf_count\": 60,\n",
        "        \"branching_factor\": 3.3,\n",
        "        \"entropy\": 2.1,\n",
        "        \"weak_leaves\": 20,\n",
        "    }\n",
        "    state_vec = encode_state_to_vector(snap)  # (64,)\n",
        "\n",
        "    # Fake actions\n",
        "    actions = [\n",
        "        (\"split\", \"node_1\", None),\n",
        "        (\"prune\", \"node_5\", None),\n",
        "        (\"reorder\", \"node_2\", None),\n",
        "        (\"lock\", \"node_9\", \"soft\"),\n",
        "        (\"unlock\", \"node_9\", None),\n",
        "    ]\n",
        "    action_vecs = [encode_action_to_vector(a) for a in actions]  # each (50,)\n",
        "\n",
        "    model = TinyModel()\n",
        "    bridge = TLiteActionScorer(model, device=\"cpu\")\n",
        "    scores = bridge.score_actions(state_vec, action_vecs)\n",
        "    print(\"scores:\", scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QXl7yLej0uY",
        "outputId": "c7fd6c9a-b405-4602-cd58-9b0ac2b7251d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nfrom tlite_rl_bridge import TLiteActionScorer\\n\\nbridge = TLiteActionScorer(tlite_model, device='cpu')\\n\\ndef neural_choose_action(state, candidate_actions):\\n    state_vec = encode_state_to_vector(state)\\n    action_vecs = [encode_action_to_vector(a) for a in candidate_actions]\\n\\n    neural_scores = bridge.score_actions(state_vec, action_vecs)\\n    shaped_scores = acres.apply_shaping(neural_scores, state)\\n\\n    best_index = shaped_scores.index(max(shaped_scores))\\n    return candidate_actions[best_index]\""
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "from tlite_rl_bridge import TLiteActionScorer\n",
        "\n",
        "bridge = TLiteActionScorer(tlite_model, device='cpu')\n",
        "\n",
        "def neural_choose_action(state, candidate_actions):\n",
        "    state_vec = encode_state_to_vector(state)\n",
        "    action_vecs = [encode_action_to_vector(a) for a in candidate_actions]\n",
        "\n",
        "    neural_scores = bridge.score_actions(state_vec, action_vecs)\n",
        "    shaped_scores = acres.apply_shaping(neural_scores, state)\n",
        "\n",
        "    best_index = shaped_scores.index(max(shaped_scores))\n",
        "    return candidate_actions[best_index]\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzGnTYXp60GJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def neural_choose_action(state, candidate_actions):\n",
        "    \"\"\"\n",
        "    Placeholder decision policy before TLite distillation.\n",
        "    Uses ACReS reward shaping to pick the best action.\n",
        "    \"\"\"\n",
        "    scored = [(a, acres.quick_score(state, a)) for a in candidate_actions]\n",
        "    scored.sort(key=lambda x: x[1], reverse=True)\n",
        "    return scored[0][0] if scored else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loj4guXMhwHB"
      },
      "outputs": [],
      "source": [
        "\n",
        "def score_candidates_with_neural(self, state_vector, candidates):\n",
        "    # Step 1: Neural scoring\n",
        "    action_vectors = [encode_action_to_vector(c[\"action\"]) for c in candidates]\n",
        "    neural_scores = bridge.score_actions(state_vector, action_vectors)\n",
        "\n",
        "    for cand, ns in zip(candidates, neural_scores):\n",
        "        cand[\"scores\"][\"neural_score\"] = ns\n",
        "\n",
        "    # Step 2: Apply A-CRES shaping per candidate\n",
        "    for cand in candidates:\n",
        "        base = cand[\"scores\"][\"neural_score\"]\n",
        "        shaped = acres.apply_shaping([base], state_vector)[0]   # convert list → scalar\n",
        "        cand[\"scores\"][\"shaped\"] = shaped\n",
        "\n",
        "    # Step 3: Final combined score (RL + Neural + A-CRES)\n",
        "    for cand in candidates:\n",
        "        cand[\"scores\"][\"final_score\"] = (\n",
        "            0.4  * cand[\"scores\"][\"neural_score\"] +\n",
        "            0.35 * cand[\"scores\"].get(\"acres_score\", cand[\"scores\"][\"shaped\"]) +\n",
        "            0.25 * cand[\"scores\"][\"rl_value\"]\n",
        "        )\n",
        "\n",
        "    return candidates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTO6zpSrkAZi"
      },
      "outputs": [],
      "source": [
        "\n",
        "def mix_scores(c, w_neural, w_rl, w_acres):\n",
        "    s = c[\"scores\"]\n",
        "    neural = s.get(\"neural_score\", 0.0)\n",
        "    rl_val = s.get(\"adjusted_quality\", 0.0)\n",
        "    acres = s.get(\"shaped\", 0.0)  # <-- will add next line\n",
        "\n",
        "    return (w_neural * neural) + (w_rl * rl_val) + (w_acres * acres)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eYLx4Bylhfq"
      },
      "outputs": [],
      "source": [
        "\n",
        "import copy\n",
        "import torch\n",
        "\n",
        "class ConversationEnv:\n",
        "    \"\"\"\n",
        "    Mode A: Fixed step conversation mode\n",
        "    Agent generates response → Feedback computed → Next step.\n",
        "    Episode ends after N steps.\n",
        "    \"\"\"\n",
        "    def __init__(self, max_turns=6):\n",
        "        self.max_turns = max_turns\n",
        "        self.turn = 0\n",
        "\n",
        "    def reset(self, initial_input):\n",
        "        self.turn = 0\n",
        "        self.history = [initial_input]\n",
        "        return initial_input\n",
        "\n",
        "    def step(self, action, reward_system, stability_score_fn):\n",
        "        \"\"\"\n",
        "        action = selected candidate response (text or tree action)\n",
        "        \"\"\"\n",
        "        self.history.append(action)\n",
        "        self.turn += 1\n",
        "\n",
        "        # Compute conversational stability (no confusion / topic drift)\n",
        "        stability = stability_score_fn(self.history)\n",
        "        reward = reward_system.shaping_for_conversation(stability)\n",
        "\n",
        "        done = (self.turn >= self.max_turns)\n",
        "        return action, reward, done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYoDXOu6lhif"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TreeEnv:\n",
        "    \"\"\"\n",
        "    Mode B: Stop when stability threshold is reached.\n",
        "    Agent edits / refines tree representation gradually.\n",
        "    \"\"\"\n",
        "    def __init__(self, stability_threshold=0.72):\n",
        "        self.stability_threshold = stability_threshold\n",
        "\n",
        "    def reset(self, tree):\n",
        "        self.tree = copy.deepcopy(tree)\n",
        "        return self.tree\n",
        "\n",
        "    def step(self, action, apply_edit_fn, reward_system, compute_stability):\n",
        "        \"\"\"\n",
        "        action = structured tree edit (add/merge/replace/prune)\n",
        "        \"\"\"\n",
        "        self.tree = apply_edit_fn(self.tree, action)\n",
        "\n",
        "        stability = compute_stability(self.tree)\n",
        "        reward = reward_system.shaping_for_tree(stability)\n",
        "\n",
        "        done = (stability >= self.stability_threshold)\n",
        "\n",
        "        return self.tree, reward, done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X01TcW1Dlhlc"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TaskEnv:\n",
        "    \"\"\"\n",
        "    Mode C: STOP is a valid action.\n",
        "    Agent executes pipeline steps: Summarize → Classify → Generate → Refine.\n",
        "    Episode ends when action == \"STOP\".\n",
        "    \"\"\"\n",
        "    def reset(self, input_data):\n",
        "        self.context = input_data\n",
        "        self.output = None\n",
        "        return input_data\n",
        "\n",
        "    def step(self, action, apply_task_op_fn, reward_system):\n",
        "        if action.get(\"type\") == \"STOP\":\n",
        "            # End of multi-stage task\n",
        "            final_quality = reward_system.evaluate_final_output(self.output)\n",
        "            return self.output, final_quality, True\n",
        "\n",
        "        # Perform transformation step\n",
        "        self.output = apply_task_op_fn(self.context, action)\n",
        "        reward = reward_system.shaping_for_task_step(self.output)\n",
        "\n",
        "        return self.output, reward, False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0RGgs1sMkrr"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "import math\n",
        "\n",
        "class TreeStabilityEnv:\n",
        "    \"\"\"\n",
        "    Agent must construct a stable expression tree.\n",
        "    State = current tree signature + stability score.\n",
        "    Actions = grow / modify / prune nodes.\n",
        "    Episode ends on success or instability collapse.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.tree = {\"nodes\": 1, \"depth\": 1, \"balance\": 1.0}  # minimal stable seed\n",
        "        self.steps = 0\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        # return compact numeric state representation\n",
        "        return [\n",
        "            self.tree[\"nodes\"],\n",
        "            self.tree[\"depth\"],\n",
        "            self.tree[\"balance\"]\n",
        "        ]\n",
        "\n",
        "    def get_actions(self):\n",
        "        \"\"\"\n",
        "        Possible structural modifications.\n",
        "        \"\"\"\n",
        "        return [\n",
        "            {\"type\": \"add_child\"},\n",
        "            {\"type\": \"add_sibling\"},\n",
        "            {\"type\": \"prune_branch\"},\n",
        "            {\"type\": \"duplicate_subtree\"}\n",
        "        ]\n",
        "\n",
        "    def step(self, action):\n",
        "        self.steps += 1\n",
        "        t = self.tree\n",
        "\n",
        "        # APPLY ACTION EFFECTS\n",
        "        if action[\"type\"] == \"add_child\":\n",
        "            t[\"nodes\"] += 1\n",
        "            t[\"depth\"] += random.choice([0, 1])\n",
        "            t[\"balance\"] *= random.uniform(0.95, 1.05)\n",
        "\n",
        "        elif action[\"type\"] == \"add_sibling\":\n",
        "            t[\"nodes\"] += 1\n",
        "            t[\"balance\"] *= random.uniform(0.9, 1.1)\n",
        "\n",
        "        elif action[\"type\"] == \"prune_branch\":\n",
        "            t[\"nodes\"] = max(1, t[\"nodes\"] - random.randint(1, 3))\n",
        "            t[\"balance\"] *= random.uniform(0.95, 1.05)\n",
        "\n",
        "        elif action[\"type\"] == \"duplicate_subtree\":\n",
        "            t[\"nodes\"] += random.randint(1, 4)\n",
        "            t[\"depth\"] += random.choice([0, 1])\n",
        "            t[\"balance\"] *= random.uniform(0.85, 1.15)\n",
        "\n",
        "        # Define stability score (EIS)\n",
        "        stability = math.exp(-abs(t[\"balance\"] - 1.0) * t[\"depth\"])\n",
        "\n",
        "        # End conditions\n",
        "        done = False\n",
        "        if stability < 0.15:   # collapse\n",
        "            reward = -3\n",
        "            done = True\n",
        "        elif t[\"nodes\"] >= 12 and stability > 0.6:\n",
        "            reward = +5       # successful tree\n",
        "            done = True\n",
        "        else:\n",
        "            reward = stability\n",
        "\n",
        "        return self._get_state(), reward, done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "760mL3YvZfUQ"
      },
      "source": [
        "# Phase 6: Deep Tree Expansion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIjHDbFivWgj",
        "outputId": "4739dc09-e4a5-4770-f048-3f663d3a6658"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Data loaded. Shape: (1200, 30)\n",
            "Phase 6: MPNet embedding + dynamic PCA (kept)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c26ce6aa5e5c4d259240b6d8eb1c474b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f229f110cb0494c9458b666a5371acf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f237768be097477e814f7d1dd575d29c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd75a48d2bfa4a96b3dc30f60283cacf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "080b11b0030a417999235bcc7ee2c9ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae0a4827e1bb46258f0f7ae242517f64",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a9073f610244dc1afa782e26e81af4d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b6dbe8015f640bca9b60a64871dd7ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25a13404e7ea4581b0ea2cafa9b663a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f141459eaf5c43ae807c4ef034e9aca5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3225670f81464a4aaa02b53fb3ffe9b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "085a9828598f42aebae0810dd447dd19",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a14cb32e9423497bb90903befcb09efa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf71ee1d995e40669b766ed49fc0f9a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1b07189d5474c678e04c22f749c02bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bd65c39fcad462db24eb1906ca94969",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8292d9fcd324906bb5bfa117fc05907",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff5efb1b67524cea85ff351f8d14df6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13636ab273c749fb9e948bcc7661f623",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79aa336e847045c1ad85a16adbd3ae1e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9eec831b60d44976a5e121148a4a476f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef66a4985d0e4a17ac02b41c96713626",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f737a8d70b6c4c689582dc9e6a62c22e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "faf04a0c773b48bc851e786a46da8baf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c5b9d95d64c4eb7b81a7c49e4475a59",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fdbdc66ab9a4c7fa15e62e3d9e441d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "475cbf18a3264d6da5550e21c24eedac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56e5023c4a9149bf937dd555bb9eb148",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6152090ba8c5446ca066ed5c7649b3f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7e437a1b0334c1bbc6b0fc9467df614",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a74e47fd18384683ac47977994c76a29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c827bab863534d9e988318573ecd5e86",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba4fd4217c5e425a8be2f86f101cc9ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4daedcaa43d44aa4875df590ce0b8d83",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9d9d6bc9a6c41269f159f4b79547e54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0feb38398494e24a23dc06e3ccb0b56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6b9f4a4303f4b0b8c3f848208b1d0b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8592d1299a6a4d37a5cdb90755d1702e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7be4a63ec64a4ddca3b9fcfea0361fd5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fdca49fab96742b5bbd14323b55a488f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0430f6b7a03843098a8e7d82c373ed19",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ MPNet feature embeddings generated with dynamic PCA dimensioning (stored in mpnet_column_value_vectors).\n",
            "\n",
            "Phase 6: Column-name concept grouping (TF-IDF + Agglomerative)\n",
            "✅ Column concept groups formed:\n",
            "\n",
            "Group 0: ['Body Size', 'Body Weight', 'Height', 'Bone Structure', 'Complexion', 'Eyelashes', 'Cheeks', 'Nose', 'Teeth and gums', 'Lips', 'Nails', 'Appetite', 'Liking tastes', 'Dosha', 'Metabolism Type', 'Climate Preference']\n",
            "Group 1: ['General feel of skin', 'Texture of Skin', 'Hair Color', 'Appearance of Hair', 'Shape of face', 'Eyes', 'Blinking of Eyes', 'Skin Sensitivity']\n",
            "Group 7: ['Stress Levels']\n",
            "Group 6: ['Sleep Patterns']\n",
            "Group 5: ['Dietary Habits']\n",
            "Group 4: ['Physical Activity Level']\n",
            "Group 3: ['Water Intake']\n",
            "Group 2: ['Digestion Quality']\n",
            "\n",
            "✅ Final Anchors Created:\n",
            "ANCHOR_0: ['Body Size', 'Body Weight', 'Height', 'Bone Structure', 'Complexion', 'Eyelashes', 'Cheeks', 'Nose', 'Teeth and gums', 'Lips', 'Nails', 'Appetite', 'Liking tastes', 'Dosha', 'Metabolism Type', 'Climate Preference']\n",
            "ANCHOR_1: ['General feel of skin', 'Texture of Skin', 'Hair Color', 'Appearance of Hair', 'Shape of face', 'Eyes', 'Blinking of Eyes', 'Skin Sensitivity']\n",
            "ANCHOR_7: ['Stress Levels']\n",
            "ANCHOR_6: ['Sleep Patterns']\n",
            "ANCHOR_5: ['Dietary Habits']\n",
            "ANCHOR_4: ['Physical Activity Level']\n",
            "ANCHOR_3: ['Water Intake']\n",
            "ANCHOR_2: ['Digestion Quality']\n",
            "\n",
            "Phase 6: TF-IDF + PCA on column VALUEs (this creates column_value_vectors used for vec_pairs)\n",
            "✅ Column value embedding forms created (variable: column_value_vectors).\n",
            "\n",
            "Phase 6: Build vec_pairs (anchors per column -> mean/padded 50-d vectors)\n",
            "Prepared 61 vec_pairs for global tree build (device=cpu).\n",
            "Builder returned root: prakriti_global\n",
            "Depth (builder): 2\n",
            "Node count (calc): 62\n",
            "Snapshot Metrics: {'depth': 1, 'node_count': 62, 'leaf_count': 61, 'branching_factor': 61.0, 'entropy': 0.0, 'weak_leaves': 61, 'branch_flip_rate': 0.0}\n",
            "\n",
            "Checking column name consistency...\n",
            "\n",
            "Columns in dataset: 30\n",
            "Columns in embeddings: feature_vectors missing\n",
            "feature_vectors missing — no rebuild attempted.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Phase 6 — Combined, fixed minimal issues (keeps your exact pipeline logic)\n",
        "\n",
        "# Install\n",
        "!pip install sentence-transformers --quiet\n",
        "\n",
        "# Standard imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import KMeans\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ---------------------------\n",
        "# Load data (you used df earlier) — make data alias so later code works\n",
        "# ---------------------------\n",
        "df = pd.read_csv(\"/content/Updated_Prakriti_With_Features.csv\")\n",
        "df = df.dropna(axis=0, how='any').reset_index(drop=True)\n",
        "data = df   # <-- minimal fix so 'data' references work\n",
        "print(\"✅ Data loaded. Shape:\", df.shape)\n",
        "\n",
        "# ---------------------------\n",
        "# MPNet + PCA embedding block (kept as you wrote it)\n",
        "#   -> store into mpnet_column_value_vectors to avoid overwrite\n",
        "# ---------------------------\n",
        "print(\"Phase 6: MPNet embedding + dynamic PCA (kept)\")\n",
        "mpnet_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "def embed_text_list_mpnet(text_list, batch_size=64):\n",
        "    text_list = [str(x) for x in text_list]\n",
        "    vecs = mpnet_model.encode(text_list, batch_size=batch_size, show_progress_bar=True)\n",
        "    return np.array(vecs, dtype=np.float32)\n",
        "\n",
        "mpnet_column_value_vectors = {}\n",
        "\n",
        "for col in df.columns:\n",
        "    # Only process textual / categorical columns\n",
        "    if df[col].dtype != object:\n",
        "        continue\n",
        "\n",
        "    values = list(df[col].unique())\n",
        "    if not values:\n",
        "        continue\n",
        "\n",
        "    # Embed unique values using MPNet\n",
        "    vectors = embed_text_list_mpnet(values, batch_size=64)\n",
        "\n",
        "    # Dynamic PCA dimension — prevents error\n",
        "    n_values = len(values)\n",
        "    original_dim = vectors.shape[1]\n",
        "    n_components = max(2, min(50, n_values, original_dim))  # never <2\n",
        "\n",
        "    pca = PCA(n_components=n_components, random_state=42)\n",
        "    compressed = pca.fit_transform(vectors)\n",
        "\n",
        "    mpnet_column_value_vectors[col] = {\n",
        "        \"values\": values,\n",
        "        \"vectors\": compressed,\n",
        "        \"pca_dim\": n_components\n",
        "    }\n",
        "\n",
        "print(\"✅ MPNet feature embeddings generated with dynamic PCA dimensioning (stored in mpnet_column_value_vectors).\")\n",
        "\n",
        "# ---------------------------\n",
        "# Anchor grouping using TF-IDF over column NAMES (as in your cell)\n",
        "# ---------------------------\n",
        "print(\"\\nPhase 6: Column-name concept grouping (TF-IDF + Agglomerative)\")\n",
        "\n",
        "col_names = list(data.columns)\n",
        "vectorizer = TfidfVectorizer()\n",
        "name_vectors = vectorizer.fit_transform(col_names).toarray()\n",
        "name_vectors = normalize(name_vectors)\n",
        "\n",
        "sim = cosine_similarity(name_vectors)\n",
        "\n",
        "n_clusters = min(8, len(col_names)//3) if len(col_names) > 8 else 3\n",
        "clustering = AgglomerativeClustering(n_clusters=n_clusters, metric=\"precomputed\", linkage=\"average\")\n",
        "labels = clustering.fit_predict(1 - sim)\n",
        "\n",
        "anchor_map = {}\n",
        "for col, group_id in zip(col_names, labels):\n",
        "    anchor_map.setdefault(group_id, []).append(col)\n",
        "\n",
        "print(\"✅ Column concept groups formed:\\n\")\n",
        "for gid, cols in anchor_map.items():\n",
        "    print(f\"Group {gid}: {cols}\")\n",
        "\n",
        "final_anchors = {\n",
        "    gid: {\n",
        "        \"anchor_label\": f\"ANCHOR_{gid}\",\n",
        "        \"columns\": cols\n",
        "    }\n",
        "    for gid, cols in anchor_map.items()\n",
        "}\n",
        "\n",
        "print(\"\\n✅ Final Anchors Created:\")\n",
        "for gid, info in final_anchors.items():\n",
        "    print(f\"{info['anchor_label']}: {info['columns']}\")\n",
        "\n",
        "# ---------------------------\n",
        "# TF-IDF on VALUE strings + PCA -> this will be the column_value_vectors used for vec_pairs\n",
        "# (keeps your TF-IDF→PCA code; this will overwrite variable column_value_vectors by design in your original flow)\n",
        "# ---------------------------\n",
        "print(\"\\nPhase 6: TF-IDF + PCA on column VALUEs (this creates column_value_vectors used for vec_pairs)\")\n",
        "\n",
        "column_value_vectors = {}\n",
        "\n",
        "for col in data.columns:\n",
        "    vals = data[col].astype(str).unique()  # unique class values\n",
        "    if len(vals) == 0:\n",
        "        continue\n",
        "\n",
        "    # TF-IDF vectorize the unique value strings for the column\n",
        "    vectorizer_val = TfidfVectorizer()\n",
        "    X = vectorizer_val.fit_transform(vals)  # shape: (n_unique_vals, vocab_size)\n",
        "\n",
        "    # determine pca_dim safely\n",
        "    pca_dim = min(50, X.shape[1], X.shape[0])  # avoid PCA errors\n",
        "    if pca_dim < 2:\n",
        "        # fallback: embed as one-hot padded to 50\n",
        "        X_dense = np.eye(len(vals), 50, dtype=np.float32)\n",
        "    else:\n",
        "        pca_val = PCA(n_components=pca_dim, random_state=42)\n",
        "        X_dense = pca_val.fit_transform(X.toarray()).astype(np.float32)\n",
        "        # pad to length 50 if needed\n",
        "        if X_dense.shape[1] < 50:\n",
        "            X_dense = np.pad(X_dense, ((0,0), (0, 50-X_dense.shape[1])), mode='constant')\n",
        "\n",
        "    # normalize rows\n",
        "    X_dense = normalize(X_dense)\n",
        "\n",
        "    column_value_vectors[col] = {\n",
        "        \"values\": list(vals),\n",
        "        \"vectors\": X_dense.astype(np.float32)\n",
        "    }\n",
        "\n",
        "print(\"✅ Column value embedding forms created (variable: column_value_vectors).\")\n",
        "\n",
        "# ---------------------------\n",
        "# Prepare vec_pairs from final_anchors using column_value_vectors (your original logic)\n",
        "# ---------------------------\n",
        "print(\"\\nPhase 6: Build vec_pairs (anchors per column -> mean/padded 50-d vectors)\")\n",
        "\n",
        "import torch\n",
        "vec_pairs = []\n",
        "anchor_meta = []\n",
        "\n",
        "for group_id, info in final_anchors.items():\n",
        "    cols = info[\"columns\"]\n",
        "    for col in cols:\n",
        "        if col not in column_value_vectors:\n",
        "            print(f\"[WARN] Column {col} missing in column_value_vectors — skipping.\")\n",
        "            continue\n",
        "        vals = column_value_vectors[col][\"values\"]\n",
        "        vecs = column_value_vectors[col][\"vectors\"]  # shape: (n_vals, pca_dim) where pca_dim <= 50\n",
        "\n",
        "        padded_vecs = []\n",
        "        for v in vecs:\n",
        "            v = np.asarray(v, dtype=np.float32)\n",
        "            if v.ndim == 0:\n",
        "                v = np.expand_dims(v, 0)\n",
        "            if v.shape[0] >= 50:\n",
        "                pv = v[:50]\n",
        "            else:\n",
        "                pv = np.pad(v, (0, 50 - v.shape[0]), mode='constant')\n",
        "            padded_vecs.append(pv)\n",
        "        if not padded_vecs:\n",
        "            continue\n",
        "        padded_vecs = np.array(padded_vecs, dtype=np.float32)\n",
        "\n",
        "        # cluster these padded vectors to produce anchors per column\n",
        "        n_vals = padded_vecs.shape[0]\n",
        "        n_clusters_col = 3 if n_vals >= 6 else (2 if n_vals >= 3 else 1)\n",
        "        if n_clusters_col == 1:\n",
        "            mean_vec = padded_vecs.mean(axis=0)\n",
        "            token = f\"{col}::A0\"\n",
        "            vec_pairs.append((token, torch.tensor(mean_vec, dtype=torch.float32)))\n",
        "            anchor_meta.append((group_id, col, 0, token))\n",
        "        else:\n",
        "            kmeans = KMeans(n_clusters=n_clusters_col, random_state=42).fit(padded_vecs)\n",
        "            for lab in range(n_clusters_col):\n",
        "                idxs = np.where(kmeans.labels_ == lab)[0].tolist()\n",
        "                if not idxs:\n",
        "                    continue\n",
        "                mean_vec = padded_vecs[idxs].mean(axis=0)\n",
        "                token = f\"{col}::A{lab}\"\n",
        "                vec_pairs.append((token, torch.tensor(mean_vec, dtype=torch.float32)))\n",
        "                anchor_meta.append((group_id, col, lab, token))\n",
        "\n",
        "print(f\"Prepared {len(vec_pairs)} vec_pairs for global tree build (device=cpu).\")\n",
        "\n",
        "# ---------------------------\n",
        "# Build tree using TreeBuilderV2 (dim=50 to match TF-IDF→PCA padded vectors)\n",
        "# ---------------------------\n",
        "from TreeBuilderV2 import TreeBuilderV2\n",
        "from TreeNodeV1 import TreeNodeV1\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "builder = TreeBuilderV2(device=device, dim=50, mode=\"three\")  # dim=50 matches padded vectors\n",
        "\n",
        "prakriti_root = builder.build_tree(vec_pairs, sample_id=\"prakriti_global\")\n",
        "print(\"Builder returned root:\", prakriti_root.node_id if prakriti_root else None)\n",
        "print(\"Depth (builder):\", prakriti_root.get_depth() if prakriti_root else \"None\")\n",
        "# count_nodes may be defined elsewhere in your notebook; fallback safe count:\n",
        "def count_nodes_safe(root):\n",
        "    if root is None:\n",
        "        return 0\n",
        "    cnt = 0\n",
        "    stack = [root]\n",
        "    seen = set()\n",
        "    while stack:\n",
        "        n = stack.pop()\n",
        "        if id(n) in seen:\n",
        "            continue\n",
        "        seen.add(id(n))\n",
        "        cnt += 1\n",
        "        for c in getattr(n, \"children\", []) or []:\n",
        "            stack.append(c)\n",
        "    return cnt\n",
        "\n",
        "print(\"Node count (calc):\", count_nodes_safe(prakriti_root))\n",
        "\n",
        "prakriti_tree = prakriti_root\n",
        "\n",
        "# ---------------------------\n",
        "# Phase 6 — Cell 6: Snapshot\n",
        "# ---------------------------\n",
        "from TreeSnapshot import TreeSnapshot\n",
        "if prakriti_tree is not None:\n",
        "    snapshot = TreeSnapshot(prakriti_tree).to_dict()\n",
        "    print(\"Snapshot Metrics:\", snapshot)\n",
        "else:\n",
        "    print(\"No tree produced; snapshot unavailable.\")\n",
        "\n",
        "# ---------------------------\n",
        "# Optional: Column name / embedding consistency check (keeps your final block behavior)\n",
        "# ---------------------------\n",
        "print(\"\\nChecking column name consistency...\\n\")\n",
        "dataset_columns = set(df.columns)\n",
        "embedding_columns = set()  # feature_vectors was not built in this cell; preserve your logic by making it safe\n",
        "\n",
        "if 'feature_vectors' in globals() and isinstance(feature_vectors, dict):\n",
        "    embedding_columns = set(feature_vectors.keys())\n",
        "else:\n",
        "    embedding_columns = None\n",
        "\n",
        "print(\"Columns in dataset:\", len(dataset_columns))\n",
        "print(\"Columns in embeddings:\", len(embedding_columns) if embedding_columns is not None else \"feature_vectors missing\")\n",
        "\n",
        "if embedding_columns:\n",
        "    missing_in_embed = dataset_columns - embedding_columns\n",
        "    missing_in_data = embedding_columns - dataset_columns\n",
        "\n",
        "    print(\"\\nMissing in embeddings:\", missing_in_embed)\n",
        "    print(\"Missing in dataset:\", missing_in_data)\n",
        "\n",
        "    if len(missing_in_embed) > 0:\n",
        "        print(\"\\n🛠️ REBUILDING feature_vectors with correct column names...\")\n",
        "        feature_vectors = {col: torch.randn(50) for col in dataset_columns}\n",
        "        print(\"✅ feature_vectors rebuilt.\")\n",
        "else:\n",
        "    print(\"feature_vectors missing — no rebuild attempted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biiT7vIke7Q5"
      },
      "source": [
        "# Phase 6.5: Stability Correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSda8v0LfSUU",
        "outputId": "ad6a3051-9561-4379-da4c-e5c99c0f03b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phase 6.5 helpers loaded.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Phase 6.5 Helpers (run once)\n",
        "import copy, random, traceback, numpy as np\n",
        "from collections import defaultdict, deque\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Cycle-safe traversals & utilities\n",
        "def collect_all_nodes_safe(root):\n",
        "    if root is None: return []\n",
        "    out, q, seen = [], deque([root]), set()\n",
        "    while q:\n",
        "        n = q.popleft()\n",
        "        if id(n) in seen: continue\n",
        "        seen.add(id(n)); out.append(n)\n",
        "        for c in getattr(n, \"children\", []) or []: q.append(c)\n",
        "    return out\n",
        "\n",
        "def collect_leaves_safe(root):\n",
        "    return [n for n in collect_all_nodes_safe(root) if not (getattr(n, \"children\", None))]\n",
        "\n",
        "def count_nodes_safe(root):\n",
        "    return len(collect_all_nodes_safe(root))\n",
        "\n",
        "def get_tree_depth_safe(root):\n",
        "    if root is None: return 0\n",
        "    q, seen = deque([(root,0)]), set(); maxd=0\n",
        "    while q:\n",
        "        n,d = q.popleft()\n",
        "        if id(n) in seen: continue\n",
        "        seen.add(id(n)); maxd = max(maxd, d)\n",
        "        for c in getattr(n, \"children\", []) or []: q.append((c,d+1))\n",
        "    return maxd+1\n",
        "\n",
        "def reset_levels_safe(root):\n",
        "    from collections import deque\n",
        "    if root is None: return\n",
        "    q, seen = deque([(root,0)]), set()\n",
        "    while q:\n",
        "        n, lv = q.popleft()\n",
        "        if id(n) in seen: continue\n",
        "        seen.add(id(n)); n.level = lv\n",
        "        for c in getattr(n, \"children\", []) or []: q.append((c, lv+1))\n",
        "\n",
        "def detect_cycle(root):\n",
        "    if root is None: return False\n",
        "    q, seen = deque([root]), set()\n",
        "    while q:\n",
        "        n = q.popleft()\n",
        "        if id(n) in seen: return True\n",
        "        seen.add(id(n))\n",
        "        for c in getattr(n, \"children\", []) or []: q.append(c)\n",
        "    return False\n",
        "\n",
        "def copy_tree_remove_cycles(root):\n",
        "    # Shallow-copy nodes, avoid linking child that points to ancestor id\n",
        "    if root is None: return None\n",
        "    from copy import deepcopy\n",
        "    new_root = type(root)(node_id=root.node_id, value=root.value, level=root.level)\n",
        "    stack = [(root, new_root, set([id(root)]))]\n",
        "    while stack:\n",
        "        orig, newn, ancestors = stack.pop()\n",
        "        for c in getattr(orig, \"children\", []) or []:\n",
        "            if id(c) in ancestors:\n",
        "                continue\n",
        "            cn = type(c)(node_id=c.node_id, value=c.value, level=newn.level+1)\n",
        "            # copy cached_vector if present\n",
        "            if hasattr(c, \"cached_vector\"):\n",
        "                try: cn.cached_vector = copy.deepcopy(c.cached_vector)\n",
        "                except: cn.cached_vector = None\n",
        "            newn.add_child(cn)\n",
        "            new_anc = set(ancestors); new_anc.add(id(c))\n",
        "            stack.append((c, cn, new_anc))\n",
        "    reset_levels_safe(new_root)\n",
        "    return new_root\n",
        "\n",
        "print(\"Phase 6.5 helpers loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSnqcFruGzqz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfRKVblGv4Ue",
        "outputId": "68d8995a-5715-4edf-9a74-f19191bb5840"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pass1 complete. Depth: 4 Node count: 155\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Pass 1: Balanced deepening (high granularity)\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "root = prakriti_tree  # from your Phase 6 result\n",
        "if detect_cycle(root):\n",
        "    print(\"Cycle found — cleaning first...\")\n",
        "    root = copy_tree_remove_cycles(root)\n",
        "\n",
        "leaves = [n for n in collect_all_nodes_safe(root) if getattr(n, \"cached_vector\", None) is not None or getattr(n, \"value\", None)]\n",
        "# collect vectors for those leaves (if cached_vector missing we will skip)\n",
        "leaf_objs = []\n",
        "leaf_vecs = []\n",
        "for n in leaves:\n",
        "    vec = getattr(n, \"cached_vector\", None)\n",
        "    if vec is None:\n",
        "        # fallback: try to create vector from name via TF-IDF (light)\n",
        "        leaf_text = str(n.value)\n",
        "        # simple char-level fallback vector\n",
        "        arr = np.array([ord(ch)%100 for ch in leaf_text[:50]], dtype=np.float32)\n",
        "        if arr.size < 50:\n",
        "            arr = np.pad(arr, (0, 50-arr.size))\n",
        "        vec = arr\n",
        "    else:\n",
        "        # ensure it's numpy\n",
        "        try:\n",
        "            vec = np.array(vec[:50], dtype=np.float32)\n",
        "        except Exception:\n",
        "            vec = np.zeros(50, dtype=np.float32)\n",
        "    leaf_objs.append(n); leaf_vecs.append(vec)\n",
        "\n",
        "leaf_vecs = np.stack(leaf_vecs)\n",
        "leaf_count = len(leaf_vecs)\n",
        "target_clusters = min(max(12, leaf_count//2), leaf_count)  # aggressive high granularity\n",
        "if target_clusters < 2: target_clusters = 2\n",
        "\n",
        "kmeans = KMeans(n_clusters=target_clusters, random_state=42).fit(leaf_vecs)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Build new root and mid groups (clone nodes to avoid cycles)\n",
        "new_root = type(root)(node_id=root.node_id, value=root.value, level=0)\n",
        "groups = {}\n",
        "for lab, obj in zip(labels, leaf_objs):\n",
        "    groups.setdefault(lab, []).append(obj)\n",
        "\n",
        "import copy\n",
        "for lab, members in groups.items():\n",
        "    gnode = type(root)(node_id=f\"mid_g_{lab}\", value=f\"MidGroup_{lab}\", level=1)\n",
        "    for m in members:\n",
        "        gnode.add_child(copy.deepcopy(m))\n",
        "    new_root.add_child(gnode)\n",
        "\n",
        "reset_levels_safe(new_root)\n",
        "prakriti_tree = new_root\n",
        "print(\"Pass1 complete. Depth:\", get_tree_depth_safe(prakriti_tree), \"Node count:\", count_nodes_safe(prakriti_tree))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1qPAbIFv4dR",
        "outputId": "bcd9c74b-e7d6-485b-ddc1-0cfd8db028f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pass2 complete. Depth: 4 Node count: 164\n"
          ]
        }
      ],
      "source": [
        "# Pass 2: Strict hierarchical clustering inside mid-groups\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "root = prakriti_tree\n",
        "for g in list(getattr(root, \"children\", [])):\n",
        "    # take its child leaves (direct children might be cloned value-nodes)\n",
        "    direct = [c for c in getattr(g, \"children\", [])]\n",
        "    if len(direct) <= 4:\n",
        "        continue\n",
        "    vecs = []\n",
        "    nodes_list = []\n",
        "    for c in direct:\n",
        "        v = getattr(c, \"cached_vector\", None)\n",
        "        if v is None:\n",
        "            # try to derive a lightweight vector from text\n",
        "            arr = np.array([ord(ch)%100 for ch in (c.value or \"\")[:50]], dtype=np.float32)\n",
        "            if arr.size < 50: arr = np.pad(arr, (0, 50-arr.size))\n",
        "            v = arr\n",
        "        nodes_list.append(c); vecs.append(np.array(v[:50], dtype=np.float32))\n",
        "    if len(vecs) <= 3: continue\n",
        "    n_sub = min(max(2, len(vecs)//3), 8)\n",
        "    agg = AgglomerativeClustering(n_clusters=n_sub, metric='euclidean', linkage='ward')\n",
        "    labels = agg.fit_predict(np.stack(vecs))\n",
        "    # create subnodes\n",
        "    from collections import defaultdict\n",
        "    submap = defaultdict(list)\n",
        "    for node_obj, lab in zip(nodes_list, labels):\n",
        "        submap[lab].append(node_obj)\n",
        "    new_subnodes = []\n",
        "    for lab, members in submap.items():\n",
        "        subnode = type(g)(node_id=f\"{g.node_id}_sub_{lab}\", value=f\"{g.value}_sub{lab}\", level=g.level+1)\n",
        "        for m in members:\n",
        "            subnode.add_child(copy.deepcopy(m))\n",
        "        new_subnodes.append(subnode)\n",
        "    # append any other children not in nodes_list\n",
        "    others = [c for c in g.children if c not in nodes_list]\n",
        "    g.children = new_subnodes + others\n",
        "\n",
        "reset_levels_safe(root)\n",
        "prakriti_tree = root\n",
        "print(\"Pass2 complete. Depth:\", get_tree_depth_safe(prakriti_tree), \"Node count:\", count_nodes_safe(prakriti_tree))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPrfzZaRv4lb",
        "outputId": "f744e533-3e82-4932-90d4-02e487032c30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pass3 complete. Depth: 4 Node count: 164\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Pass 3: Semantic label refinement (human friendly)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "root = prakriti_tree\n",
        "\n",
        "def extract_terms_from_leaves(node, top_k=2):\n",
        "    leaves = [str(l.value) for l in collect_leaves_safe(node)]\n",
        "    if not leaves: return node.value\n",
        "    corpus = [\" \".join(leaves)]\n",
        "    vec = TfidfVectorizer(stop_words='english')\n",
        "    try:\n",
        "        X = vec.fit_transform(corpus)\n",
        "        fn = np.array(vec.get_feature_names_out())\n",
        "        if fn.size == 0: return node.value\n",
        "        terms = fn[:top_k].tolist()\n",
        "    except Exception:\n",
        "        terms = (node.value or \"\").split()[:top_k]\n",
        "    return f\"{node.value} — {' '.join(terms)}\"\n",
        "\n",
        "for n in collect_all_nodes_safe(root):\n",
        "    if getattr(n, \"children\", None) and len(n.children) >= 2 and n.level <= get_tree_depth_safe(root)-2:\n",
        "        try:\n",
        "            n.value = extract_terms_from_leaves(n, top_k=2)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "reset_levels_safe(root)\n",
        "prakriti_tree = root\n",
        "print(\"Pass3 complete. Depth:\", get_tree_depth_safe(prakriti_tree), \"Node count:\", count_nodes_safe(prakriti_tree))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wZJYDJSv4t5",
        "outputId": "9b53483f-ef1c-4cbf-cf93-d13065491519"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stability failures: 0 Snapshots: 300\n",
            "Final snapshot after sim: {'depth': 6, 'node_count': 225, 'leaf_count': 154, 'branching_factor': 3.155, 'entropy': 4.4649, 'weak_leaves': 154, 'branch_flip_rate': 0.0}\n",
            "Normalization complete. Snapshot: {'depth': 3, 'node_count': 164, 'leaf_count': 122, 'branching_factor': 3.881, 'entropy': 4.7916, 'weak_leaves': 122, 'branch_flip_rate': 0.0}\n",
            "Artifacts saved to /content/phase6_artifacts_high\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Stability test + normalization\n",
        "def random_action_apply(root, n_steps=200):\n",
        "    import random\n",
        "    import copy\n",
        "    rcopy = copy.deepcopy(root)\n",
        "    failures = []\n",
        "    snaps = []\n",
        "    for i in range(n_steps):\n",
        "        # sample random internal node (not root)\n",
        "        nodes = [n for n in collect_all_nodes_safe(rcopy) if n.node_id != rcopy.node_id]\n",
        "        if not nodes: break\n",
        "        tgt = random.choice(nodes)\n",
        "        op = random.choice([\"prune\",\"split\",\"reorder\",\"lock\",\"unlock\"])\n",
        "        try:\n",
        "            if op == \"prune\":\n",
        "                tgt.children = []\n",
        "            elif op == \"split\":\n",
        "                a = type(tgt)(node_id=f\"{tgt.node_id}_s1\", value=\"split1\", level=tgt.level+1)\n",
        "                b = type(tgt)(node_id=f\"{tgt.node_id}_s2\", value=\"split2\", level=tgt.level+1)\n",
        "                tgt.children = [a,b]\n",
        "            elif op == \"reorder\" and len(tgt.children)>1:\n",
        "                random.shuffle(tgt.children)\n",
        "            elif op == \"lock\":\n",
        "                if hasattr(tgt, \"lock\"): tgt.lock(\"soft\")\n",
        "            elif op == \"unlock\":\n",
        "                if hasattr(tgt, \"unlock\"): tgt.unlock()\n",
        "            if 'TreeSnapshot' in globals():\n",
        "                snaps.append(TreeSnapshot(rcopy).to_dict())\n",
        "            # normalization\n",
        "            reset_levels_safe(rcopy)\n",
        "        except Exception as e:\n",
        "            failures.append((i, str(e)))\n",
        "            break\n",
        "    return rcopy, snaps, failures\n",
        "\n",
        "sim_tree, sim_snaps, sim_failures = random_action_apply(prakriti_tree, n_steps=300)\n",
        "print(\"Stability failures:\", len(sim_failures), \"Snapshots:\", len(sim_snaps))\n",
        "try:\n",
        "    final_snap = TreeSnapshot(sim_tree).to_dict()\n",
        "    print(\"Final snapshot after sim:\", final_snap)\n",
        "except Exception as e:\n",
        "    print(\"Snapshot error:\", e)\n",
        "\n",
        "# normalization passes: prune tiny leaves and enforce fanout/depth\n",
        "def prune_weak_leaves(root, min_tokens=0):\n",
        "    changed=False\n",
        "    for n in reversed(collect_all_nodes_safe(root)):\n",
        "        if n.children: continue\n",
        "        tokens = [t for t in str(n.value).split(\",\") if t.strip()]\n",
        "        if len(tokens) <= min_tokens:\n",
        "            # find parent and remove\n",
        "            for p in collect_all_nodes_safe(root):\n",
        "                if n in getattr(p, \"children\", []):\n",
        "                    p.children = [c for c in p.children if c is not n]; changed=True; break\n",
        "    return changed\n",
        "\n",
        "iter=0\n",
        "while iter<3:\n",
        "    prune_weak_leaves(prakriti_tree, min_tokens=0)\n",
        "    reset_levels_safe(prakriti_tree)\n",
        "    iter+=1\n",
        "\n",
        "try:\n",
        "    saved_snap = TreeSnapshot(prakriti_tree).to_dict()\n",
        "    print(\"Normalization complete. Snapshot:\", saved_snap)\n",
        "except Exception as e:\n",
        "    print(\"Could not produce final snapshot:\", e)\n",
        "\n",
        "# Optionally save artifacts\n",
        "import os, json\n",
        "out_dir = \"/content/phase6_artifacts_high\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "with open(os.path.join(out_dir, \"final_tree_snapshot.json\"), \"w\") as f:\n",
        "    json.dump(saved_snap, f, indent=2)\n",
        "print(\"Artifacts saved to\", out_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ho2NNJZnJ-Gf",
        "outputId": "c7ebd862-2d4f-4df7-a8f3-575f949d88ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Helpers imported & aliased.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Ensure local .py modules can be imported\n",
        "import sys, importlib, os\n",
        "if '/content' not in sys.path:\n",
        "    sys.path.append('/content')\n",
        "\n",
        "# Import RL helpers\n",
        "from check_action_allowed import check_action_allowed\n",
        "from apply_action import apply_action\n",
        "from compute_reward import compute_reward\n",
        "\n",
        "# The function in generative_decision_loop_safe.py is named `generative_decision_loop`\n",
        "# Phase2Env expects `generative_decision_loop_safe`, so alias it on import:\n",
        "from generative_decision_loop_safe import generative_decision_loop as generative_decision_loop_safe\n",
        "\n",
        "# (Optional) hot-reload if you edit files frequently\n",
        "importlib.reload(sys.modules['check_action_allowed'])\n",
        "importlib.reload(sys.modules['apply_action'])\n",
        "importlib.reload(sys.modules['compute_reward'])\n",
        "importlib.reload(sys.modules['generative_decision_loop_safe'])\n",
        "\n",
        "print(\"✅ Helpers imported & aliased.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnU28qf4wN0m",
        "outputId": "7ecdeedd-55b6-45b2-a8dc-2cf75a954530"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EIS: 1.9221278048780488\n",
            "Reward test: -0.752\n",
            "Tickets: {'G': 0, 'B': 1, 'Y': 0, 'R': 0, 'P': 0}\n",
            "Log: {'EIS': 1.882, 'IS': 0.376, 'coeffs': {'alpha': 1.565, 'beta': 0.624, 'gamma': 1.941, 'delta': 2.129}, 'tickets': {'G': 0, 'B': 1, 'Y': 0, 'R': 0, 'P': 0}, 'temp_tickets': {'G': 0, 'B': 0, 'Y': 0, 'R': 0}, 'outcome': 'neutral', 'pos': 0.0, 'neg': 0.0, 'lesson': 0, 'insight': 0, 'reward': -0.752}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "prev_snap = TreeSnapshot(prakriti_tree).to_dict()\n",
        "\n",
        "entropy = prev_snap.get(\"entropy\", 0.0)\n",
        "stability = 1.0 - prev_snap.get(\"branch_flip_rate\", 0.0) if \"branch_flip_rate\" in prev_snap else 1.0\n",
        "depth_ratio = prev_snap.get(\"depth\", 1) / max(1, prev_snap.get(\"node_count\", 1))\n",
        "EIS = 0.4*entropy + 0.3*(1-stability) + 0.3*depth_ratio\n",
        "print(\"EIS:\", EIS)\n",
        "\n",
        "# create test new snapshot\n",
        "new_snap = dict(prev_snap)\n",
        "new_snap[\"entropy\"] = prev_snap[\"entropy\"] - 0.1\n",
        "\n",
        "# **Add missing required fields**\n",
        "new_snap[\"pos_score\"] = 0.0\n",
        "new_snap[\"neg_score\"] = 0.0\n",
        "\n",
        "tickets = {\"G\":0,\"B\":0,\"Y\":0,\"R\":0,\"P\":0}\n",
        "temp_tickets = {\"G\":0,\"B\":0,\"Y\":0,\"R\":0}\n",
        "decay_q = []\n",
        "\n",
        "reward, tickets, failure_memory, log = compute_reward(\n",
        "    prev_snap, new_snap, None, tickets, {}, temp_tickets, decay_q\n",
        ")\n",
        "\n",
        "print(\"Reward test:\", reward)\n",
        "print(\"Tickets:\", tickets)\n",
        "print(\"Log:\", log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSuvYnvo8QGn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMn0CziK8QiT"
      },
      "source": [
        "# Phase 7: RL Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzcnII3H8UEK",
        "outputId": "e7c7eea1-bb0e-4549-8e72-8791bfb5bd55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Tokens built: 93\n",
            "✅ data_bin shape: (1200, 93)\n",
            "   Appearance of Hair:Dry, black, knotted, brittle  \\\n",
            "0                                                0   \n",
            "1                                                1   \n",
            "2                                                1   \n",
            "\n",
            "   Appearance of Hair:Straight, oily  Appearance of Hair:Thick, curly  \\\n",
            "0                                  1                                0   \n",
            "1                                  0                                0   \n",
            "2                                  0                                0   \n",
            "\n",
            "   Appetite:Irregular, Scanty  Appetite:Slow but steady  \\\n",
            "0                           0                         1   \n",
            "1                           0                         1   \n",
            "2                           0                         1   \n",
            "\n",
            "   Appetite:Strong, Unbearable  Blinking of Eyes:Excessive Blinking  \\\n",
            "0                            0                                    0   \n",
            "1                            0                                    0   \n",
            "2                            0                                    0   \n",
            "\n",
            "   Blinking of Eyes:Moderate Blinking  Blinking of Eyes:More or less stable  \\\n",
            "0                                   1                                     0   \n",
            "1                                   1                                     0   \n",
            "2                                   1                                     0   \n",
            "\n",
            "   Body Size:Large  \n",
            "0                0  \n",
            "1                0  \n",
            "2                0  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Phase 7 — Cell 1: Build (data_bin, feature_vectors) for Phase2Env\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "assert 'df' in globals(), \"df missing (Phase 6 output).\"\n",
        "assert 'column_value_vectors' in globals(), \"column_value_vectors missing (Phase 6 value embeddings).\"\n",
        "\n",
        "# 1) build the token universe and the feature_vectors dict (token -> 50-d vector)\n",
        "token_list = []\n",
        "feature_vectors = {}\n",
        "for col, pack in column_value_vectors.items():\n",
        "    vals = pack[\"values\"]\n",
        "    vecs = pack[\"vectors\"]  # 2D np array, padded/trimmed to 50 in Phase 6\n",
        "    for v, vv in zip(vals, vecs):\n",
        "        tok = f\"{col}:{v}\"\n",
        "        token_list.append(tok)\n",
        "        feature_vectors[tok] = torch.tensor(vv[:50], dtype=torch.float32)\n",
        "\n",
        "token_list = sorted(set(token_list))\n",
        "token_index = {t:i for i,t in enumerate(token_list)}\n",
        "print(f\"✅ Tokens built: {len(token_list)}\")\n",
        "\n",
        "# 2) build a binary matrix rows x tokens\n",
        "rows = []\n",
        "for _, row in df.iterrows():\n",
        "    binrow = np.zeros(len(token_list), dtype=np.int8)\n",
        "    for col in df.columns:\n",
        "        val = str(row[col])\n",
        "        tok = f\"{col}:{val}\"\n",
        "        if tok in token_index:\n",
        "            binrow[token_index[tok]] = 1\n",
        "    rows.append(binrow)\n",
        "\n",
        "data_bin = pd.DataFrame(rows, columns=token_list)\n",
        "print(\"✅ data_bin shape:\", data_bin.shape)\n",
        "\n",
        "# Keep a tiny view\n",
        "print(data_bin.head(3).iloc[:, : min(10, data_bin.shape[1])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyZrDXhMOb40"
      },
      "outputs": [],
      "source": [
        "\n",
        "def reset_global_env(env, global_tree):\n",
        "    import copy\n",
        "    env.tree = copy.deepcopy(global_tree)  # freeze master tree\n",
        "    env.snapshot = TreeSnapshot(env.tree).to_dict()\n",
        "    env.steps = 0\n",
        "\n",
        "    # seed tickets\n",
        "    env.tickets = {\"G\": 5, \"B\": 10, \"Y\": 3, \"R\": 0, \"P\": 0}\n",
        "    env.temp_tickets = {\"G\":0,\"B\":0,\"Y\":0,\"R\":0}\n",
        "    env.decay_queue = []\n",
        "\n",
        "    return env._snapshot_to_obs(env.snapshot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Agb9yDZSe-T"
      },
      "outputs": [],
      "source": [
        "\n",
        "#def seed_env_tickets(env):\n",
        "   # env.tickets = {\"G\": 5, \"B\": 8, \"Y\": 2, \"R\": 0, \"P\": 0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwgpYZs08ZUe",
        "outputId": "040e9674-936a-4a40-c72e-2b6c2c37b6d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ RL Environment ready.\n",
            "First obs: [ 1. 31. 30. 30.  0. 30.] shape: (6,)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Phase 7 — Cell 2: Environment wiring\n",
        "import torch\n",
        "from TreeBuilderV2 import TreeBuilderV2\n",
        "from Phase2Env import Phase2Env  # your existing env\n",
        "from TreeSnapshot import TreeSnapshot\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# IMPORTANT: Phase2Env.builder.dim must match the vectors used inside env->build_tree.\n",
        "# Our feature_vectors are 50-D (TF-IDF+PCA padded), so:\n",
        "builder = TreeBuilderV2(device=device, dim=50, mode=\"three\")\n",
        "\n",
        "# Construct env with data_bin & feature_vectors\n",
        "env = Phase2Env(builder, data_bin, feature_vectors, max_edits=20)\n",
        "print(\"✅ RL Environment ready.\")\n",
        "\n",
        "# quick reset\n",
        "obs = env.reset( idx=np.random.randint(0, len(data_bin)) )\n",
        "print(\"First obs:\", obs, \"shape:\", obs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uwWrDZ6NGuf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Put this in a new cell ---\n",
        "\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "def seed_env_tickets(env, B=8, G=5, Y=2, R=0, P=0):\n",
        "    env.tickets.update({\"B\": B, \"G\": G, \"Y\": Y, \"R\": R, \"P\": P})\n",
        "\n",
        "def sample_valid_action_basic(tree):\n",
        "    \"\"\"Always returns a concrete action; lets env.step() enforce gates.\"\"\"\n",
        "    # collect all nodes\n",
        "    nodes = []\n",
        "    q = deque([tree])\n",
        "    while q:\n",
        "        n = q.popleft()\n",
        "        nodes.append(n)\n",
        "        for c in getattr(n, \"children\", []) or []:\n",
        "            q.append(c)\n",
        "\n",
        "    # fallback to root if somehow empty\n",
        "    if not nodes:\n",
        "        return (\"prune\", getattr(tree, \"node_id\", None), None)\n",
        "\n",
        "    tgt = random.choice(nodes)\n",
        "    op  = random.choice([\"split\", \"prune\", \"lock\", \"unlock\"])  # (skip reorder for now)\n",
        "    extra = \"soft\" if op == \"lock\" else None\n",
        "    return (op, getattr(tgt, \"node_id\", None), extra)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C15tyg70FFhM",
        "outputId": "edd91788-977b-4e6a-eeda-8cdfaa840276"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1: total_reward=-0.025 info={'action': 'split', 'target': 'row825_12_1', 'extra': None, 'status': 'ok', 'EIS': 0.65, 'IS': 0.13, 'coeffs': {'alpha': 1.195, 'beta': 0.87, 'gamma': 1.325, 'delta': 1.39}, 'tickets': {'G': 3, 'B': 21, 'Y': 2, 'R': 0, 'P': 0}, 'temp_tickets': {'G': 0, 'B': 0, 'Y': 0, 'R': 0}, 'outcome': 'neutral', 'pos': 0.16331, 'neg': 0.117, 'lesson': 0, 'insight': 0, 'reward': -0.166619}\n",
            "Episode 2: total_reward=0.917 info={'action': 'reorder', 'target': 'row1056_24_0_R', 'extra': None, 'status': 'fail', 'EIS': 0.018, 'IS': 0.004, 'coeffs': {'alpha': 1.005, 'beta': 0.996, 'gamma': 1.009, 'delta': 1.011}, 'tickets': {'G': 3, 'B': 21, 'Y': 2, 'R': 0, 'P': 0}, 'temp_tickets': {'G': 0, 'B': 0, 'Y': 0, 'R': 0}, 'outcome': 'neutral', 'pos': 0.0, 'neg': 0.0, 'lesson': 0, 'insight': 0, 'reward': -0.008}\n",
            "Episode 3: total_reward=-1.115 info={'action': 'reorder', 'target': 'row1087_18_2', 'extra': None, 'status': 'fail', 'EIS': 0.943, 'IS': 0.189, 'coeffs': {'alpha': 1.283, 'beta': 0.811, 'gamma': 1.472, 'delta': 1.566}, 'tickets': {'G': 3, 'B': 21, 'Y': 2, 'R': 0, 'P': 0}, 'temp_tickets': {'G': 0, 'B': 0, 'Y': 0, 'R': 0}, 'outcome': 'neutral', 'pos': 0.0, 'neg': 0.0, 'lesson': 0, 'insight': 0, 'reward': -0.378}\n",
            "Episode 4: total_reward=-0.079 info={'action': 'split', 'target': 'row141_24_0', 'extra': None, 'status': 'ok', 'EIS': 0.943, 'IS': 0.189, 'coeffs': {'alpha': 1.283, 'beta': 0.811, 'gamma': 1.472, 'delta': 1.566}, 'tickets': {'G': 3, 'B': 21, 'Y': 2, 'R': 0, 'P': 0}, 'temp_tickets': {'G': 0, 'B': 0, 'Y': 0, 'R': 0}, 'outcome': 'neutral', 'pos': 0.06531, 'neg': 0.06438, 'lesson': 0, 'insight': 0, 'reward': -0.346439}\n",
            "Episode 5: total_reward=-1.183 info={'action': 'reorder', 'target': 'row840_24_2', 'extra': None, 'status': 'fail', 'EIS': 0.815, 'IS': 0.163, 'coeffs': {'alpha': 1.245, 'beta': 0.837, 'gamma': 1.408, 'delta': 1.489}, 'tickets': {'G': 3, 'B': 21, 'Y': 2, 'R': 0, 'P': 0}, 'temp_tickets': {'G': 0, 'B': 0, 'Y': 0, 'R': 0}, 'outcome': 'neutral', 'pos': 0.0, 'neg': 0.0, 'lesson': 0, 'insight': 0, 'reward': -0.326}\n",
            "Episode 6: total_reward=-1.562 info={'action': 'unlock', 'target': 'row811_3_1', 'extra': None, 'status': 'ok', 'EIS': 0.815, 'IS': 0.163, 'coeffs': {'alpha': 1.245, 'beta': 0.837, 'gamma': 1.408, 'delta': 1.489}, 'tickets': {'G': 3, 'B': 21, 'Y': 2, 'R': 0, 'P': 0}, 'temp_tickets': {'G': 0, 'B': 0, 'Y': 0, 'R': 0}, 'outcome': 'neutral', 'pos': 0.0, 'neg': 0.0, 'lesson': 0, 'insight': 0, 'reward': -0.326}\n",
            "Episode 7: total_reward=1.006 info={'action': 'prune', 'target': 'row711_12_2', 'extra': None, 'status': 'ok', 'EIS': 0.026, 'IS': 0.005, 'coeffs': {'alpha': 1.008, 'beta': 0.995, 'gamma': 1.013, 'delta': 1.015}, 'tickets': {'G': 4, 'B': 18, 'Y': 2, 'R': 1, 'P': 0}, 'temp_tickets': {'G': 0, 'B': 0, 'Y': 0, 'R': 0}, 'outcome': 'neutral', 'pos': 0.0, 'neg': 0.0, 'lesson': 0, 'insight': 0, 'reward': -0.01}\n",
            "Episode 8: total_reward=5.053 info={'action': 'split', 'target': 'row4', 'extra': None, 'status': 'ok', 'EIS': 0.1, 'IS': 0.02, 'coeffs': {'alpha': 1.03, 'beta': 0.98, 'gamma': 1.05, 'delta': 1.06}, 'tickets': {'G': 7, 'B': 18, 'Y': 2, 'R': 0, 'P': 0}, 'temp_tickets': {'G': 0, 'B': 0, 'Y': 0, 'R': 0}, 'outcome': 'success', 'pos': 0.15, 'neg': 0.0, 'lesson': 0, 'insight': 0, 'reward': 0.1145}\n",
            "Episode 9: total_reward=0.917 info={'action': 'reorder', 'target': 'row735_15_0', 'extra': None, 'status': 'fail', 'EIS': 0.018, 'IS': 0.004, 'coeffs': {'alpha': 1.005, 'beta': 0.996, 'gamma': 1.009, 'delta': 1.011}, 'tickets': {'G': 3, 'B': 21, 'Y': 2, 'R': 0, 'P': 0}, 'temp_tickets': {'G': 0, 'B': 0, 'Y': 0, 'R': 0}, 'outcome': 'neutral', 'pos': 0.0, 'neg': 0.0, 'lesson': 0, 'insight': 0, 'reward': -0.008}\n",
            "Episode 10: total_reward=-0.393 info={'action': 'reorder', 'target': 'row837_0_1', 'extra': None, 'status': 'fail', 'EIS': 0.65, 'IS': 0.13, 'coeffs': {'alpha': 1.195, 'beta': 0.87, 'gamma': 1.325, 'delta': 1.39}, 'tickets': {'G': 3, 'B': 21, 'Y': 2, 'R': 0, 'P': 0}, 'temp_tickets': {'G': 0, 'B': 0, 'Y': 0, 'R': 0}, 'outcome': 'neutral', 'pos': 0.0, 'neg': 0.0, 'lesson': 0, 'insight': 0, 'reward': -0.26}\n",
            "Episode 11: total_reward=-0.133 info={'action': 'split', 'target': 'row1015_24_0', 'extra': None, 'status': 'ok', 'EIS': 0.815, 'IS': 0.163, 'coeffs': {'alpha': 1.245, 'beta': 0.837, 'gamma': 1.408, 'delta': 1.489}, 'tickets': {'G': 3, 'B': 21, 'Y': 2, 'R': 0, 'P': 0}, 'temp_tickets': {'G': 0, 'B': 0, 'Y': 0, 'R': 0}, 'outcome': 'neutral', 'pos': 0.098, 'neg': 0.083, 'lesson': 0, 'insight': 0, 'reward': -0.273492}\n",
            "Episode 12: total_reward=5.989 info={'action': 'lock', 'target': 'row202', 'extra': 'soft', 'status': 'ok', 'EIS': 0.1, 'IS': 0.02, 'coeffs': {'alpha': 1.03, 'beta': 0.98, 'gamma': 1.05, 'delta': 1.06}, 'tickets': {'G': 5, 'B': 17, 'Y': 2, 'R': 1, 'P': 0}, 'temp_tickets': {'G': 0, 'B': 0, 'Y': 0, 'R': 0}, 'outcome': 'neutral', 'pos': 0.0, 'neg': 0.0, 'lesson': 0, 'insight': 0, 'reward': -0.04}\n",
            "Episode 13: total_reward=0.929 info={'action': 'prune', 'target': 'row496_9_1', 'extra': None, 'status': 'ok', 'EIS': 0.018, 'IS': 0.004, 'coeffs': {'alpha': 1.005, 'beta': 0.996, 'gamma': 1.009, 'delta': 1.011}, 'tickets': {'G': 3, 'B': 21, 'Y': 2, 'R': 0, 'P': 0}, 'temp_tickets': {'G': 0, 'B': 0, 'Y': 0, 'R': 0}, 'outcome': 'neutral', 'pos': 0.0, 'neg': 0.0, 'lesson': 0, 'insight': 0, 'reward': -0.008}\n",
            "Episode 14: total_reward=0.005 info={'action': 'prune', 'target': 'row987_9_2', 'extra': None, 'status': 'ok', 'EIS': 0.65, 'IS': 0.13, 'coeffs': {'alpha': 1.195, 'beta': 0.87, 'gamma': 1.325, 'delta': 1.39}, 'tickets': {'G': 3, 'B': 21, 'Y': 2, 'R': 0, 'P': 0}, 'temp_tickets': {'G': 0, 'B': 0, 'Y': 0, 'R': 0}, 'outcome': 'neutral', 'pos': 0.0, 'neg': 0.0, 'lesson': 0, 'insight': 0, 'reward': -0.26}\n",
            "Episode 15: total_reward=-1.249 info={'action': 'reorder', 'target': 'row1054_18_0_R', 'extra': None, 'status': 'fail', 'EIS': 0.815, 'IS': 0.163, 'coeffs': {'alpha': 1.245, 'beta': 0.837, 'gamma': 1.408, 'delta': 1.489}, 'tickets': {'G': 3, 'B': 21, 'Y': 2, 'R': 0, 'P': 0}, 'temp_tickets': {'G': 0, 'B': 0, 'Y': 0, 'R': 0}, 'outcome': 'neutral', 'pos': 0.0, 'neg': 0.0, 'lesson': 0, 'insight': 0, 'reward': -0.326}\n",
            "Episode 16: total_reward=-0.378 info={'action': 'prune', 'target': 'row809_9_1', 'extra': None, 'status': 'ok', 'EIS': 0.657, 'IS': 0.131, 'coeffs': {'alpha': 1.197, 'beta': 0.869, 'gamma': 1.329, 'delta': 1.394}, 'tickets': {'G': 3, 'B': 21, 'Y': 2, 'R': 0, 'P': 0}, 'temp_tickets': {'G': 0, 'B': 0, 'Y': 0, 'R': 0}, 'outcome': 'neutral', 'pos': 0.0, 'neg': 0.0, 'lesson': 0, 'insight': 0, 'reward': -0.262}\n",
            "Episode 17: total_reward=-0.608 info={'action': 'reorder', 'target': 'row881_24_1_R', 'extra': None, 'status': 'fail', 'EIS': 0.65, 'IS': 0.13, 'coeffs': {'alpha': 1.195, 'beta': 0.87, 'gamma': 1.325, 'delta': 1.39}, 'tickets': {'G': 3, 'B': 21, 'Y': 2, 'R': 0, 'P': 0}, 'temp_tickets': {'G': 0, 'B': 0, 'Y': 0, 'R': 0}, 'outcome': 'neutral', 'pos': 0.0, 'neg': 0.0, 'lesson': 0, 'insight': 0, 'reward': -0.26}\n",
            "Episode 18: total_reward=0.129 info={'action': 'split', 'target': 'row306_3_2', 'extra': None, 'status': 'ok', 'EIS': 0.65, 'IS': 0.13, 'coeffs': {'alpha': 1.195, 'beta': 0.87, 'gamma': 1.325, 'delta': 1.39}, 'tickets': {'G': 3, 'B': 21, 'Y': 2, 'R': 0, 'P': 0}, 'temp_tickets': {'G': 0, 'B': 0, 'Y': 0, 'R': 0}, 'outcome': 'neutral', 'pos': 0.16331, 'neg': 0.117, 'lesson': 0, 'insight': 0, 'reward': -0.166619}\n",
            "Episode 19: total_reward=0.766 info={'action': 'reorder', 'target': 'row1168_6_0', 'extra': None, 'status': 'fail', 'EIS': 0.417, 'IS': 0.083, 'coeffs': {'alpha': 1.125, 'beta': 0.917, 'gamma': 1.209, 'delta': 1.25}, 'tickets': {'G': 3, 'B': 21, 'Y': 2, 'R': 0, 'P': 0}, 'temp_tickets': {'G': 0, 'B': 0, 'Y': 0, 'R': 0}, 'outcome': 'neutral', 'pos': 0.0, 'neg': 0.0, 'lesson': 0, 'insight': 0, 'reward': -0.166}\n",
            "Episode 20: total_reward=-0.573 info={'action': 'split', 'target': 'row695_24_1', 'extra': None, 'status': 'ok', 'EIS': 0.815, 'IS': 0.163, 'coeffs': {'alpha': 1.245, 'beta': 0.837, 'gamma': 1.408, 'delta': 1.489}, 'tickets': {'G': 3, 'B': 21, 'Y': 2, 'R': 0, 'P': 0}, 'temp_tickets': {'G': 0, 'B': 0, 'Y': 0, 'R': 0}, 'outcome': 'neutral', 'pos': 0.098, 'neg': 0.083, 'lesson': 0, 'insight': 0, 'reward': -0.273492}\n",
            "Random-policy mean reward: 0.42079864999999994\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Phase 7 — Cell 3: Sanity rollout with random valid actions (root-protected)\n",
        "import random\n",
        "import copy\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "ROOT_IDS = {\"prakriti_global\"}  # protect the real root\n",
        "\n",
        "def sample_valid_action(tree, tickets):\n",
        "    def is_protected(node_id):\n",
        "        return node_id in ROOT_IDS\n",
        "\n",
        "    nodes = []\n",
        "    q = deque([tree])\n",
        "    while q:\n",
        "        n = q.popleft()\n",
        "        # skip protected nodes\n",
        "        if not is_protected(getattr(n, \"node_id\", None)):\n",
        "            nodes.append(n)\n",
        "        for c in getattr(n, \"children\", []) or []:\n",
        "            q.append(c)\n",
        "\n",
        "    if not nodes:\n",
        "        return (\"noop\", None, None)  # nothing safe to do\n",
        "\n",
        "    for _ in range(40):\n",
        "        tgt = random.choice(nodes)\n",
        "        op = random.choice([\"split\",\"prune\",\"reorder\",\"lock\",\"unlock\"])\n",
        "        extra = \"soft\" if op==\"lock\" else None\n",
        "        action = (op, tgt.node_id, extra)\n",
        "\n",
        "        if \"check_action_allowed\" in globals():\n",
        "            ok, cost, reason = check_action_allowed(action, tickets)\n",
        "            if ok:\n",
        "                return action\n",
        "        else:\n",
        "            return action\n",
        "\n",
        "    return (\"noop\", None, None)\n",
        "\n",
        "\n",
        "def run_random_episode(env, max_steps=10):\n",
        "    # reset to the global deep tree\n",
        "    #obs = reset_global_env(env, prakriti_tree)\n",
        "    obs = env.reset(idx=np.random.randint(0, len(data_bin)))\n",
        "    seed_env_tickets(env)   # IMPORTANT\n",
        "    total_reward = 0.0\n",
        "    info = {}\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        action = sample_valid_action(env.tree, env.tickets)\n",
        "        if action[0] == \"noop\":\n",
        "            break\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "    return total_reward, info\n",
        "\n",
        "\n",
        "# run episodes\n",
        "scores = []\n",
        "for ep in range(20):\n",
        "    R, info = run_random_episode(env, max_steps=12)\n",
        "    scores.append(R)\n",
        "    print(f\"Episode {ep+1}: total_reward={R:.3f} info={info.get('log',{})}\")\n",
        "\n",
        "print(\"Random-policy mean reward:\", float(np.mean(scores)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1D6go3CLPZ-x"
      },
      "outputs": [],
      "source": [
        "\n",
        "def greedy_action(env):\n",
        "    best_reward = -1e9\n",
        "    best_action = None\n",
        "\n",
        "    #def is_protected_node(node_id):\n",
        "        #return node_id in [\"prakriti_global\", \"root\", \"prakriti_root\"]\n",
        "\n",
        "    # enumerate candidate actions\n",
        "    actions = []\n",
        "    from collections import deque\n",
        "    q = deque([env.tree])\n",
        "    nodes = []\n",
        "    while q:\n",
        "        n = q.popleft()\n",
        "        nodes.append(n)\n",
        "        for c in getattr(n, \"children\", []) or []:\n",
        "            q.append(c)\n",
        "\n",
        "    ops = [\"split\", \"prune\", \"lock\", \"unlock\"]\n",
        "    for node in nodes:\n",
        "        for op in ops:\n",
        "            extra = \"soft\" if op==\"lock\" else None\n",
        "            action = (op, node.node_id, extra)\n",
        "\n",
        "            ok, cost, reason = check_action_allowed(action, env.tickets)\n",
        "            if not ok:\n",
        "                continue\n",
        "\n",
        "            # simulate step without modifying real tree\n",
        "            snap_before = TreeSnapshot(env.tree).to_dict()\n",
        "            tree_copy = copy.deepcopy(env.tree)\n",
        "            tickets_copy = env.tickets.copy()\n",
        "\n",
        "            log = apply_action(tree_copy, action)\n",
        "            snap_after = TreeSnapshot(tree_copy).to_dict()\n",
        "\n",
        "            reward, _, _, _ = compute_reward(\n",
        "                snap_before, snap_after,\n",
        "                mode=\"train\",\n",
        "                tickets=tickets_copy,\n",
        "                failure_memory={},\n",
        "                temp_tickets={},\n",
        "                decay_queue=[]\n",
        "            )\n",
        "\n",
        "            if reward > best_reward:\n",
        "                best_reward = reward\n",
        "                best_action = action\n",
        "\n",
        "    return best_action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f60GkIoPaNv"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_greedy_episode(env, max_steps=10):\n",
        "    obs = reset_global_env(env, prakriti_tree)\n",
        "    total_reward = 0\n",
        "    for step in range(max_steps):\n",
        "        action = greedy_action(env)\n",
        "        if action is None:\n",
        "            break\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "    return total_reward, info\n",
        "\n",
        "scores = []\n",
        "for ep in range(10):\n",
        "    R, info = run_greedy_episode(env)\n",
        "    scores.append(R)\n",
        "    print(\"Episode\", ep+1, \"Reward:\", R)\n",
        "\n",
        "print(\"Greedy mean reward:\", np.mean(scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-jmZg4uFF1-",
        "outputId": "b5805df0-0767-4fc6-e52c-8dd92006138d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy baseline mean reward: -0.24034649999999963\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Phase 7 — Cell 4: Greedy baseline via A-CRES gating + quick scoring\n",
        "def candidate_set(tree):\n",
        "    # small candidate pool per step\n",
        "    from collections import deque\n",
        "    nodes = []\n",
        "    q = deque([tree])\n",
        "    while q:\n",
        "        n = q.popleft()\n",
        "        nodes.append(n)\n",
        "        for c in getattr(n, \"children\", []) or []:\n",
        "            q.append(c)\n",
        "    if not nodes: return []\n",
        "    cands = []\n",
        "    for _ in range(12):\n",
        "        tgt = random.choice(nodes)\n",
        "        op = random.choice([\"split\",\"prune\",\"reorder\",\"lock\",\"unlock\"])\n",
        "        extra = \"soft\" if op==\"lock\" else None\n",
        "        cands.append((op, tgt.node_id, extra))\n",
        "    return cands\n",
        "\n",
        "def greedy_episode(env, max_steps=12):\n",
        "    obs = env.reset(idx=np.random.randint(0, len(data_bin)))\n",
        "    total_R = 0.0\n",
        "    for step in range(max_steps):\n",
        "        cands = candidate_set(env.tree)\n",
        "        # filter by gating\n",
        "        allowed = []\n",
        "        for a in cands:\n",
        "            if \"check_action_allowed\" in globals():\n",
        "                ok, cost, reason = check_action_allowed(a, env.tickets)\n",
        "                if ok: allowed.append((a, cost))\n",
        "            else:\n",
        "                allowed.append((a, {}))\n",
        "        if not allowed:\n",
        "            # fallback\n",
        "            a = (\"prune\", getattr(env.tree,\"node_id\",None), None)\n",
        "        else:\n",
        "            # rudimentary shape score using EIS delta heuristic on a copy\n",
        "            best = None; best_score = -1e9\n",
        "            for (a, _) in allowed:\n",
        "                # just prefer structure-shaping ops\n",
        "                op = a[0]\n",
        "                score = {\"split\": +2.0, \"reorder\": +0.5, \"lock\": +0.1, \"unlock\": +0.1, \"prune\": -0.1}.get(op, 0.0)\n",
        "                if score > best_score:\n",
        "                    best = a; best_score = score\n",
        "            a = best\n",
        "        obs, reward, done, info = env.step(a)\n",
        "        total_R += reward\n",
        "        if done: break\n",
        "    return total_R\n",
        "\n",
        "greedy_scores = [greedy_episode(env) for _ in range(10)]\n",
        "print(\"Greedy baseline mean reward:\", float(np.mean(greedy_scores)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPAP5HtQFGAN",
        "outputId": "27ab7351-0a75-4503-bddc-d5b41730c10f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before: {'depth': 1, 'node_count': 31, 'leaf_count': 30, 'branching_factor': 30.0, 'entropy': 0.0, 'weak_leaves': 30}\n",
            "After : {'depth': 2, 'node_count': 5, 'leaf_count': 3, 'branching_factor': 2.0, 'entropy': -0.0, 'weak_leaves': 3}\n",
            "\n",
            "=== Explanation (sample) ===\n",
            " Node None anchored as None; entropy 0.00, branching 0.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Phase 7 — Cell 5: Evaluation metrics + explanation\n",
        "\n",
        "# snapshot BEFORE a greedy episode\n",
        "_ = env.reset(idx=np.random.randint(0, len(data_bin)))\n",
        "snap_before = TreeSnapshot(env.tree).to_dict()\n",
        "\n",
        "_ = greedy_episode(env)\n",
        "snap_after = TreeSnapshot(env.tree).to_dict()\n",
        "\n",
        "def pick(x,k): return {kk:x.get(kk) for kk in k}\n",
        "keys = [\"depth\",\"node_count\",\"leaf_count\",\"branching_factor\",\"entropy\",\"weak_leaves\"]\n",
        "print(\"Before:\", pick(snap_before, keys))\n",
        "print(\"After :\", pick(snap_after,  keys))\n",
        "\n",
        "# Build a human explanation\n",
        "try:\n",
        "    expl = build_explanation(snap_after, final_anchors, retrievals=[])\n",
        "    print(\"\\n=== Explanation (sample) ===\\n\", expl[:2000])\n",
        "except Exception as e:\n",
        "    print(\"Explanation error:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bwQ_wTT93zF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiUfgej-pWeT"
      },
      "source": [
        "# Phase 8(Clean version) (use this only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNgAaG9WGI9N"
      },
      "outputs": [],
      "source": [
        "\n",
        "teacher_pos3 = []\n",
        "experience_neg3 = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrHAvEUApZLA",
        "outputId": "a5ada084-578c-470f-a38a-0debf700464b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing phase8_clean.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile phase8_clean.py\n",
        "# ===============================\n",
        "# PHASE 8 — CLEAN (self-contained)\n",
        "# ===============================\n",
        "import os, pickle, copy, random\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "from collections import deque\n",
        "from TLiteComponents import TLiteV6\n",
        "from tlite_rl_bridge import encode_state_to_vector, encode_action_to_vector\n",
        "from check_action_allowed import check_action_allowed\n",
        "from TreeSnapshot import TreeSnapshot\n",
        "from apply_action import apply_action\n",
        "\n",
        "# -------------------\n",
        "# Device & policy\n",
        "# -------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def init_policy(existing=None, dim=50, lr=1e-4):\n",
        "    \"\"\"\n",
        "    Reuse an existing TLiteV6 if provided, else create a fresh one.\n",
        "    Returns (policy, optimizer, mse_loss)\n",
        "    \"\"\"\n",
        "    if existing is None:\n",
        "        policy = TLiteV6(dim=dim, device=device).to(device)\n",
        "    else:\n",
        "        policy = existing\n",
        "    opt = torch.optim.Adam(policy.parameters(), lr=lr)\n",
        "    mse = nn.MSELoss()\n",
        "    return policy, opt, mse\n",
        "\n",
        "# -------------------\n",
        "# Simple teacher score\n",
        "# -------------------\n",
        "def teacher_score(prev, new):\n",
        "    \"\"\"\n",
        "    Higher is better.\n",
        "    Entropy ↓, weak_leaves ↓, branching_factor → 3.0\n",
        "    \"\"\"\n",
        "    dE = prev[\"entropy\"] - new[\"entropy\"]\n",
        "    dW = prev[\"weak_leaves\"] - new[\"weak_leaves\"]\n",
        "    dB = abs(3.0 - new[\"branching_factor\"])\n",
        "    return 1.4*dE + 1.6*dW - 0.3*dB\n",
        "\n",
        "# -------------------\n",
        "# Legal actions (structural only)\n",
        "# -------------------\n",
        "ROOT_IDS = {\"prakriti_global\", \"root\", \"prakriti_root\"}\n",
        "STRUCT_OPS = (\"split\", \"prune\")  # cheap & safe\n",
        "\n",
        "def _is_protected(nid): return nid in ROOT_IDS\n",
        "def _is_leaf(n): return not getattr(n, \"children\", [])\n",
        "\n",
        "\n",
        "def legal_candidates_clean(tree, tickets, ops=STRUCT_OPS):\n",
        "    cands, q = [], deque([tree])\n",
        "    while q:\n",
        "        n = q.popleft()\n",
        "        nid = getattr(n, \"node_id\", None)\n",
        "        if _is_protected(nid):\n",
        "            for ch in getattr(n, \"children\", []) or []:\n",
        "                q.append(ch)\n",
        "            continue\n",
        "        for op in ops:\n",
        "            a = (op, nid, None)\n",
        "            ok, _, _ = check_action_allowed(a, tickets)\n",
        "            if not ok:\n",
        "                continue\n",
        "            if op == \"prune\" and _is_leaf(n):  # skip pointless prune\n",
        "                continue\n",
        "            cands.append(a)\n",
        "        for ch in getattr(n, \"children\", []) or []:\n",
        "            q.append(ch)\n",
        "    return cands\n",
        "\n",
        "# -------------------\n",
        "# Clone + evaluate gain offline\n",
        "# -------------------\n",
        "def clone_tree(root):\n",
        "    if root is None: return None\n",
        "    cloned = type(root)(\n",
        "        node_id=root.node_id, value=root.value, children=[],\n",
        "        level=root.level, rule=root.rule, confidence=root.confidence,\n",
        "        lock_flag=root.lock_flag, provenance=root.provenance,\n",
        "        difficulty_tag=root.difficulty_tag, branch_tag=root.branch_tag,\n",
        "    )\n",
        "    if getattr(root, \"cached_vector\", None) is not None:\n",
        "        try: cloned.cached_vector = copy.deepcopy(root.cached_vector)\n",
        "        except Exception: cloned.cached_vector = None\n",
        "    for ch in getattr(root, \"children\", []) or []:\n",
        "        cloned.add_child(clone_tree(ch))\n",
        "    return cloned\n",
        "\n",
        "def snapshot_of(root): return TreeSnapshot(root).to_dict()\n",
        "\n",
        "def eval_action_gain(snapshot_before, tree_root, action):\n",
        "    try:\n",
        "        tmp = clone_tree(tree_root)\n",
        "        log = apply_action(tmp, action)\n",
        "        if log.get(\"status\") in (\"ok\", \"unknown_action\", \"fail\"):\n",
        "            snap_after = snapshot_of(tmp)\n",
        "            return teacher_score(snapshot_before, snap_after), snap_after\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None, None\n",
        "\n",
        "# -------------------\n",
        "# STOP-aware actor\n",
        "# -------------------\n",
        "def select_action_stop_aware(snapshot, env, policy, score_floor=0.25, temperature=1.2, epsilon=0.15):\n",
        "    cands = legal_candidates_clean(env.tree, env.tickets)\n",
        "    if not cands: return None\n",
        "\n",
        "    if np.random.rand() < epsilon:\n",
        "        return random.choice(cands)\n",
        "\n",
        "    scores = []\n",
        "    with torch.no_grad():\n",
        "        for a in cands:\n",
        "            v = encode_action_to_vector(a)\n",
        "            s = policy(torch.tensor(v, dtype=torch.float32, device=device).unsqueeze(0)).item()\n",
        "            # tiny heuristic nudge\n",
        "            bf = snapshot.get(\"branching_factor\", 0.0)\n",
        "            ent = snapshot.get(\"entropy\", 0.0)\n",
        "            if a[0] == \"split\" and bf < 3.0: s += 0.10\n",
        "            if a[0] == \"prune\" and (bf > 6.0 or ent > 1.0): s += 0.10\n",
        "            scores.append(s)\n",
        "\n",
        "    if not scores:\n",
        "        return None\n",
        "    best = max(scores)\n",
        "    if best <= score_floor:\n",
        "        return None  # STOP\n",
        "\n",
        "    z = np.array(scores, dtype=np.float64) / max(1e-6, float(temperature))\n",
        "    z -= z.max()\n",
        "    p = np.exp(z); p /= p.sum()\n",
        "    idx = int(np.random.choice(len(cands), p=p))\n",
        "    return cands[idx]\n",
        "\n",
        "def run_episode_stop_aware(env, global_tree, policy, max_steps=10, score_floor=0.25):\n",
        "    _ = env.reset(use_global_tree=True, global_tree=global_tree)\n",
        "    env.tickets.update({\"B\":20,\"G\":6,\"Y\":2,\"R\":0,\"P\":0})\n",
        "    total = 0.0\n",
        "    for _ in range(max_steps):\n",
        "        a = select_action_stop_aware(env.snapshot, env, policy, score_floor=score_floor)\n",
        "        if a is None: break\n",
        "        _, r, done, _ = env.step(a)\n",
        "        total += float(r)\n",
        "        if done: break\n",
        "    return total\n",
        "\n",
        "# -------------------\n",
        "# Buffers (state, action, label)\n",
        "# -------------------\n",
        "teacher_pos3 = []     # (s_vec, a_vec, gain) including STOP-positive labels\n",
        "experience_neg3 = []  # (s_vec, a_vec, reward<0)\n",
        "\n",
        "def migrate_old_buffers():\n",
        "    \"\"\"\n",
        "    If older globals teacher_pos / experience_neg exist in the notebook,\n",
        "    normalize them into (s_vec, a_vec, label) triplets.\n",
        "    \"\"\"\n",
        "    def _zeros_state_vec(dim=64): return np.zeros(dim, dtype=np.float32)\n",
        "    added_t = added_n = 0\n",
        "    old_t = globals().get(\"teacher_pos\", [])\n",
        "    old_n = globals().get(\"experience_neg\", [])\n",
        "    for item in old_t:\n",
        "        if len(item) == 3:\n",
        "            s,a,g = item\n",
        "            teacher_pos3.append((np.asarray(s, np.float32), np.asarray(a, np.float32), float(g))); added_t += 1\n",
        "        elif len(item) == 2:\n",
        "            a,g = item\n",
        "            teacher_pos3.append((_zeros_state_vec(), np.asarray(a, np.float32), float(g))); added_t += 1\n",
        "    for item in old_n:\n",
        "        if len(item) == 3:\n",
        "            s,a,r = item\n",
        "            experience_neg3.append((np.asarray(s, np.float32), np.asarray(a, np.float32), float(r))); added_n += 1\n",
        "        elif len(item) == 2:\n",
        "            a,r = item\n",
        "            experience_neg3.append((_zeros_state_vec(), np.asarray(a, np.float32), float(r))); added_n += 1\n",
        "    print(f\"↻ Migrated teacher_pos: +{added_t} | experience_neg: +{added_n}\")\n",
        "\n",
        "# -------------------\n",
        "# Data collection\n",
        "# -------------------\n",
        "def collect_teacher_strong(env, global_tree, policy, episodes=6, max_steps=6, beam=20, min_gain=1e-6, stop_label=1.5):\n",
        "    \"\"\"\n",
        "    Adds (s,a,gain) for actions with positive true teacher gain.\n",
        "    If no candidate improves (or no actions), adds a STOP-positive label.\n",
        "    \"\"\"\n",
        "    added = 0\n",
        "    for _ in range(episodes):\n",
        "        _ = env.reset(use_global_tree=True, global_tree=global_tree)\n",
        "        env.tickets.update({\"B\":20,\"G\":6,\"Y\":2})\n",
        "        for _ in range(max_steps):\n",
        "            snap0 = dict(env.snapshot)\n",
        "            cands = legal_candidates_clean(env.tree, env.tickets)\n",
        "\n",
        "            if not cands:\n",
        "                teacher_pos3.append((encode_state_to_vector(snap0),\n",
        "                                     encode_action_to_vector((\"STOP\", None, None)),\n",
        "                                     float(stop_label)))\n",
        "                added += 1\n",
        "                break\n",
        "\n",
        "            # Rank by current actor\n",
        "            with torch.no_grad():\n",
        "                scored = []\n",
        "                for a in cands:\n",
        "                    v = encode_action_to_vector(a)\n",
        "                    s = policy(torch.tensor(v, dtype=torch.float32, device=device).unsqueeze(0)).item()\n",
        "                    scored.append((a, s))\n",
        "                scored.sort(key=lambda x: x[1], reverse=True)\n",
        "                top = [a for (a,_) in scored[:beam]]\n",
        "\n",
        "            # Evaluate true gains\n",
        "            gains = []\n",
        "            for a in top:\n",
        "                g, _ = eval_action_gain(snap0, env.tree, a)\n",
        "                gains.append((a, g))\n",
        "\n",
        "            # Pick best\n",
        "            best_action, best_gain = max(gains, key=lambda x: (x[1] if x[1] is not None else -1e9))\n",
        "\n",
        "            # If nothing helps, label STOP\n",
        "            if best_gain is None or best_gain <= 0:\n",
        "                teacher_pos3.append((encode_state_to_vector(snap0),\n",
        "                                     encode_action_to_vector((\"STOP\", None, None)),\n",
        "                                     float(stop_label)))\n",
        "                added += 1\n",
        "                break\n",
        "\n",
        "            # Otherwise add all positive gains\n",
        "            s_vec0 = encode_state_to_vector(snap0)\n",
        "            for a, g in gains:\n",
        "                if g is not None and g > min_gain:\n",
        "                    teacher_pos3.append((s_vec0, encode_action_to_vector(a), float(g)))\n",
        "                    added += 1\n",
        "\n",
        "            # Move with best-gain action\n",
        "            env.step(best_action)\n",
        "    print(f\"🧑‍🏫 Strong teacher positives: +{added} (total {len(teacher_pos3)})\")\n",
        "\n",
        "def collect_experience_negatives_clean(env, global_tree, policy=None, episodes=6, max_steps=6):\n",
        "    \"\"\"\n",
        "    Adds (s,a,reward) only when reward < 0 (strict negatives).\n",
        "    \"\"\"\n",
        "    added = 0\n",
        "    for _ in range(episodes):\n",
        "        _ = env.reset(use_global_tree=True, global_tree=global_tree)\n",
        "        env.tickets.update({\"B\":20,\"G\":6,\"Y\":2})\n",
        "        prev = dict(env.snapshot)\n",
        "        for _ in range(max_steps):\n",
        "            cands = legal_candidates_clean(env.tree, env.tickets)\n",
        "            if not cands: break\n",
        "            a = random.choice(cands)\n",
        "            s_prev = encode_state_to_vector(prev)\n",
        "            _, r, done, _ = env.step(a)\n",
        "            if float(r) < 0:\n",
        "                experience_neg3.append((s_prev, encode_action_to_vector(a), float(r)))\n",
        "                added += 1\n",
        "            prev = dict(env.snapshot)\n",
        "            if done: break\n",
        "    print(f\"📦 Experience negatives: +{added} (total {len(experience_neg3)})\")\n",
        "\n",
        "def collect_stop_preference(env, global_tree, policy, trials=8, score_floor=0.25):\n",
        "    \"\"\"\n",
        "    Compare STOP vs BEST-ACTION.\n",
        "    If STOP is better → reinforce STOP as positive.\n",
        "    If ACTION is better → reinforce ACTION as positive.\n",
        "    \"\"\"\n",
        "    added = 0\n",
        "    for _ in range(trials):\n",
        "        _ = env.reset(use_global_tree=True, global_tree=global_tree)\n",
        "        env.tickets.update({\"B\":20,\"G\":6,\"Y\":2})\n",
        "\n",
        "        snap0 = dict(env.snapshot)\n",
        "\n",
        "        # Evaluate STOP outcome\n",
        "        stop_reward = 0.0  # STOP = do nothing\n",
        "\n",
        "        # Evaluate best continuation\n",
        "        cands = legal_candidates_clean(env.tree, env.tickets)\n",
        "        if not cands:\n",
        "            continue_reward = -0.1  # weak penalty if nothing to do\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                best, best_score = None, -1e9\n",
        "                for a in cands:\n",
        "                    v = encode_action_to_vector(a)\n",
        "                    s = policy(torch.tensor(v, dtype=torch.float32, device=device).unsqueeze(0)).item()\n",
        "                    if s > best_score: best_score, best = s, a\n",
        "            _, continue_reward, _, _ = env.step(best)\n",
        "            continue_reward = float(continue_reward)\n",
        "\n",
        "        if stop_reward >= continue_reward:\n",
        "            teacher_pos3.append((\n",
        "                encode_state_to_vector(snap0),\n",
        "                encode_action_to_vector((\"STOP\", None, None)),\n",
        "                +2.0\n",
        "            ))\n",
        "        else:\n",
        "            teacher_pos3.append((\n",
        "                encode_state_to_vector(snap0),\n",
        "                encode_action_to_vector(best),\n",
        "                +1.5\n",
        "            ))\n",
        "        added += 1\n",
        "    print(f\"🟢 STOP-Preference labels: +{added} (total {len(teacher_pos3)})\")\n",
        "\n",
        "def collect_balanced_stop_depth(env, global_tree, policy, trials=8, score_floor=0.25):\n",
        "    \"\"\"\n",
        "    Phase 11 Balanced depth control:\n",
        "    If tree entropy is low → prefer STOP.\n",
        "    If entropy is high → encourage more expansion.\n",
        "    \"\"\"\n",
        "    added = 0\n",
        "    for _ in range(trials):\n",
        "        _ = env.reset(use_global_tree=True, global_tree=global_tree)\n",
        "        env.tickets.update({\"B\":20,\"G\":6,\"Y\":2})\n",
        "\n",
        "        snap = dict(env.snapshot)\n",
        "        ent = snap[\"entropy\"]\n",
        "\n",
        "        # STOP is better when tree is already neat\n",
        "        if ent < 2.2:\n",
        "            teacher_pos3.append((\n",
        "                encode_state_to_vector(snap),\n",
        "                encode_action_to_vector((\"STOP\", None, None)),\n",
        "                +1.8\n",
        "            ))\n",
        "        # CONTINUE is better when tree is messy\n",
        "        else:\n",
        "            cands = legal_candidates_clean(env.tree, env.tickets)\n",
        "            if not cands:\n",
        "                continue\n",
        "            with torch.no_grad():\n",
        "                scored = []\n",
        "                for a in cands:\n",
        "                    v = encode_action_to_vector(a)\n",
        "                    s = policy(torch.tensor(v, dtype=torch.float32, device=device).unsqueeze(0)).item()\n",
        "                    scored.append((a, s))\n",
        "                best, _ = max(scored, key=lambda x: x[1])\n",
        "\n",
        "            teacher_pos3.append((\n",
        "                encode_state_to_vector(snap),\n",
        "                encode_action_to_vector(best),\n",
        "                +1.6\n",
        "            ))\n",
        "        added += 1\n",
        "    print(f\"⚖️ Balanced STOP/Depth labels added: +{added} (total {len(teacher_pos3)})\")\n",
        "\n",
        "# -------------------\n",
        "# Training\n",
        "# -------------------\n",
        "def make_weighted_pairs_clean(positives, negatives, policy, k_neg=3, max_pairs=2048):\n",
        "    if not positives or not negatives:\n",
        "        print(\"⚠️ Need both positives & negatives.\"); return []\n",
        "    t_scores = np.array([r for (_,_,r) in positives], dtype=np.float32)\n",
        "    t_min, t_max = float(t_scores.min()), float(t_scores.max())\n",
        "    def w_norm(r):\n",
        "        if t_max <= t_min: return 1.0\n",
        "        z = (r - t_min) / (t_max - t_min)\n",
        "        return 0.1 + 0.9*float(z)\n",
        "    # Hard negatives by current actor\n",
        "    neg_scored = []\n",
        "    with torch.no_grad():\n",
        "        for (_, a_vec, r_neg) in negatives:\n",
        "            s = policy(torch.tensor(a_vec, dtype=torch.float32, device=device).unsqueeze(0)).item()\n",
        "            neg_scored.append((a_vec, s))\n",
        "    neg_scored.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    pairs = []\n",
        "    for (_, a_pos, r_pos) in positives:\n",
        "        w = w_norm(r_pos)\n",
        "        pool = neg_scored[:min(512, len(neg_scored))]\n",
        "        for _ in range(k_neg):\n",
        "            a_neg, _ = random.choice(pool)\n",
        "            pairs.append((a_pos, a_neg, w))\n",
        "            if len(pairs) >= max_pairs: break\n",
        "        if len(pairs) >= max_pairs: break\n",
        "    random.shuffle(pairs)\n",
        "    print(f\"✅ Weighted pairs: {len(pairs)}\")\n",
        "    return pairs\n",
        "\n",
        "def train_hybrid_pairwise_clean(policy, opt, mse, positives, negatives,\n",
        "                                epochs=6, batch_size=128,\n",
        "                                margin=0.2, w_pair=0.7, w_reg=0.3,\n",
        "                                max_pairs=2048, k_neg=3):\n",
        "    pairs = make_weighted_pairs_clean(positives, negatives, policy, k_neg=k_neg, max_pairs=max_pairs)\n",
        "    if not pairs: return\n",
        "\n",
        "    reg_actions = np.asarray([a for (_,a,_) in positives], dtype=np.float32)\n",
        "    reg_targets = np.asarray([np.tanh(r/3.0) for (_,_,r) in positives], dtype=np.float32).reshape(-1,1)\n",
        "\n",
        "    def batched(lst, bsz):\n",
        "        for i in range(0, len(lst), bsz): yield lst[i:i+bsz]\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        random.shuffle(pairs)\n",
        "        perm = np.random.permutation(len(reg_actions))\n",
        "        regA, regY = reg_actions[perm], reg_targets[perm]\n",
        "        pw_iter = batched(pairs, batch_size)\n",
        "        rg_iter = batched(list(zip(regA, regY)), batch_size)\n",
        "\n",
        "        losses = []\n",
        "        while True:\n",
        "            did = False\n",
        "            try:\n",
        "                pwb = next(pw_iter); did = True\n",
        "                a_pos = torch.tensor(np.asarray([p[0] for p in pwb]), dtype=torch.float32, device=device)\n",
        "                a_neg = torch.tensor(np.asarray([p[1] for p in pwb]), dtype=torch.float32, device=device)\n",
        "                s_pos = policy(a_pos); s_neg = policy(a_neg)\n",
        "                loss_pair = torch.relu(margin - s_pos + s_neg).mean()\n",
        "            except StopIteration:\n",
        "                loss_pair = None\n",
        "            try:\n",
        "                rgb = next(rg_iter); did = True\n",
        "                acts = torch.tensor(np.asarray([x[0] for x in rgb]), dtype=torch.float32, device=device)\n",
        "                tgts = torch.tensor(np.asarray([x[1] for x in rgb]), dtype=torch.float32, device=device)\n",
        "                preds = policy(acts)\n",
        "                loss_reg = mse(preds, tgts)\n",
        "            except StopIteration:\n",
        "                loss_reg = None\n",
        "\n",
        "            if not did: break\n",
        "            loss_total = 0.0\n",
        "            if loss_pair is not None: loss_total += w_pair * loss_pair\n",
        "            if loss_reg  is not None: loss_total += w_reg  * loss_reg\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss_total.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n",
        "            opt.step()\n",
        "            losses.append(loss_total.item())\n",
        "        print(f\"Epoch {ep+1} | Hybrid loss: {np.mean(losses):.6f}\")\n",
        "\n",
        "# -------------------\n",
        "# Eval\n",
        "# -------------------\n",
        "def show_clean_stats():\n",
        "    if teacher_pos3:\n",
        "        ps = np.array([r for (_,_,r) in teacher_pos3], dtype=np.float32)\n",
        "        print(f\"Teacher+  n={len(ps)} | min={ps.min():.3f} max={ps.max():.3f} mean={ps.mean():.3f}\")\n",
        "    else:\n",
        "        print(\"Teacher+  n=0\")\n",
        "    if experience_neg3:\n",
        "        ns = np.array([r for (_,_,r) in experience_neg3], dtype=np.float32)\n",
        "        print(f\"Exp−      n={len(ns)} | min={ns.min():.3f} max={ns.max():.3f} mean={ns.mean():.3f}\")\n",
        "    else:\n",
        "        print(\"Exp−      n=0\")\n",
        "\n",
        "def eval_policy_stop_and_greedy(env, global_tree, policy, rounds=10, score_floor=0.25):\n",
        "    soft_scores = [run_episode_stop_aware(env, global_tree, policy, max_steps=10, score_floor=score_floor)\n",
        "                   for _ in range(rounds)]\n",
        "    print(f\"STOP-AWARE mean reward: {np.mean(soft_scores):.4f}\")\n",
        "    print(\"STOP-AWARE scores:\", soft_scores)\n",
        "\n",
        "    def run_greedy_once():\n",
        "        _ = env.reset(use_global_tree=True, global_tree=global_tree)\n",
        "        env.tickets.update({\"B\":20,\"G\":6,\"Y\":2})\n",
        "        total = 0.0\n",
        "        for _ in range(10):\n",
        "            cands = legal_candidates_clean(env.tree, env.tickets)\n",
        "            if not cands: break\n",
        "            best, best_score = None, -1e9\n",
        "            with torch.no_grad():\n",
        "                for a in cands:\n",
        "                    v = encode_action_to_vector(a)\n",
        "                    s = policy(torch.tensor(v, dtype=torch.float32, device=device).unsqueeze(0)).item()\n",
        "                    if s > best_score:\n",
        "                        best_score, best = s, a\n",
        "            _, r, done, _ = env.step(best)\n",
        "            total += float(r)\n",
        "            if done: break\n",
        "        return total\n",
        "\n",
        "    greedy_scores = [run_greedy_once() for _ in range(rounds)]\n",
        "    print(f\"Greedy mean reward: {np.mean(greedy_scores):.4f}\")\n",
        "    print(\"Greedy scores:\", greedy_scores)\n",
        "    return float(np.mean(soft_scores)), float(np.mean(greedy_scores))\n",
        "\n",
        "# -------------------\n",
        "# Checkpointing (survive Colab)\n",
        "# -------------------\n",
        "def _ensure_dir(d):\n",
        "    try: os.makedirs(d, exist_ok=True)\n",
        "    except Exception: pass\n",
        "\n",
        "def save_checkpoint(dirpath, name, policy, opt, meta=None):\n",
        "    _ensure_dir(dirpath)\n",
        "    torch.save({\n",
        "        \"state_dict\": policy.state_dict(),\n",
        "        \"optimizer\": opt.state_dict(),\n",
        "        \"meta\": meta or {}\n",
        "    }, os.path.join(dirpath, f\"{name}.pt\"))\n",
        "    # save buffers too\n",
        "    with open(os.path.join(dirpath, f\"{name}_buffers.pkl\"), \"wb\") as f:\n",
        "        pickle.dump({\"teacher_pos3\": teacher_pos3, \"experience_neg3\": experience_neg3}, f)\n",
        "    print(f\"✅ Saved checkpoint → {name}\")\n",
        "\n",
        "def load_checkpoint(dirpath, name, policy, opt):\n",
        "    path = os.path.join(dirpath, f\"{name}.pt\")\n",
        "    bufp = os.path.join(dirpath, f\"{name}_buffers.pkl\")\n",
        "    if not os.path.exists(path): return False\n",
        "    data = torch.load(path, map_location=device)\n",
        "    policy.load_state_dict(data[\"state_dict\"])\n",
        "    if \"optimizer\" in data:\n",
        "        try: opt.load_state_dict(data[\"optimizer\"])\n",
        "        except Exception: pass\n",
        "    if os.path.exists(bufp):\n",
        "        try:\n",
        "            with open(bufp, \"rb\") as f:\n",
        "                bufs = pickle.load(f)\n",
        "                teacher_pos3.clear(); teacher_pos3.extend(bufs.get(\"teacher_pos3\", []))\n",
        "                experience_neg3.clear(); experience_neg3.extend(bufs.get(\"experience_neg3\", []))\n",
        "        except Exception: pass\n",
        "    print(f\"✅ Loaded checkpoint ← {name}\")\n",
        "    return True\n",
        "\n",
        "# -------------------\n",
        "# Multi-cycle trainer (auto-save each cycle + BEST)\n",
        "# -------------------\n",
        "def train_cycles(env,\n",
        "                 global_tree,\n",
        "                 existing_policy=None,\n",
        "                 cycles=10,\n",
        "                 episodes_per_cycle=6,\n",
        "                 max_steps_per_ep=6,\n",
        "                 beam=12,\n",
        "                 min_gain=1e-6,\n",
        "                 stop_label=1.5,\n",
        "                 margin=0.2,\n",
        "                 w_pair=0.7,\n",
        "                 w_reg=0.3,\n",
        "                 batch_size=128,\n",
        "                 max_pairs=2048,\n",
        "                 score_floor_eval=0.25,\n",
        "                 ckpt_dir=\"/content/phase8_ckpts\",\n",
        "                 ckpt_prefix=\"cycle\"):\n",
        "    policy, opt, mse = init_policy(existing_policy)\n",
        "    migrate_old_buffers()\n",
        "\n",
        "    best_soft = -1e9\n",
        "    for i in range(1, cycles+1):\n",
        "        print(f\"\\n===== TRAINING CYCLE {i} =====\")\n",
        "        # collect\n",
        "        collect_teacher_strong(env, global_tree, policy,\n",
        "                               episodes=episodes_per_cycle,\n",
        "                               max_steps=max_steps_per_ep,\n",
        "                               beam=beam,\n",
        "                               min_gain=min_gain,\n",
        "                               stop_label=stop_label)\n",
        "        collect_experience_negatives_clean(env, global_tree, policy,\n",
        "                                           episodes=episodes_per_cycle,\n",
        "                                           max_steps=max_steps_per_ep)\n",
        "\n",
        "        # stats\n",
        "        show_clean_stats()\n",
        "\n",
        "        # train\n",
        "        train_hybrid_pairwise_clean(policy, opt, mse,\n",
        "                                    teacher_pos3, experience_neg3,\n",
        "                                    epochs=6, batch_size=batch_size,\n",
        "                                    margin=margin, w_pair=w_pair, w_reg=w_reg,\n",
        "                                    max_pairs=max_pairs, k_neg=3)\n",
        "\n",
        "        # eval & save\n",
        "        soft_mean, _ = eval_policy_stop_and_greedy(env, global_tree, policy, rounds=10, score_floor=score_floor_eval)\n",
        "        meta = {\"cycle\": i, \"soft_mean\": soft_mean}\n",
        "\n",
        "        save_checkpoint(ckpt_dir, f\"{ckpt_prefix}_{i}\", policy, opt, meta)\n",
        "        if soft_mean > best_soft:\n",
        "            best_soft = soft_mean\n",
        "            save_checkpoint(ckpt_dir, \"BEST\", policy, opt, {\"cycle\": i, \"soft_mean\": soft_mean})\n",
        "\n",
        "    return policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQ08X8V6ykmM",
        "outputId": "d8e800d6-a485-45f2-840c-8476ec6b0683"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-08 09:49:51,076 - INFO - Initialized TLiteExpert dim=50 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=50 on cpu\n",
            "2025-11-08 09:49:51,085 - INFO - Initialized TLiteExpert dim=50 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=50 on cpu\n",
            "2025-11-08 09:49:51,090 - INFO - Initialized TLiteExpert dim=50 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=50 on cpu\n",
            "2025-11-08 09:49:51,102 - INFO - Initialized TLiteExpert dim=50 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=50 on cpu\n",
            "2025-11-08 09:49:51,107 - INFO - Initialized TLiteExpert dim=50 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=50 on cpu\n",
            "2025-11-08 09:49:51,110 - INFO - Initialized TLiteExpert dim=50 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=50 on cpu\n",
            "2025-11-08 09:49:51,118 - INFO - Initialized TLiteExpert dim=50 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=50 on cpu\n",
            "2025-11-08 09:49:51,124 - INFO - Initialized TLiteExpert dim=50 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=50 on cpu\n",
            "2025-11-08 09:49:51,131 - INFO - Initialized TLiteRouter num_experts=8, top_k=2 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteRouter num_experts=8, top_k=2 on cpu\n",
            "2025-11-08 09:49:51,134 - INFO - Initialized TLiteV6 num_experts=8 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteV6 num_experts=8 on cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "from phase8_clean import *\n",
        "policy, opt, mse = init_policy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLNgZ1h36V22"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, pickle, torch, numpy as np\n",
        "\n",
        "SAVE_PATH = \"/content/drive/MyDrive/chaturya_saves\"\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "\n",
        "best_score = -9999  # Higher STOP-aware score = better model\n",
        "\n",
        "def save_all(tag):\n",
        "    \"\"\"Save policy + teacher_pos3 + experience_neg3 + prakriti_tree.\"\"\"\n",
        "    global policy, teacher_pos3, experience_neg3, prakriti_tree\n",
        "\n",
        "    torch.save(policy.state_dict(), f\"{SAVE_PATH}/policy_{tag}.pt\")\n",
        "\n",
        "    with open(f\"{SAVE_PATH}/teacher_pos3_{tag}.pkl\", \"wb\") as f:\n",
        "        pickle.dump(teacher_pos3, f)\n",
        "\n",
        "    with open(f\"{SAVE_PATH}/experience_neg3_{tag}.pkl\", \"wb\") as f:\n",
        "        pickle.dump(experience_neg3, f)\n",
        "\n",
        "    with open(f\"{SAVE_PATH}/prakriti_tree_{tag}.pkl\", \"wb\") as f:\n",
        "        pickle.dump(prakriti_tree, f)\n",
        "\n",
        "    print(f\"✅ Saved checkpoint → {tag}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIuGUF12IH2Q",
        "outputId": "c3fdfb16-e3d3-434c-aa03-c39c3bbd3e68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== TRAINING CYCLE 1 =====\n",
            "🧑‍🏫 Strong teacher positives: +90 (total 137)\n",
            "📦 Experience negatives: +29 (total 59)\n",
            "Teacher+  n=137 | min=0.400 max=95.925 mean=3.028\n",
            "Exp−      n=59 | min=-0.712 max=-0.069 mean=-0.525\n",
            "✅ Weighted pairs: 411\n",
            "Epoch 1 | Hybrid loss: 0.092942\n",
            "Epoch 2 | Hybrid loss: 0.070395\n",
            "Epoch 3 | Hybrid loss: 0.052965\n",
            "Epoch 4 | Hybrid loss: 0.056668\n",
            "Epoch 5 | Hybrid loss: 0.052509\n",
            "Epoch 6 | Hybrid loss: 0.049455\n",
            "STOP-AWARE mean reward = -4.6559\n",
            "✅ Saved checkpoint → cycle_1\n",
            "✅ Saved checkpoint → BEST\n",
            "\n",
            "===== TRAINING CYCLE 2 =====\n",
            "🧑‍🏫 Strong teacher positives: +68 (total 205)\n",
            "📦 Experience negatives: +30 (total 89)\n",
            "Teacher+  n=205 | min=0.306 max=95.925 mean=2.443\n",
            "Exp−      n=89 | min=-0.712 max=-0.047 mean=-0.476\n",
            "✅ Weighted pairs: 615\n",
            "Epoch 1 | Hybrid loss: 0.055487\n",
            "Epoch 2 | Hybrid loss: 0.055025\n",
            "Epoch 3 | Hybrid loss: 0.053375\n",
            "Epoch 4 | Hybrid loss: 0.052182\n",
            "Epoch 5 | Hybrid loss: 0.050626\n",
            "Epoch 6 | Hybrid loss: 0.050775\n",
            "STOP-AWARE mean reward = -5.1481\n",
            "✅ Saved checkpoint → cycle_2\n",
            "\n",
            "===== TRAINING CYCLE 3 =====\n",
            "🧑‍🏫 Strong teacher positives: +135 (total 340)\n",
            "📦 Experience negatives: +29 (total 118)\n",
            "Teacher+  n=340 | min=0.306 max=95.925 mean=2.137\n",
            "Exp−      n=118 | min=-0.712 max=-0.047 mean=-0.455\n",
            "✅ Weighted pairs: 1020\n",
            "Epoch 1 | Hybrid loss: 0.042284\n",
            "Epoch 2 | Hybrid loss: 0.040624\n",
            "Epoch 3 | Hybrid loss: 0.039515\n",
            "Epoch 4 | Hybrid loss: 0.038603\n",
            "Epoch 5 | Hybrid loss: 0.037569\n",
            "Epoch 6 | Hybrid loss: 0.037570\n",
            "STOP-AWARE mean reward = -4.3764\n",
            "✅ Saved checkpoint → cycle_3\n",
            "✅ Saved checkpoint → BEST\n",
            "\n",
            "===== TRAINING CYCLE 4 =====\n",
            "🧑‍🏫 Strong teacher positives: +73 (total 413)\n",
            "📦 Experience negatives: +30 (total 148)\n",
            "Teacher+  n=413 | min=0.306 max=95.925 mean=2.056\n",
            "Exp−      n=148 | min=-0.712 max=-0.028 mean=-0.440\n",
            "✅ Weighted pairs: 1239\n",
            "Epoch 1 | Hybrid loss: 0.038129\n",
            "Epoch 2 | Hybrid loss: 0.037232\n",
            "Epoch 3 | Hybrid loss: 0.036362\n",
            "Epoch 4 | Hybrid loss: 0.035810\n",
            "Epoch 5 | Hybrid loss: 0.035921\n",
            "Epoch 6 | Hybrid loss: 0.035264\n",
            "STOP-AWARE mean reward = -4.6816\n",
            "✅ Saved checkpoint → cycle_4\n",
            "\n",
            "===== TRAINING CYCLE 5 =====\n",
            "🧑‍🏫 Strong teacher positives: +70 (total 483)\n",
            "📦 Experience negatives: +30 (total 178)\n",
            "Teacher+  n=483 | min=0.306 max=95.925 mean=1.995\n",
            "Exp−      n=178 | min=-0.712 max=-0.028 mean=-0.451\n",
            "✅ Weighted pairs: 1449\n",
            "Epoch 1 | Hybrid loss: 0.027466\n",
            "Epoch 2 | Hybrid loss: 0.026865\n",
            "Epoch 3 | Hybrid loss: 0.025664\n",
            "Epoch 4 | Hybrid loss: 0.025494\n",
            "Epoch 5 | Hybrid loss: 0.025553\n",
            "Epoch 6 | Hybrid loss: 0.025366\n",
            "STOP-AWARE mean reward = -4.9228\n",
            "✅ Saved checkpoint → cycle_5\n",
            "\n",
            "===== TRAINING CYCLE 6 =====\n",
            "🧑‍🏫 Strong teacher positives: +123 (total 606)\n",
            "📦 Experience negatives: +29 (total 207)\n",
            "Teacher+  n=606 | min=0.306 max=95.925 mean=1.862\n",
            "Exp−      n=207 | min=-0.712 max=-0.028 mean=-0.447\n",
            "✅ Weighted pairs: 1818\n",
            "Epoch 1 | Hybrid loss: 0.027932\n",
            "Epoch 2 | Hybrid loss: 0.026314\n",
            "Epoch 3 | Hybrid loss: 0.026039\n",
            "Epoch 4 | Hybrid loss: 0.025366\n",
            "Epoch 5 | Hybrid loss: 0.025132\n",
            "Epoch 6 | Hybrid loss: 0.024831\n",
            "STOP-AWARE mean reward = -4.7028\n",
            "✅ Saved checkpoint → cycle_6\n",
            "\n",
            "===== TRAINING CYCLE 7 =====\n",
            "🧑‍🏫 Strong teacher positives: +78 (total 684)\n",
            "📦 Experience negatives: +30 (total 237)\n",
            "Teacher+  n=684 | min=0.306 max=95.925 mean=1.799\n",
            "Exp−      n=237 | min=-0.712 max=-0.028 mean=-0.448\n",
            "✅ Weighted pairs: 2048\n",
            "Epoch 1 | Hybrid loss: 0.028084\n",
            "Epoch 2 | Hybrid loss: 0.027821\n",
            "Epoch 3 | Hybrid loss: 0.027704\n",
            "Epoch 4 | Hybrid loss: 0.027498\n",
            "Epoch 5 | Hybrid loss: 0.027254\n",
            "Epoch 6 | Hybrid loss: 0.027179\n",
            "STOP-AWARE mean reward = -4.5694\n",
            "✅ Saved checkpoint → cycle_7\n",
            "\n",
            "===== TRAINING CYCLE 8 =====\n",
            "🧑‍🏫 Strong teacher positives: +104 (total 788)\n",
            "📦 Experience negatives: +30 (total 267)\n",
            "Teacher+  n=788 | min=0.306 max=95.925 mean=1.789\n",
            "Exp−      n=267 | min=-0.712 max=-0.028 mean=-0.446\n",
            "✅ Weighted pairs: 2048\n",
            "Epoch 1 | Hybrid loss: 0.027052\n",
            "Epoch 2 | Hybrid loss: 0.027376\n",
            "Epoch 3 | Hybrid loss: 0.027353\n",
            "Epoch 4 | Hybrid loss: 0.027274\n",
            "Epoch 5 | Hybrid loss: 0.026826\n",
            "Epoch 6 | Hybrid loss: 0.026673\n",
            "STOP-AWARE mean reward = -4.8121\n",
            "✅ Saved checkpoint → cycle_8\n",
            "\n",
            "===== TRAINING CYCLE 9 =====\n",
            "🧑‍🏫 Strong teacher positives: +104 (total 892)\n",
            "📦 Experience negatives: +30 (total 297)\n",
            "Teacher+  n=892 | min=0.306 max=95.925 mean=1.766\n",
            "Exp−      n=297 | min=-0.712 max=-0.028 mean=-0.442\n",
            "✅ Weighted pairs: 2048\n",
            "Epoch 1 | Hybrid loss: 0.026825\n",
            "Epoch 2 | Hybrid loss: 0.026731\n",
            "Epoch 3 | Hybrid loss: 0.026694\n",
            "Epoch 4 | Hybrid loss: 0.026656\n",
            "Epoch 5 | Hybrid loss: 0.026632\n",
            "Epoch 6 | Hybrid loss: 0.026675\n",
            "STOP-AWARE mean reward = -5.0293\n",
            "✅ Saved checkpoint → cycle_9\n",
            "\n",
            "===== TRAINING CYCLE 10 =====\n",
            "🧑‍🏫 Strong teacher positives: +102 (total 994)\n",
            "📦 Experience negatives: +30 (total 327)\n",
            "Teacher+  n=994 | min=0.306 max=95.925 mean=1.747\n",
            "Exp−      n=327 | min=-0.712 max=-0.028 mean=-0.438\n",
            "✅ Weighted pairs: 2048\n",
            "Epoch 1 | Hybrid loss: 0.027037\n",
            "Epoch 2 | Hybrid loss: 0.027011\n",
            "Epoch 3 | Hybrid loss: 0.026941\n",
            "Epoch 4 | Hybrid loss: 0.026969\n",
            "Epoch 5 | Hybrid loss: 0.026950\n",
            "Epoch 6 | Hybrid loss: 0.026910\n",
            "STOP-AWARE mean reward = -5.1698\n",
            "✅ Saved checkpoint → cycle_10\n",
            "\n",
            "===== TRAINING CYCLE 11 =====\n",
            "🧑‍🏫 Strong teacher positives: +123 (total 1117)\n",
            "📦 Experience negatives: +30 (total 357)\n",
            "Teacher+  n=1117 | min=0.306 max=95.925 mean=1.782\n",
            "Exp−      n=357 | min=-0.712 max=-0.011 mean=-0.431\n",
            "✅ Weighted pairs: 2048\n",
            "Epoch 1 | Hybrid loss: 0.029978\n",
            "Epoch 2 | Hybrid loss: 0.029984\n",
            "Epoch 3 | Hybrid loss: 0.029945\n",
            "Epoch 4 | Hybrid loss: 0.029963\n",
            "Epoch 5 | Hybrid loss: 0.030018\n",
            "Epoch 6 | Hybrid loss: 0.029897\n",
            "STOP-AWARE mean reward = -5.2529\n",
            "✅ Saved checkpoint → cycle_11\n",
            "\n",
            "===== TRAINING CYCLE 12 =====\n",
            "🧑‍🏫 Strong teacher positives: +97 (total 1214)\n",
            "📦 Experience negatives: +30 (total 387)\n",
            "Teacher+  n=1214 | min=0.306 max=95.925 mean=1.794\n",
            "Exp−      n=387 | min=-0.712 max=-0.011 mean=-0.431\n",
            "✅ Weighted pairs: 2048\n",
            "Epoch 1 | Hybrid loss: 0.028905\n",
            "Epoch 2 | Hybrid loss: 0.028960\n",
            "Epoch 3 | Hybrid loss: 0.028995\n",
            "Epoch 4 | Hybrid loss: 0.028965\n",
            "Epoch 5 | Hybrid loss: 0.028965\n",
            "Epoch 6 | Hybrid loss: 0.028922\n",
            "STOP-AWARE mean reward = -5.4399\n",
            "✅ Saved checkpoint → cycle_12\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ---- PHASE 9 TRAINING LOOP (Corrected) ----\n",
        "\n",
        "best_score = -9999\n",
        "\n",
        "for cycle in range(12):   # or any number of cycles\n",
        "    print(f\"\\n===== TRAINING CYCLE {cycle+1} =====\")\n",
        "\n",
        "    # (1) Collect training data\n",
        "    collect_teacher_strong(env, prakriti_tree, policy, episodes=5, max_steps=6)\n",
        "    collect_experience_negatives_clean(env, prakriti_tree, policy, episodes=5, max_steps=6)\n",
        "    show_clean_stats()\n",
        "\n",
        "    # (2) Train policy\n",
        "    train_hybrid_pairwise_clean(\n",
        "        policy, opt, mse,\n",
        "        teacher_pos3, experience_neg3,\n",
        "        epochs=6, batch_size=128,\n",
        "        margin=0.2, w_pair=0.7, w_reg=0.3,\n",
        "        max_pairs=2048, k_neg=3\n",
        "    )\n",
        "\n",
        "    # (3) Evaluate performance\n",
        "    stop_scores = [run_episode_stop_aware(env, prakriti_tree, policy, max_steps=10, score_floor=0.25)\n",
        "                   for _ in range(10)]\n",
        "    mean_stop = np.mean(stop_scores)\n",
        "    print(f\"STOP-AWARE mean reward = {mean_stop:.4f}\")\n",
        "\n",
        "    # (4) Save model\n",
        "    save_all(f\"cycle_{cycle+1}\")\n",
        "\n",
        "    if mean_stop > best_score:\n",
        "        best_score = mean_stop\n",
        "        save_all(\"BEST\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV2Ccq66Hl_-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IkebNOxKEsQ"
      },
      "source": [
        "# Phase 9: Stop aware"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BY7YcGfAKJi7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Phase 9 — Cell 1: Prakriti profile + STOP scoring ===\n",
        "\n",
        "# Prakriti (classification-tree) targets\n",
        "PRAKRITI_CFG = {\n",
        "    \"bf_target\": 3.0,          # branching factor target\n",
        "    \"bf_tol\": 0.7,             # acceptable band: [2.3 .. 3.7]\n",
        "    \"entropy_good\": 0.65,      # \"ordered enough\"\n",
        "    \"weak_ratio_good\": 0.12,   # weak_leaves <= 12% of nodes\n",
        "    \"depth_cap\": 7             # avoid overly deep trees\n",
        "}\n",
        "\n",
        "def _ratio(x, y):\n",
        "    y = max(1, y)\n",
        "    return float(x)/float(y)\n",
        "\n",
        "def stop_score(snapshot, cfg=PRAKRITI_CFG):\n",
        "    \"\"\"\n",
        "    Returns a score in ~[0,1.5] (higher = safer to STOP).\n",
        "    Combines: low entropy, near-target branching factor, few weak leaves, not-too-deep.\n",
        "    \"\"\"\n",
        "    bf = snapshot.get(\"branching_factor\", 0.0)\n",
        "    ent = snapshot.get(\"entropy\", 1.0)\n",
        "    nodes = snapshot.get(\"node_count\", 1)\n",
        "    weak = snapshot.get(\"weak_leaves\", 0)\n",
        "    depth = snapshot.get(\"depth\", 1)\n",
        "\n",
        "    # (1) entropy contribution\n",
        "    s_ent = max(0.0, (cfg[\"entropy_good\"] - ent) / max(1e-6, cfg[\"entropy_good\"]))  # ≤0 when ent>good, up to 1.0 when ent≈0\n",
        "\n",
        "    # (2) branching closeness to 3.0\n",
        "    bf_err = abs(cfg[\"bf_target\"] - bf)\n",
        "    s_bf = max(0.0, 1.0 - bf_err / cfg[\"bf_tol\"])  # within band → near 1\n",
        "\n",
        "    # (3) weak leaves small ratio\n",
        "    wr = _ratio(weak, nodes)\n",
        "    s_weak = max(0.0, 1.0 - (wr / cfg[\"weak_ratio_good\"]))  # if weak ratio ≤ target → near 1\n",
        "\n",
        "    # (4) depth not too big (soft)\n",
        "    s_depth = 1.0 if depth <= cfg[\"depth_cap\"] else max(0.0, 1.0 - 0.15*(depth - cfg[\"depth_cap\"]))\n",
        "\n",
        "    # weights tuned for Prakriti (structure quality > depth)\n",
        "    score = 0.38*s_ent + 0.38*s_bf + 0.18*s_weak + 0.06*s_depth\n",
        "    return float(score)\n",
        "\n",
        "def should_stop(snapshot, tau=0.62):\n",
        "    \"\"\"Return True if STOP score exceeds threshold.\"\"\"\n",
        "    return stop_score(snapshot) >= float(tau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9GvxbulKLmG"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Phase 9 — Cell 2: STOP-aware selector (no changes to action encoder) ===\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def select_action_or_stop(snapshot, env, policy, device,\n",
        "                          tau=0.62,               # STOP threshold (Prakriti tuned)\n",
        "                          epsilon=0.15,           # exploration\n",
        "                          temperature=1.2,        # softmax over policy\n",
        "                          beam=None):             # optional top-K pruning\n",
        "    # 1) STOP gate first\n",
        "    if should_stop(snapshot, tau=tau):\n",
        "        return (\"STOP\", None, None)\n",
        "\n",
        "    # 2) Otherwise choose a structural action (your existing legal_candidates)\n",
        "    cands = legal_candidates(env.tree, env.tickets)\n",
        "    if not cands:  # nothing to do → STOP\n",
        "        return (\"STOP\", None, None)\n",
        "\n",
        "    # ε-greedy explore\n",
        "    if np.random.rand() < epsilon:\n",
        "        return random.choice(cands)\n",
        "\n",
        "    # score with policy\n",
        "    with torch.no_grad():\n",
        "        scored = []\n",
        "        for a in cands:\n",
        "            avec = encode_action_to_vector(a)\n",
        "            at = torch.tensor(avec, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "            s = policy(at).item()\n",
        "            scored.append((a, s))\n",
        "\n",
        "    # optional beam to stabilize\n",
        "    if beam is not None and beam > 0:\n",
        "        scored.sort(key=lambda x: x[1], reverse=True)\n",
        "        scored = scored[:min(beam, len(scored))]\n",
        "\n",
        "    # softmax sampling\n",
        "    vals = np.array([s for _, s in scored], dtype=np.float64) / float(temperature)\n",
        "    vals -= vals.max()\n",
        "    p = np.exp(vals); p /= p.sum() if p.sum() > 0 else 1.0\n",
        "    idx = np.random.choice(len(scored), p=p)\n",
        "    return scored[idx][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yi946VBKLo7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Phase 9 — Cell 3: STOP-aware episode + reward shaping ===\n",
        "\n",
        "def stop_bonus(snapshot, cfg=PRAKRITI_CFG):\n",
        "    \"\"\"\n",
        "    Shaping reward when STOP is chosen:\n",
        "      +2.0 if structure is strong,\n",
        "      +1.0 if decent,\n",
        "      -2.0 if premature.\n",
        "    \"\"\"\n",
        "    sc = stop_score(snapshot, cfg)\n",
        "    if sc >= 0.85:  # excellent\n",
        "        return +2.0\n",
        "    if sc >= 0.62:  # good enough (τ)\n",
        "        return +1.0\n",
        "    return -2.0     # shouldn't have stopped\n",
        "\n",
        "def run_stopaware_episode(env, policy, device,\n",
        "                          max_steps=12,\n",
        "                          tau=0.62, epsilon=0.15, temperature=1.2, beam=12):\n",
        "    \"\"\"\n",
        "    Uses select_action_or_stop(). If STOP returned, apply STOP bonus and end episode.\n",
        "    \"\"\"\n",
        "    _ = env.reset(use_global_tree=True, global_tree=prakriti_tree)\n",
        "    seed_env_tickets(env, B=20, G=6, Y=2)\n",
        "\n",
        "    total_reward = 0.0\n",
        "    for _ in range(max_steps):\n",
        "        action = select_action_or_stop(env.snapshot, env, policy, device,\n",
        "                                       tau=tau, epsilon=epsilon,\n",
        "                                       temperature=temperature, beam=beam)\n",
        "        if action and action[0] == \"STOP\":\n",
        "            total_reward += stop_bonus(env.snapshot)\n",
        "            break\n",
        "\n",
        "        if action is None:\n",
        "            # no legal actions → treat as STOP-attempt\n",
        "            total_reward += stop_bonus(env.snapshot)\n",
        "            break\n",
        "\n",
        "        _, r, done, _ = env.step(action)\n",
        "        total_reward += float(r)\n",
        "        if done:\n",
        "            # if environment ends early, apply *final* STOP check once\n",
        "            total_reward += max(0.0, stop_bonus(env.snapshot))\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dumhOnMbKLtM"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Phase 9 — Cell 4: Final evaluation (tables-ready) ===\n",
        "import numpy as np\n",
        "\n",
        "def eval_random_baseline(env, rounds=10, max_steps=12):\n",
        "    def _random_episode():\n",
        "        _ = env.reset(use_global_tree=True, global_tree=prakriti_tree)\n",
        "        seed_env_tickets(env, B=20, G=6, Y=2)\n",
        "        tot=0.0\n",
        "        for _ in range(max_steps):\n",
        "            c = legal_candidates(env.tree, env.tickets)\n",
        "            if not c:\n",
        "                break\n",
        "            a = random.choice(c)\n",
        "            _, r, done, _ = env.step(a)\n",
        "            tot += float(r)\n",
        "            if done: break\n",
        "        return tot\n",
        "    scores = [ _random_episode() for _ in range(rounds) ]\n",
        "    return float(np.mean(scores)), scores\n",
        "\n",
        "def eval_soft_policy(env, policy, rounds=10, max_steps=12):\n",
        "    scores = [ run_policy_episode(env, max_steps=max_steps, greedy=False) for _ in range(rounds) ]\n",
        "    return float(np.mean(scores)), scores\n",
        "\n",
        "def eval_greedy_policy(env, policy, rounds=10, max_steps=12):\n",
        "    scores = [ run_policy_episode(env, max_steps=max_steps, greedy=True) for _ in range(rounds) ]\n",
        "    return float(np.mean(scores)), scores\n",
        "\n",
        "def eval_stopaware(env, policy, rounds=10, max_steps=12, tau=0.62):\n",
        "    scores = [ run_stopaware_episode(env, policy, device, max_steps=max_steps, tau=tau) for _ in range(rounds) ]\n",
        "    return float(np.mean(scores)), scores\n",
        "\n",
        "# ---- Run all & print table ----\n",
        "rnd_mean, rnd_scores   = eval_random_baseline(env, rounds=10, max_steps=12)\n",
        "soft_mean, soft_scores = eval_soft_policy(env, policy, rounds=10, max_steps=12)\n",
        "grdy_mean, grdy_scores = eval_greedy_policy(env, policy, rounds=10, max_steps=12)\n",
        "stp_mean,  stp_scores  = eval_stopaware(env, policy, rounds=10, max_steps=12, tau=0.58)\n",
        "\n",
        "print(\"\\n=== Phase 9 — Final Benchmarks (Prakriti) ===\")\n",
        "print(f\"Random baseline      : {rnd_mean:.3f}  | {rnd_scores}\")\n",
        "print(f\"Policy (soft)        : {soft_mean:.3f}  | {soft_scores}\")\n",
        "print(f\"Policy (greedy)      : {grdy_mean:.3f}  | {grdy_scores}\")\n",
        "print(f\"STOP-aware (ours)    : {stp_mean:.3f}  | {stp_scores}\")\n",
        "\n",
        "# Optional: quick ablation of τ\n",
        "for tau in [0.55, 0.62, 0.70]:\n",
        "    m,_ = eval_stopaware(env, policy, rounds=10, max_steps=12, tau=tau)\n",
        "    print(f\"STOP-aware tau={tau:.2f} → mean {m:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkXE4slRkeHv"
      },
      "source": [
        "# Phase 10: Evaluation Harness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04FEaZF5BL3I",
        "outputId": "0ba10483-8633-419a-9ba3-5f3eb49d6921"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting benchmarks.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile benchmarks.py\n",
        "import json, time, numpy as np\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "def run_stop_aware(env, policy, global_tree, rounds=10, score_floor=0.25):\n",
        "    from phase8_clean import run_episode_stop_aware\n",
        "    scores = []\n",
        "    for _ in range(rounds):\n",
        "        s = run_episode_stop_aware(env, global_tree, policy, max_steps=10, score_floor=score_floor)\n",
        "        scores.append(float(s))\n",
        "    return scores\n",
        "\n",
        "def run_greedy(env, policy, global_tree, rounds=10):\n",
        "    from phase8_clean import legal_candidates_clean\n",
        "    from tlite_rl_bridge import encode_action_to_vector\n",
        "\n",
        "    scores = []\n",
        "    for _ in range(rounds):\n",
        "        _ = env.reset(use_global_tree=True, global_tree=global_tree)\n",
        "        env.tickets.update({\"B\":20,\"G\":6,\"Y\":2,\"R\":0,\"P\":0})\n",
        "        total = 0.0\n",
        "\n",
        "        for _ in range(10):\n",
        "            cands = legal_candidates_clean(env.tree, env.tickets)\n",
        "            if not cands:\n",
        "                break\n",
        "\n",
        "            best = None\n",
        "            best_score = -1e9\n",
        "            with torch.no_grad():\n",
        "                for a in cands:\n",
        "                    v = encode_action_to_vector(a)\n",
        "                    s = policy(torch.tensor(v, dtype=torch.float32).unsqueeze(0)).item()\n",
        "                    if s > best_score:\n",
        "                        best_score = s\n",
        "                        best = a\n",
        "\n",
        "            _, r, done, _ = env.step(best)\n",
        "            total += float(r)\n",
        "            if done: break\n",
        "\n",
        "        scores.append(total)\n",
        "    return scores\n",
        "\n",
        "def summarize(scores):\n",
        "    arr = np.array(scores, dtype=np.float32)\n",
        "    return {\n",
        "        \"mean\": float(arr.mean()),\n",
        "        \"std\": float(arr.std()),\n",
        "        \"min\": float(arr.min()),\n",
        "        \"max\": float(arr.max()),\n",
        "        \"n\": int(len(arr)),\n",
        "    }\n",
        "\n",
        "def run_benchmark(env, policy, global_tree, name=\"phase9_eval\", rounds=10, score_floor=0.25):\n",
        "    t0 = time.time()\n",
        "\n",
        "    stop_scores   = run_stop_aware(env, policy, global_tree, rounds, score_floor)\n",
        "    greedy_scores = run_greedy(env, policy, global_tree, rounds)\n",
        "\n",
        "    result = {\n",
        "        \"name\": name,\n",
        "        \"rounds\": rounds,\n",
        "        \"score_floor\": score_floor,\n",
        "        \"stop_aware\": summarize(stop_scores),\n",
        "        \"greedy\": summarize(greedy_scores),\n",
        "        \"stop_raw\": stop_scores,\n",
        "        \"greedy_raw\": greedy_scores,\n",
        "        \"time_sec\": round(time.time() - t0, 3),\n",
        "    }\n",
        "\n",
        "    Path(\"benchmarks\").mkdir(exist_ok=True)\n",
        "    out = Path(f\"benchmarks/{name}.json\")\n",
        "    out.write_text(json.dumps(result, indent=2))\n",
        "\n",
        "    print(\"\\n=== BENCHMARK RESULT ===\")\n",
        "    print(json.dumps(result, indent=2))\n",
        "    print(f\"Saved → {out}\")\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0DYvQ0DBe67",
        "outputId": "00e292ab-93a4-4a02-b98d-1813c4a7239b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<module 'phase8_clean' from '/content/phase8_clean.py'>"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import importlib, phase8_clean\n",
        "importlib.reload(phase8_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd-wogfVBN6f",
        "outputId": "a45331e4-2b71-4131-9883-76f8e8d10cca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<module 'benchmarks' from '/content/benchmarks.py'>"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import importlib, benchmarks\n",
        "importlib.reload(benchmarks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1IRQnGFBXRk",
        "outputId": "c334f0d2-bc2e-4aca-dcd1-6d8309c51606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== BENCHMARK RESULT ===\n",
            "{\n",
            "  \"name\": \"phase9_test\",\n",
            "  \"rounds\": 10,\n",
            "  \"score_floor\": 0.25,\n",
            "  \"stop_aware\": {\n",
            "    \"mean\": -4.266352653503418,\n",
            "    \"std\": 0.9958307147026062,\n",
            "    \"min\": -5.303299903869629,\n",
            "    \"max\": -1.804226040840149,\n",
            "    \"n\": 10\n",
            "  },\n",
            "  \"greedy\": {\n",
            "    \"mean\": -1.4933630228042603,\n",
            "    \"std\": 0.41320204734802246,\n",
            "    \"min\": -2.5925400257110596,\n",
            "    \"max\": -0.8016899824142456,\n",
            "    \"n\": 10\n",
            "  },\n",
            "  \"stop_raw\": [\n",
            "    -1.8042259999999999,\n",
            "    -4.412287,\n",
            "    -4.978529,\n",
            "    -5.3033,\n",
            "    -3.27971,\n",
            "    -4.878804000000001,\n",
            "    -4.078247,\n",
            "    -4.217779,\n",
            "    -5.147636,\n",
            "    -4.563009\n",
            "  ],\n",
            "  \"greedy_raw\": [\n",
            "    -0.80169,\n",
            "    -2.5925399999999996,\n",
            "    -1.4424249999999994,\n",
            "    -1.4424249999999994,\n",
            "    -1.4424249999999994,\n",
            "    -1.4424249999999994,\n",
            "    -1.4424249999999994,\n",
            "    -1.4424249999999994,\n",
            "    -1.4424249999999994,\n",
            "    -1.4424249999999994\n",
            "  ],\n",
            "  \"time_sec\": 27.931\n",
            "}\n",
            "Saved → benchmarks/phase9_test.json\n"
          ]
        }
      ],
      "source": [
        "result = benchmarks.run_benchmark(env, policy, prakriti_tree, name=\"phase9_test\", rounds=10, score_floor=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrDJkFrkB2pk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nEs72LBB5MN"
      },
      "source": [
        "Phase 9.2: STOP Preference Stabilization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxAfpbuuDEgs"
      },
      "outputs": [],
      "source": [
        "\n",
        "import importlib\n",
        "import phase8_clean\n",
        "importlib.reload(phase8_clean)\n",
        "\n",
        "from phase8_clean import (\n",
        "    collect_stop_preference,\n",
        "    train_hybrid_pairwise_clean,\n",
        "    teacher_pos3,\n",
        "    experience_neg3,\n",
        "    show_clean_stats,\n",
        "    run_episode_stop_aware\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOE7RZF3Df97",
        "outputId": "e1b53943-ef10-4fee-c84c-4510f1e57abc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📦 Experience negatives: +35 (total 35)\n",
            "Teacher+  n=0\n",
            "Exp−      n=35 | min=-0.705 max=-0.203 mean=-0.597\n"
          ]
        }
      ],
      "source": [
        "\n",
        "collect_experience_negatives_clean(env, prakriti_tree, policy, episodes=6, max_steps=6)\n",
        "show_clean_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60q4SoLhCK0w",
        "outputId": "3c6001ab-caf1-4532-e472-58c3d4e94dae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟢 STOP-Preference labels: +8 (total 72)\n",
            "Teacher+  n=72 | min=1.500 max=2.000 mean=1.750\n",
            "Exp−      n=31 | min=-0.542 max=-0.047 mean=-0.358\n",
            "✅ Weighted pairs: 216\n",
            "Epoch 1 | Hybrid loss: 0.096932\n",
            "Epoch 2 | Hybrid loss: 0.094097\n",
            "Epoch 3 | Hybrid loss: 0.093561\n",
            "Epoch 4 | Hybrid loss: 0.094047\n",
            "STOP-AWARE mean reward: -3.8799\n",
            "STOP-AWARE scores: [-2.254638, -2.556985, -2.470725, -4.067169000000001, -4.340739, -4.277401, -3.9980580000000008, -4.575845, -5.237764, -5.019211]\n",
            "Greedy mean reward: -1.5384\n",
            "Greedy scores: [-1.8323489999999998, -2.01194, -1.4424249999999994, -1.4424249999999994, -1.4424249999999994, -1.4424249999999994, -1.4424249999999994, -1.4424249999999994, -1.4424249999999994, -1.4424249999999994]\n",
            "🟢 STOP-Preference labels: +8 (total 80)\n",
            "Teacher+  n=80 | min=1.500 max=2.000 mean=1.750\n",
            "Exp−      n=31 | min=-0.542 max=-0.047 mean=-0.358\n",
            "✅ Weighted pairs: 240\n",
            "Epoch 1 | Hybrid loss: 0.091628\n",
            "Epoch 2 | Hybrid loss: 0.083996\n",
            "Epoch 3 | Hybrid loss: 0.083426\n",
            "Epoch 4 | Hybrid loss: 0.081774\n",
            "STOP-AWARE mean reward: -4.4453\n",
            "STOP-AWARE scores: [-1.1615519999999997, -3.364932, -4.1875979999999995, -5.351362000000001, -5.573124999999999, -5.671848999999999, -4.969672, -5.525737, -4.634917, -4.0124319999999996]\n",
            "Greedy mean reward: -1.4132\n",
            "Greedy scores: [-1.1500920000000003, -1.4424249999999994, -1.4424249999999994, -1.4424249999999994, -1.4424249999999994, -1.4424249999999994, -1.4424249999999994, -1.4424249999999994, -1.4424249999999994, -1.4424249999999994]\n",
            "🟢 STOP-Preference labels: +8 (total 88)\n",
            "Teacher+  n=88 | min=1.500 max=2.000 mean=1.750\n",
            "Exp−      n=31 | min=-0.542 max=-0.047 mean=-0.358\n",
            "✅ Weighted pairs: 264\n",
            "Epoch 1 | Hybrid loss: 0.085511\n",
            "Epoch 2 | Hybrid loss: 0.081410\n",
            "Epoch 3 | Hybrid loss: 0.067056\n",
            "Epoch 4 | Hybrid loss: 0.078002\n",
            "STOP-AWARE mean reward: -4.9684\n",
            "STOP-AWARE scores: [-2.180838, -4.6317770000000005, -4.996435, -4.558199, -5.496525000000001, -4.941832000000001, -5.398538999999999, -5.6367460000000005, -5.511995, -6.331607000000001]\n",
            "Greedy mean reward: -6.5309\n",
            "Greedy scores: [-5.7289449999999995, -6.62, -6.62, -6.62, -6.62, -6.62, -6.62, -6.62, -6.62, -6.62]\n",
            "🟢 STOP-Preference labels: +8 (total 96)\n",
            "Teacher+  n=96 | min=1.500 max=2.000 mean=1.771\n",
            "Exp−      n=31 | min=-0.542 max=-0.047 mean=-0.358\n",
            "✅ Weighted pairs: 288\n",
            "Epoch 1 | Hybrid loss: 0.067542\n",
            "Epoch 2 | Hybrid loss: 0.058658\n",
            "Epoch 3 | Hybrid loss: 0.061933\n",
            "Epoch 4 | Hybrid loss: 0.067862\n",
            "STOP-AWARE mean reward: -5.1031\n",
            "STOP-AWARE scores: [-5.0746329999999995, -5.184354, -4.712746, -5.1746859999999995, -5.143427, -5.639193, -5.191204, -4.441238, -5.787152000000001, -4.682066999999999]\n",
            "Greedy mean reward: -6.4609\n",
            "Greedy scores: [-6.469275, -6.46, -6.46, -6.46, -6.46, -6.46, -6.46, -6.46, -6.46, -6.46]\n",
            "🟢 STOP-Preference labels: +8 (total 104)\n",
            "Teacher+  n=104 | min=1.500 max=2.000 mean=1.788\n",
            "Exp−      n=31 | min=-0.542 max=-0.047 mean=-0.358\n",
            "✅ Weighted pairs: 312\n",
            "Epoch 1 | Hybrid loss: 0.049039\n",
            "Epoch 2 | Hybrid loss: 0.048299\n",
            "Epoch 3 | Hybrid loss: 0.052058\n",
            "Epoch 4 | Hybrid loss: 0.051907\n",
            "STOP-AWARE mean reward: -5.0127\n",
            "STOP-AWARE scores: [-5.2525569999999995, -5.5678469999999995, -5.64137, -6.1128, -3.8403219999999996, -5.01291, -4.428458999999999, -5.087058999999999, -4.137443, -5.046106]\n",
            "Greedy mean reward: -5.6214\n",
            "Greedy scores: [-5.633510000000002, -5.620000000000002, -5.620000000000002, -5.620000000000002, -5.620000000000002, -5.620000000000002, -5.620000000000002, -5.620000000000002, -5.620000000000002, -5.620000000000002]\n",
            "🟢 STOP-Preference labels: +8 (total 112)\n",
            "Teacher+  n=112 | min=1.500 max=2.000 mean=1.804\n",
            "Exp−      n=31 | min=-0.542 max=-0.047 mean=-0.358\n",
            "✅ Weighted pairs: 336\n",
            "Epoch 1 | Hybrid loss: 0.040544\n",
            "Epoch 2 | Hybrid loss: 0.042066\n",
            "Epoch 3 | Hybrid loss: 0.044829\n",
            "Epoch 4 | Hybrid loss: 0.036441\n",
            "STOP-AWARE mean reward: -5.0509\n",
            "STOP-AWARE scores: [-4.820180000000001, -6.129232999999999, -6.003519, -4.3407610000000005, -5.378776, -3.761008, -5.244991, -5.279644, -4.495745, -5.05484]\n",
            "Greedy mean reward: -6.3210\n",
            "Greedy scores: [-6.330208999999999, -6.319999999999999, -6.319999999999999, -6.319999999999999, -6.319999999999999, -6.319999999999999, -6.319999999999999, -6.319999999999999, -6.319999999999999, -6.319999999999999]\n",
            "🟢 STOP-Preference labels: +8 (total 120)\n",
            "Teacher+  n=120 | min=1.500 max=2.000 mean=1.817\n",
            "Exp−      n=31 | min=-0.542 max=-0.047 mean=-0.358\n",
            "✅ Weighted pairs: 360\n",
            "Epoch 1 | Hybrid loss: 0.037003\n",
            "Epoch 2 | Hybrid loss: 0.036432\n",
            "Epoch 3 | Hybrid loss: 0.036834\n",
            "Epoch 4 | Hybrid loss: 0.037124\n",
            "STOP-AWARE mean reward: -5.3891\n",
            "STOP-AWARE scores: [-4.131226, -5.891041, -5.139397, -5.734896000000001, -5.184833, -6.163343000000001, -5.473198, -5.306744999999999, -4.425042, -6.441306999999999]\n",
            "Greedy mean reward: -6.9800\n",
            "Greedy scores: [-6.98, -6.98, -6.98, -6.98, -6.98, -6.98, -6.98, -6.98, -6.98, -6.98]\n",
            "🟢 STOP-Preference labels: +8 (total 128)\n",
            "Teacher+  n=128 | min=1.500 max=2.000 mean=1.828\n",
            "Exp−      n=31 | min=-0.542 max=-0.047 mean=-0.358\n",
            "✅ Weighted pairs: 384\n",
            "Epoch 1 | Hybrid loss: 0.035746\n",
            "Epoch 2 | Hybrid loss: 0.035783\n",
            "Epoch 3 | Hybrid loss: 0.035791\n",
            "Epoch 4 | Hybrid loss: 0.035776\n",
            "STOP-AWARE mean reward: -5.7023\n",
            "STOP-AWARE scores: [-4.480348, -5.6227, -5.833914000000001, -5.678064, -4.604239000000001, -6.976304, -5.506381999999999, -5.642843, -6.612297000000001, -6.065982]\n",
            "Greedy mean reward: -6.9119\n",
            "Greedy scores: [-6.298670000000001, -6.98, -6.98, -6.98, -6.98, -6.98, -6.98, -6.98, -6.98, -6.98]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for step in range(8):  # just ~8 rounds is enough\n",
        "    collect_stop_preference(env, prakriti_tree, policy, trials=8)\n",
        "    show_clean_stats()\n",
        "    train_hybrid_pairwise_clean(policy, opt, mse, teacher_pos3, experience_neg3,\n",
        "                                epochs=4, batch_size=128, max_pairs=1024)\n",
        "    eval_policy_stop_and_greedy(env, prakriti_tree, policy, rounds=10, score_floor=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Hklyk17FN6q",
        "outputId": "b0d6b6af-1b04-4e7c-9b4d-9f6ed26646c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STOP-AWARE mean reward: -4.8766\n",
            "STOP-AWARE scores: [-5.358662000000001, -5.82298, -4.627963, -5.356586, -4.9787, -3.9705399999999997, -4.696218999999999, -5.399184, -4.340446999999999, -4.21488]\n",
            "Greedy mean reward: -5.8000\n",
            "Greedy scores: [-5.8, -5.8, -5.8, -5.8, -5.8, -5.8, -5.8, -5.8, -5.8, -5.8]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(-4.876616100000001, -5.799999999999999)"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_policy_stop_and_greedy(env, prakriti_tree, policy, rounds=10, score_floor=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd5TvIAMGMJz",
        "outputId": "880bec8d-df46-4708-c2c5-4f32e4567613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧑‍🏫 Strong teacher positives: +40 (total 124)\n",
            "📦 Experience negatives: +29 (total 135)\n",
            "Teacher+  n=124 | min=1.417 max=6.365 mean=1.804\n",
            "Exp−      n=135 | min=-0.705 max=-0.117 mean=-0.565\n",
            "✅ Weighted pairs: 372\n",
            "Epoch 1 | Hybrid loss: 0.090747\n",
            "Epoch 2 | Hybrid loss: 0.090613\n",
            "Epoch 3 | Hybrid loss: 0.090534\n",
            "Epoch 4 | Hybrid loss: 0.090409\n",
            "STOP-AWARE: -5.5009144\n",
            "🧑‍🏫 Strong teacher positives: +25 (total 149)\n",
            "📦 Experience negatives: +29 (total 164)\n",
            "Teacher+  n=149 | min=1.417 max=6.365 mean=1.776\n",
            "Exp−      n=164 | min=-0.705 max=-0.117 mean=-0.568\n",
            "✅ Weighted pairs: 447\n",
            "Epoch 1 | Hybrid loss: 0.086419\n",
            "Epoch 2 | Hybrid loss: 0.083844\n",
            "Epoch 3 | Hybrid loss: 0.086020\n",
            "Epoch 4 | Hybrid loss: 0.086497\n",
            "STOP-AWARE: -5.1621974\n",
            "🧑‍🏫 Strong teacher positives: +20 (total 169)\n",
            "📦 Experience negatives: +30 (total 194)\n",
            "Teacher+  n=169 | min=1.417 max=6.365 mean=1.774\n",
            "Exp−      n=194 | min=-0.705 max=-0.017 mean=-0.574\n",
            "✅ Weighted pairs: 507\n",
            "Epoch 1 | Hybrid loss: 0.089822\n",
            "Epoch 2 | Hybrid loss: 0.089930\n",
            "Epoch 3 | Hybrid loss: 0.089945\n",
            "Epoch 4 | Hybrid loss: 0.090218\n",
            "STOP-AWARE: -5.9062842\n",
            "🧑‍🏫 Strong teacher positives: +20 (total 189)\n",
            "📦 Experience negatives: +30 (total 224)\n",
            "Teacher+  n=189 | min=1.417 max=6.400 mean=1.818\n",
            "Exp−      n=224 | min=-0.715 max=-0.017 mean=-0.580\n",
            "✅ Weighted pairs: 567\n",
            "Epoch 1 | Hybrid loss: 0.092421\n",
            "Epoch 2 | Hybrid loss: 0.090261\n",
            "Epoch 3 | Hybrid loss: 0.090305\n",
            "Epoch 4 | Hybrid loss: 0.090981\n",
            "STOP-AWARE: -5.890828099999999\n",
            "🧑‍🏫 Strong teacher positives: +15 (total 204)\n",
            "📦 Experience negatives: +30 (total 254)\n",
            "Teacher+  n=204 | min=1.417 max=7.978 mean=1.829\n",
            "Exp−      n=254 | min=-0.734 max=-0.017 mean=-0.589\n",
            "✅ Weighted pairs: 612\n",
            "Epoch 1 | Hybrid loss: 0.092377\n",
            "Epoch 2 | Hybrid loss: 0.092510\n",
            "Epoch 3 | Hybrid loss: 0.092532\n",
            "Epoch 4 | Hybrid loss: 0.092629\n",
            "STOP-AWARE: -5.582739\n",
            "🧑‍🏫 Strong teacher positives: +20 (total 224)\n",
            "📦 Experience negatives: +30 (total 284)\n",
            "Teacher+  n=224 | min=1.417 max=7.978 mean=1.824\n",
            "Exp−      n=284 | min=-0.734 max=-0.017 mean=-0.589\n",
            "✅ Weighted pairs: 672\n",
            "Epoch 1 | Hybrid loss: 0.086899\n",
            "Epoch 2 | Hybrid loss: 0.091755\n",
            "Epoch 3 | Hybrid loss: 0.091743\n",
            "Epoch 4 | Hybrid loss: 0.087930\n",
            "STOP-AWARE: -5.354968999999999\n",
            "🧑‍🏫 Strong teacher positives: +6 (total 230)\n",
            "📦 Experience negatives: +30 (total 314)\n",
            "Teacher+  n=230 | min=1.417 max=7.978 mean=1.816\n",
            "Exp−      n=314 | min=-0.734 max=-0.017 mean=-0.590\n",
            "✅ Weighted pairs: 690\n",
            "Epoch 1 | Hybrid loss: 0.083171\n",
            "Epoch 2 | Hybrid loss: 0.085142\n",
            "Epoch 3 | Hybrid loss: 0.086828\n",
            "Epoch 4 | Hybrid loss: 0.087425\n",
            "STOP-AWARE: -4.4697879\n",
            "🧑‍🏫 Strong teacher positives: +20 (total 250)\n",
            "📦 Experience negatives: +30 (total 344)\n",
            "Teacher+  n=250 | min=1.356 max=7.978 mean=1.793\n",
            "Exp−      n=344 | min=-0.734 max=-0.017 mean=-0.583\n",
            "✅ Weighted pairs: 750\n",
            "Epoch 1 | Hybrid loss: 0.085126\n",
            "Epoch 2 | Hybrid loss: 0.085238\n",
            "Epoch 3 | Hybrid loss: 0.085250\n",
            "Epoch 4 | Hybrid loss: 0.085220\n",
            "STOP-AWARE: -5.7523935999999996\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for step in range(8):\n",
        "    collect_teacher_strong(env, prakriti_tree, policy, episodes=5, max_steps=6, beam=20)\n",
        "    collect_experience_negatives_clean(env, prakriti_tree, policy, episodes=5, max_steps=6)\n",
        "    show_clean_stats()\n",
        "    train_hybrid_pairwise_clean(policy, opt, mse, teacher_pos3, experience_neg3,\n",
        "                                epochs=4, batch_size=128, max_pairs=1024)\n",
        "    mean_stop = np.mean([run_episode_stop_aware(env, prakriti_tree, policy) for _ in range(10)])\n",
        "    print(\"STOP-AWARE:\", mean_stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUUO7km2Mh3H",
        "outputId": "c041f366-1b89-4ff0-f970-9af20c1b6bf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Phase 9 saved. Ready for Phase 10.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pickle\n",
        "\n",
        "with open(f\"{SAVE_PATH}/phase9_teacher_pos_final.pkl\", \"wb\") as f:\n",
        "    pickle.dump(teacher_pos3, f)\n",
        "\n",
        "with open(f\"{SAVE_PATH}/phase9_experience_neg_final.pkl\", \"wb\") as f:\n",
        "    pickle.dump(experience_neg3, f)\n",
        "\n",
        "torch.save(policy.state_dict(), f\"{SAVE_PATH}/phase9_policy.pt\")\n",
        "\n",
        "print(\"✅ Phase 9 saved. Ready for Phase 10.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV6P07mTvz3o"
      },
      "source": [
        "# Phase 11: Balanced Model Behaviour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaH2aTuBwVLM",
        "outputId": "30326af1-e976-450b-9ded-6f71a02fd033"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-08 13:56:48,741 - INFO - Initialized TLiteExpert dim=50 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=50 on cpu\n",
            "2025-11-08 13:56:48,745 - INFO - Initialized TLiteExpert dim=50 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=50 on cpu\n",
            "2025-11-08 13:56:48,749 - INFO - Initialized TLiteExpert dim=50 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=50 on cpu\n",
            "2025-11-08 13:56:48,755 - INFO - Initialized TLiteExpert dim=50 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=50 on cpu\n",
            "2025-11-08 13:56:48,762 - INFO - Initialized TLiteExpert dim=50 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=50 on cpu\n",
            "2025-11-08 13:56:48,768 - INFO - Initialized TLiteExpert dim=50 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=50 on cpu\n",
            "2025-11-08 13:56:48,773 - INFO - Initialized TLiteExpert dim=50 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=50 on cpu\n",
            "2025-11-08 13:56:48,778 - INFO - Initialized TLiteExpert dim=50 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=50 on cpu\n",
            "2025-11-08 13:56:48,783 - INFO - Initialized TLiteRouter num_experts=8, top_k=2 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteRouter num_experts=8, top_k=2 on cpu\n",
            "2025-11-08 13:56:48,793 - INFO - Initialized TLiteV6 num_experts=8 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteV6 num_experts=8 on cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from phase8_clean import *\n",
        "policy, opt, mse = init_policy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qKIFt_cv7xO",
        "outputId": "8868ebee-8055-40cb-c1da-f975f861cfb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== PHASE 11 — Balanced STOP/Depth ROUND 1 =====\n",
            "⚖️ Balanced STOP/Depth labels added: +8 (total 8)\n",
            "📦 Experience negatives: +23 (total 23)\n",
            "Teacher+  n=8 | min=1.600 max=1.600 mean=1.600\n",
            "Exp−      n=23 | min=-0.666 max=-0.157 mean=-0.530\n",
            "✅ Weighted pairs: 24\n",
            "Epoch 1 | Hybrid loss: 0.088832\n",
            "Epoch 2 | Hybrid loss: 0.087944\n",
            "Epoch 3 | Hybrid loss: 0.087066\n",
            "Epoch 4 | Hybrid loss: 0.086196\n",
            "Epoch 5 | Hybrid loss: 0.085336\n",
            "STOP-AWARE mean reward: -5.1054\n",
            "STOP-AWARE scores: [-5.325594, -5.828528000000001, -4.355714000000001, -4.754284999999999, -5.819572000000001, -5.470591, -4.568402, -4.474152, -5.214576999999999, -5.242301]\n",
            "Greedy mean reward: -6.6007\n",
            "Greedy scores: [-6.606702000000001, -6.6000000000000005, -6.6000000000000005, -6.6000000000000005, -6.6000000000000005, -6.6000000000000005, -6.6000000000000005, -6.6000000000000005, -6.6000000000000005, -6.6000000000000005]\n",
            "STOP-AWARE: -5.1053716 | GREEDY: -6.600670200000001\n",
            "\n",
            "===== PHASE 11 — Balanced STOP/Depth ROUND 2 =====\n",
            "⚖️ Balanced STOP/Depth labels added: +8 (total 16)\n",
            "📦 Experience negatives: +24 (total 47)\n",
            "Teacher+  n=16 | min=1.600 max=1.600 mean=1.600\n",
            "Exp−      n=47 | min=-0.692 max=-0.157 mean=-0.564\n",
            "✅ Weighted pairs: 48\n",
            "Epoch 1 | Hybrid loss: 0.081502\n",
            "Epoch 2 | Hybrid loss: 0.080446\n",
            "Epoch 3 | Hybrid loss: 0.079394\n",
            "Epoch 4 | Hybrid loss: 0.078348\n",
            "Epoch 5 | Hybrid loss: 0.077913\n",
            "STOP-AWARE mean reward: -5.9675\n",
            "STOP-AWARE scores: [-6.069109999999999, -6.056098999999999, -6.082483999999999, -6.507177999999999, -6.047578999999999, -5.505573999999999, -5.769729000000001, -6.433783, -4.619224000000001, -6.583755]\n",
            "Greedy mean reward: -7.0572\n",
            "Greedy scores: [-6.851667000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001]\n",
            "STOP-AWARE: -5.9674515 | GREEDY: -7.057166700000001\n",
            "\n",
            "===== PHASE 11 — Balanced STOP/Depth ROUND 3 =====\n",
            "⚖️ Balanced STOP/Depth labels added: +8 (total 24)\n",
            "📦 Experience negatives: +24 (total 71)\n",
            "Teacher+  n=24 | min=1.600 max=1.600 mean=1.600\n",
            "Exp−      n=71 | min=-0.725 max=-0.157 mean=-0.577\n",
            "✅ Weighted pairs: 72\n",
            "Epoch 1 | Hybrid loss: 0.080458\n",
            "Epoch 2 | Hybrid loss: 0.080289\n",
            "Epoch 3 | Hybrid loss: 0.079986\n",
            "Epoch 4 | Hybrid loss: 0.079592\n",
            "Epoch 5 | Hybrid loss: 0.079137\n",
            "STOP-AWARE mean reward: -5.2302\n",
            "STOP-AWARE scores: [-6.611259000000001, -5.976449000000001, -5.0738769999999995, -4.688963, -5.723711000000001, -5.601464000000001, -5.781964, -2.409258, -3.8192760000000003, -6.615494000000001]\n",
            "Greedy mean reward: -6.8400\n",
            "Greedy scores: [-6.840000000000001, -6.840000000000001, -6.840000000000001, -6.840000000000001, -6.840000000000001, -6.840000000000001, -6.840000000000001, -6.840000000000001, -6.840000000000001, -6.840000000000001]\n",
            "STOP-AWARE: -5.2301715 | GREEDY: -6.840000000000001\n",
            "\n",
            "===== PHASE 11 — Balanced STOP/Depth ROUND 4 =====\n",
            "⚖️ Balanced STOP/Depth labels added: +8 (total 32)\n",
            "📦 Experience negatives: +23 (total 94)\n",
            "Teacher+  n=32 | min=1.600 max=1.600 mean=1.600\n",
            "Exp−      n=94 | min=-0.725 max=-0.008 mean=-0.578\n",
            "✅ Weighted pairs: 96\n",
            "Epoch 1 | Hybrid loss: 0.074264\n",
            "Epoch 2 | Hybrid loss: 0.073740\n",
            "Epoch 3 | Hybrid loss: 0.073199\n",
            "Epoch 4 | Hybrid loss: 0.072651\n",
            "Epoch 5 | Hybrid loss: 0.072102\n",
            "STOP-AWARE mean reward: -5.5007\n",
            "STOP-AWARE scores: [-4.5793230000000005, -6.269447000000001, -5.41501, -5.539118, -5.7971770000000005, -5.79684, -5.346908000000001, -5.890276999999999, -5.399866000000001, -4.97269]\n",
            "Greedy mean reward: -6.7408\n",
            "Greedy scores: [-6.748074000000002, -6.740000000000002, -6.740000000000002, -6.740000000000002, -6.740000000000002, -6.740000000000002, -6.740000000000002, -6.740000000000002, -6.740000000000002, -6.740000000000002]\n",
            "STOP-AWARE: -5.5006656000000005 | GREEDY: -6.740807400000003\n",
            "\n",
            "===== PHASE 11 — Balanced STOP/Depth ROUND 5 =====\n",
            "⚖️ Balanced STOP/Depth labels added: +8 (total 40)\n",
            "📦 Experience negatives: +23 (total 117)\n",
            "Teacher+  n=40 | min=1.600 max=1.600 mean=1.600\n",
            "Exp−      n=117 | min=-0.725 max=-0.008 mean=-0.576\n",
            "✅ Weighted pairs: 120\n",
            "Epoch 1 | Hybrid loss: 0.077455\n",
            "Epoch 2 | Hybrid loss: 0.077297\n",
            "Epoch 3 | Hybrid loss: 0.077069\n",
            "Epoch 4 | Hybrid loss: 0.076779\n",
            "Epoch 5 | Hybrid loss: 0.076433\n",
            "STOP-AWARE mean reward: -5.1391\n",
            "STOP-AWARE scores: [-4.145340999999999, -5.610613, -4.497809, -5.216197, -5.249058000000001, -5.659434999999999, -5.924950999999999, -5.4165030000000005, -3.8011830000000004, -5.8696280000000005]\n",
            "Greedy mean reward: -6.4600\n",
            "Greedy scores: [-6.46, -6.46, -6.46, -6.46, -6.46, -6.46, -6.46, -6.46, -6.46, -6.46]\n",
            "STOP-AWARE: -5.1390718 | GREEDY: -6.459999999999999\n",
            "\n",
            "===== PHASE 11 — Balanced STOP/Depth ROUND 6 =====\n",
            "⚖️ Balanced STOP/Depth labels added: +8 (total 48)\n",
            "📦 Experience negatives: +22 (total 139)\n",
            "Teacher+  n=48 | min=1.600 max=1.600 mean=1.600\n",
            "Exp−      n=139 | min=-0.725 max=-0.008 mean=-0.571\n",
            "✅ Weighted pairs: 144\n",
            "Epoch 1 | Hybrid loss: 0.070826\n",
            "Epoch 2 | Hybrid loss: 0.080208\n",
            "Epoch 3 | Hybrid loss: 0.067798\n",
            "Epoch 4 | Hybrid loss: 0.072584\n",
            "Epoch 5 | Hybrid loss: 0.072426\n",
            "STOP-AWARE mean reward: -4.9228\n",
            "STOP-AWARE scores: [-4.427339, -4.515023, -5.243894, -5.317712, -5.081446, -4.015551, -5.055745, -4.963957, -5.620302, -4.9873639999999995]\n",
            "Greedy mean reward: -6.4610\n",
            "Greedy scores: [-6.469588, -6.46, -6.46, -6.46, -6.46, -6.46, -6.46, -6.46, -6.46, -6.46]\n",
            "STOP-AWARE: -4.9228333 | GREEDY: -6.4609588\n",
            "\n",
            "===== PHASE 11 — Balanced STOP/Depth ROUND 7 =====\n",
            "⚖️ Balanced STOP/Depth labels added: +8 (total 56)\n",
            "📦 Experience negatives: +23 (total 162)\n",
            "Teacher+  n=56 | min=1.600 max=1.600 mean=1.600\n",
            "Exp−      n=162 | min=-0.725 max=-0.008 mean=-0.562\n",
            "✅ Weighted pairs: 168\n",
            "Epoch 1 | Hybrid loss: 0.070284\n",
            "Epoch 2 | Hybrid loss: 0.069328\n",
            "Epoch 3 | Hybrid loss: 0.073007\n",
            "Epoch 4 | Hybrid loss: 0.075138\n",
            "Epoch 5 | Hybrid loss: 0.072631\n",
            "STOP-AWARE mean reward: -5.0123\n",
            "STOP-AWARE scores: [-5.079441, -4.914277, -5.259138, -6.000977999999999, -4.96943, -4.263375, -6.075958, -6.158732, -3.606691, -3.7945519999999995]\n",
            "Greedy mean reward: -6.3209\n",
            "Greedy scores: [-6.329377999999999, -6.319999999999999, -6.319999999999999, -6.319999999999999, -6.319999999999999, -6.319999999999999, -6.319999999999999, -6.319999999999999, -6.319999999999999, -6.319999999999999]\n",
            "STOP-AWARE: -5.012257199999999 | GREEDY: -6.3209378\n",
            "\n",
            "===== PHASE 11 — Balanced STOP/Depth ROUND 8 =====\n",
            "⚖️ Balanced STOP/Depth labels added: +8 (total 64)\n",
            "📦 Experience negatives: +24 (total 186)\n",
            "Teacher+  n=64 | min=1.600 max=1.600 mean=1.600\n",
            "Exp−      n=186 | min=-0.725 max=-0.008 mean=-0.555\n",
            "✅ Weighted pairs: 192\n",
            "Epoch 1 | Hybrid loss: 0.069265\n",
            "Epoch 2 | Hybrid loss: 0.066665\n",
            "Epoch 3 | Hybrid loss: 0.069756\n",
            "Epoch 4 | Hybrid loss: 0.070018\n",
            "Epoch 5 | Hybrid loss: 0.070971\n",
            "STOP-AWARE mean reward: -5.5821\n",
            "STOP-AWARE scores: [-4.635197000000001, -4.82135, -6.32462, -5.327750999999999, -5.794532, -5.029167, -5.783303, -6.248376999999999, -5.374269, -6.482499]\n",
            "Greedy mean reward: -7.0807\n",
            "Greedy scores: [-7.086635000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001]\n",
            "STOP-AWARE: -5.5821065 | GREEDY: -7.0806635\n",
            "\n",
            "===== PHASE 11 — Balanced STOP/Depth ROUND 9 =====\n",
            "⚖️ Balanced STOP/Depth labels added: +8 (total 72)\n",
            "📦 Experience negatives: +24 (total 210)\n",
            "Teacher+  n=72 | min=1.600 max=1.600 mean=1.600\n",
            "Exp−      n=210 | min=-0.725 max=-0.008 mean=-0.568\n",
            "✅ Weighted pairs: 216\n",
            "Epoch 1 | Hybrid loss: 0.064661\n",
            "Epoch 2 | Hybrid loss: 0.065394\n",
            "Epoch 3 | Hybrid loss: 0.065930\n",
            "Epoch 4 | Hybrid loss: 0.064742\n",
            "Epoch 5 | Hybrid loss: 0.065460\n",
            "STOP-AWARE mean reward: -5.7556\n",
            "STOP-AWARE scores: [-6.009738, -4.763855, -6.5512, -3.250827, -5.415602000000001, -6.481624, -5.211669, -6.618779000000001, -6.894057999999999, -6.35838]\n",
            "Greedy mean reward: -7.2800\n",
            "Greedy scores: [-7.2799999999999985, -7.2799999999999985, -7.2799999999999985, -7.2799999999999985, -7.2799999999999985, -7.2799999999999985, -7.2799999999999985, -7.2799999999999985, -7.2799999999999985, -7.2799999999999985]\n",
            "STOP-AWARE: -5.755573200000001 | GREEDY: -7.2799999999999985\n",
            "\n",
            "===== PHASE 11 — Balanced STOP/Depth ROUND 10 =====\n",
            "⚖️ Balanced STOP/Depth labels added: +8 (total 80)\n",
            "📦 Experience negatives: +23 (total 233)\n",
            "Teacher+  n=80 | min=1.600 max=1.600 mean=1.600\n",
            "Exp−      n=233 | min=-0.728 max=-0.008 mean=-0.569\n",
            "✅ Weighted pairs: 240\n",
            "Epoch 1 | Hybrid loss: 0.067762\n",
            "Epoch 2 | Hybrid loss: 0.067105\n",
            "Epoch 3 | Hybrid loss: 0.067405\n",
            "Epoch 4 | Hybrid loss: 0.067142\n",
            "Epoch 5 | Hybrid loss: 0.067273\n",
            "STOP-AWARE mean reward: -5.7106\n",
            "STOP-AWARE scores: [-5.383278, -5.779254, -5.246954, -5.284815999999999, -6.045816, -5.916416999999999, -4.8402449999999995, -6.042230999999999, -6.136664000000001, -6.4303490000000005]\n",
            "Greedy mean reward: -7.0800\n",
            "Greedy scores: [-7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001]\n",
            "STOP-AWARE: -5.7106024 | GREEDY: -7.080000000000001\n",
            "\n",
            "===== PHASE 11 — Balanced STOP/Depth ROUND 11 =====\n",
            "⚖️ Balanced STOP/Depth labels added: +8 (total 88)\n",
            "📦 Experience negatives: +24 (total 257)\n",
            "Teacher+  n=88 | min=1.600 max=1.600 mean=1.600\n",
            "Exp−      n=257 | min=-0.728 max=-0.008 mean=-0.563\n",
            "✅ Weighted pairs: 264\n",
            "Epoch 1 | Hybrid loss: 0.062762\n",
            "Epoch 2 | Hybrid loss: 0.069690\n",
            "Epoch 3 | Hybrid loss: 0.069675\n",
            "Epoch 4 | Hybrid loss: 0.062616\n",
            "Epoch 5 | Hybrid loss: 0.066095\n",
            "STOP-AWARE mean reward: -5.7051\n",
            "STOP-AWARE scores: [-6.109407000000001, -5.590170999999999, -6.144913, -4.462516, -4.726561, -5.805909, -6.447395000000001, -5.9414880000000005, -6.724083, -5.098735]\n",
            "Greedy mean reward: -7.0807\n",
            "Greedy scores: [-7.086842000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001, -7.080000000000001]\n",
            "STOP-AWARE: -5.7051178 | GREEDY: -7.0806842\n",
            "\n",
            "===== PHASE 11 — Balanced STOP/Depth ROUND 12 =====\n",
            "⚖️ Balanced STOP/Depth labels added: +8 (total 96)\n",
            "📦 Experience negatives: +23 (total 280)\n",
            "Teacher+  n=96 | min=1.600 max=1.600 mean=1.600\n",
            "Exp−      n=280 | min=-0.728 max=-0.008 mean=-0.566\n",
            "✅ Weighted pairs: 288\n",
            "Epoch 1 | Hybrid loss: 0.065351\n",
            "Epoch 2 | Hybrid loss: 0.068119\n",
            "Epoch 3 | Hybrid loss: 0.067371\n",
            "Epoch 4 | Hybrid loss: 0.063844\n",
            "Epoch 5 | Hybrid loss: 0.065199\n",
            "STOP-AWARE mean reward: -5.0472\n",
            "STOP-AWARE scores: [-5.446464, -3.542628, -3.7596849999999997, -5.194783, -5.69225, -6.143571, -5.428612, -5.458381999999999, -4.550787, -5.255151]\n",
            "Greedy mean reward: -6.3209\n",
            "Greedy scores: [-6.328962999999999, -6.319999999999999, -6.319999999999999, -6.319999999999999, -6.319999999999999, -6.319999999999999, -6.319999999999999, -6.319999999999999, -6.319999999999999, -6.319999999999999]\n",
            "STOP-AWARE: -5.047231299999999 | GREEDY: -6.320896299999999\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for step in range(12):   # ~12 rounds works well\n",
        "    print(f\"\\n===== PHASE 11 — Balanced STOP/Depth ROUND {step+1} =====\")\n",
        "\n",
        "    collect_balanced_stop_depth(env, prakriti_tree, policy, trials=8)\n",
        "    collect_experience_negatives_clean(env, prakriti_tree, policy, episodes=4, max_steps=6)\n",
        "    show_clean_stats()\n",
        "\n",
        "    train_hybrid_pairwise_clean(policy, opt, mse,\n",
        "                                teacher_pos3, experience_neg3,\n",
        "                                epochs=5, batch_size=128,\n",
        "                                margin=0.15, w_pair=0.6, w_reg=0.3,\n",
        "                                max_pairs=1800, k_neg=3)\n",
        "\n",
        "    soft, greedy = eval_policy_stop_and_greedy(env, prakriti_tree, policy,\n",
        "                                               rounds=10, score_floor=0.20)\n",
        "\n",
        "    print(\"STOP-AWARE:\", soft, \"| GREEDY:\", greedy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOYVU-MwzSTZ",
        "outputId": "99c1ea38-5d02-46d1-e5e3-47de4a060aa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Saved checkpoint → phase11_final\n"
          ]
        }
      ],
      "source": [
        "save_all(\"phase11_final\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqQnVSBNW97C"
      },
      "source": [
        "# Phase Live"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GHXWM3BXDOE"
      },
      "source": [
        "Phase 0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTOH-dcEXCpC",
        "outputId": "e5f0ee20-8d3d-4668-869b-f322c4e0f051"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Trait map written to trait_map.jsonl\n",
            "Total traits extracted: 132\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Helper: Convert Dosha label to numeric weights\n",
        "# ------------------------------------------------------\n",
        "def dosha_weights(label):\n",
        "    label = label.lower().replace(\" \", \"\")\n",
        "    parts = label.split(\"+\")\n",
        "\n",
        "    weight = {\"vata\": 0.0, \"pitta\": 0.0, \"kapha\": 0.0}\n",
        "    for p in parts:\n",
        "        if p in weight:\n",
        "            weight[p] += 1.0\n",
        "\n",
        "    total = sum(weight.values())\n",
        "    if total > 0:\n",
        "        for k in weight:\n",
        "            weight[k] /= total\n",
        "    return weight\n",
        "\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Helper: Convert cell text to trait tokens\n",
        "# ------------------------------------------------------\n",
        "def extract_traits(cell_value):\n",
        "    if not isinstance(cell_value, str):\n",
        "        return []\n",
        "\n",
        "    # Lowercase + remove punctuation\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", cell_value.lower())\n",
        "\n",
        "    # Split into words\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Remove very common low-information words\n",
        "    stop = {\"and\", \"to\", \"the\", \"of\", \"a\", \"in\", \"is\", \"very\", \"but\", \"no\"}\n",
        "    tokens = [t for t in tokens if t not in stop and len(t) > 2]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# PHASE 0 MAIN\n",
        "# Input: CSV dataset\n",
        "# Output: trait_map.jsonl\n",
        "# ------------------------------------------------------\n",
        "def build_trait_map(csv_path, label_col=\"Dosha\", output=\"trait_map.jsonl\"):\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # All columns except label are trait sources\n",
        "    trait_columns = [c for c in df.columns if c != label_col]\n",
        "\n",
        "    trait_stats = {}  # trait -> dict(vata, pitta, kapha)\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        label = str(row[label_col])\n",
        "        weights = dosha_weights(label)\n",
        "\n",
        "        for col in trait_columns:\n",
        "            traits = extract_traits(row[col])\n",
        "            for t in traits:\n",
        "                if t not in trait_stats:\n",
        "                    trait_stats[t] = {\"vata\": 0.0, \"pitta\": 0.0, \"kapha\": 0.0}\n",
        "\n",
        "                # Add weights\n",
        "                trait_stats[t][\"vata\"] += weights[\"vata\"]\n",
        "                trait_stats[t][\"pitta\"] += weights[\"pitta\"]\n",
        "                trait_stats[t][\"kapha\"] += weights[\"kapha\"]\n",
        "\n",
        "    # Normalize each trait vector\n",
        "    final_map = []\n",
        "    for trait, w in trait_stats.items():\n",
        "        total = w[\"vata\"] + w[\"pitta\"] + w[\"kapha\"]\n",
        "        if total > 0:\n",
        "            w = {k: round(v/total, 4) for k, v in w.items()}\n",
        "        final_map.append({\"trait\": trait, **w})\n",
        "\n",
        "    # Sort traits by dominance confidence\n",
        "    final_map.sort(key=lambda x: max(x[\"vata\"], x[\"pitta\"], x[\"kapha\"]), reverse=True)\n",
        "\n",
        "    # Write JSONL\n",
        "    with open(output, \"w\", encoding=\"utf-8\") as f:\n",
        "        for item in final_map:\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"[OK] Trait map written to {output}\")\n",
        "    print(f\"Total traits extracted: {len(final_map)}\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Example Run\n",
        "# ------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    build_trait_map(\"/content/Updated_Prakriti_With_Features.csv\", label_col=\"Dosha\", output=\"trait_map.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEnWtLpwZMpP"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import re, json\n",
        "from collections import Counter\n",
        "\n",
        "# If DifficultyScorer is available, we import it. Otherwise, define fallback.\n",
        "try:\n",
        "    from DifficultyScorer import DifficultyScorer\n",
        "except:\n",
        "    class DifficultyScorer:\n",
        "        def __init__(self, threshold=0.75):\n",
        "            self.threshold = threshold\n",
        "        def fit(self, df, y=None): return self\n",
        "        def transform(self, df):\n",
        "            records = []\n",
        "            for _, row in df.iterrows():\n",
        "                filled = sum(v is not None and v==v and v!=\"\" for v in row.values)\n",
        "                total = len(row.values)\n",
        "                score = filled / total\n",
        "                tree_type = \"three-tree\" if score > self.threshold else \"binary\"\n",
        "                records.append({\"difficulty_score\": round(score,4), \"tree_type\": tree_type})\n",
        "            return pd.DataFrame(records)\n",
        "\n",
        "\n",
        "def extract_tokens(text):\n",
        "    if not isinstance(text, str): return []\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n",
        "    tokens = [t for t in text.split() if len(t) > 2]\n",
        "    return tokens\n",
        "\n",
        "def phase1_process(df, label_col=\"Dosha\", out_dir=\"phase1_out\"):\n",
        "    import os\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # All columns except label\n",
        "    trait_columns = [c for c in df.columns if c != label_col]\n",
        "\n",
        "    # ------------------------\n",
        "    # (1) Trait Frequency Mining\n",
        "    # ------------------------\n",
        "    freq = Counter()\n",
        "    for _, row in df.iterrows():\n",
        "        for col in trait_columns:\n",
        "            tokens = extract_tokens(row[col])\n",
        "            freq.update(tokens)\n",
        "\n",
        "    # Filter out very rare traits\n",
        "    freq = {t: c for t, c in freq.items() if c >= 2}\n",
        "\n",
        "    # Save traits.jsonl\n",
        "    with open(f\"{out_dir}/traits.jsonl\", \"w\") as f:\n",
        "        for t, c in sorted(freq.items(), key=lambda x: -x[1]):\n",
        "            f.write(json.dumps({\"trait\": t, \"count\": c}, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    # ------------------------\n",
        "    # (2) Difficulty + Tree Type\n",
        "    # ------------------------\n",
        "    scorer = DifficultyScorer(threshold=0.6)  # lower/raise if needed\n",
        "    scores = scorer.transform(df[trait_columns])\n",
        "\n",
        "    with open(f\"{out_dir}/row_scores.jsonl\", \"w\") as f:\n",
        "        for idx, row in scores.iterrows():\n",
        "            f.write(json.dumps({\n",
        "                \"row_id\": int(idx),\n",
        "                \"difficulty_score\": row[\"difficulty_score\"],\n",
        "                \"tree_type\": row[\"tree_type\"]\n",
        "            }, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(\"✅ Phase 1 Complete\")\n",
        "    print(f\"Saved → {out_dir}/traits.jsonl\")\n",
        "    print(f\"Saved → {out_dir}/row_scores.jsonl\")\n",
        "    return freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-784nTiaFTn",
        "outputId": "073aadf4-c473-4cf7-a066-857bf44639fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Phase 1 Complete\n",
            "Saved → phase1_out/traits.jsonl\n",
            "Saved → phase1_out/row_scores.jsonl\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'medium': 2664,\n",
              " 'moderate': 6369,\n",
              " 'difficulties': 1200,\n",
              " 'gaining': 996,\n",
              " 'losing': 852,\n",
              " 'weight': 1200,\n",
              " 'average': 828,\n",
              " 'large': 1212,\n",
              " 'broad': 180,\n",
              " 'shoulders': 180,\n",
              " 'heavy': 384,\n",
              " 'bone': 768,\n",
              " 'structure': 768,\n",
              " 'white': 384,\n",
              " 'pale': 252,\n",
              " 'tans': 624,\n",
              " 'easily': 1728,\n",
              " 'dry': 2268,\n",
              " 'and': 1836,\n",
              " 'thin': 1560,\n",
              " 'cool': 814,\n",
              " 'touch': 456,\n",
              " 'rough': 1008,\n",
              " 'pigments': 192,\n",
              " 'aging': 192,\n",
              " 'black': 1320,\n",
              " 'brown': 1200,\n",
              " 'dull': 780,\n",
              " 'straight': 420,\n",
              " 'oily': 2016,\n",
              " 'long': 818,\n",
              " 'angular': 576,\n",
              " 'sized': 1512,\n",
              " 'penetrating': 444,\n",
              " 'light': 1164,\n",
              " 'sensitive': 799,\n",
              " 'eyes': 1200,\n",
              " 'eyelashes': 1200,\n",
              " 'blinking': 1044,\n",
              " 'wrinkled': 480,\n",
              " 'sunken': 480,\n",
              " 'rounded': 564,\n",
              " 'open': 312,\n",
              " 'nostrils': 312,\n",
              " 'big': 312,\n",
              " 'strong': 742,\n",
              " 'teeth': 1200,\n",
              " 'healthy': 132,\n",
              " 'gums': 1200,\n",
              " 'tight': 528,\n",
              " 'lips': 1200,\n",
              " 'which': 528,\n",
              " 'chaps': 528,\n",
              " 'thick': 828,\n",
              " 'smooth': 1308,\n",
              " 'polished': 264,\n",
              " 'slow': 576,\n",
              " 'but': 336,\n",
              " 'steady': 336,\n",
              " 'sweet': 1068,\n",
              " 'sour': 816,\n",
              " 'salty': 816,\n",
              " 'fast': 366,\n",
              " 'warm': 1049,\n",
              " 'vegan': 230,\n",
              " 'short': 680,\n",
              " 'fair': 576,\n",
              " 'skin': 576,\n",
              " 'sunburns': 576,\n",
              " 'red': 288,\n",
              " 'yellow': 288,\n",
              " 'knotted': 540,\n",
              " 'brittle': 1092,\n",
              " 'flat': 468,\n",
              " 'reddish': 588,\n",
              " 'break': 552,\n",
              " 'vegetarian': 621,\n",
              " 'sedentary': 344,\n",
              " 'high': 723,\n",
              " 'normal': 631,\n",
              " 'slim': 396,\n",
              " 'zone': 576,\n",
              " 'small': 1008,\n",
              " 'active': 576,\n",
              " 'darting': 576,\n",
              " 'dark': 948,\n",
              " 'are': 672,\n",
              " 'soft': 672,\n",
              " 'sharp': 384,\n",
              " 'flexible': 384,\n",
              " 'pink': 576,\n",
              " 'lustrous': 384,\n",
              " 'omnivorous': 349,\n",
              " 'bones': 432,\n",
              " 'prominent': 432,\n",
              " 'joints': 432,\n",
              " 'round': 468,\n",
              " 'full': 480,\n",
              " 'scanty': 948,\n",
              " 'crooked': 588,\n",
              " 'narrow': 588,\n",
              " 'low': 934,\n",
              " 'complexion': 372,\n",
              " 'pointed': 636,\n",
              " 'unbearable': 372,\n",
              " 'bitter': 384,\n",
              " 'astringent': 384,\n",
              " 'weak': 253,\n",
              " 'tall': 216,\n",
              " 'freckles': 252,\n",
              " 'many': 252,\n",
              " 'moles': 252,\n",
              " 'redness': 252,\n",
              " 'rashes': 252,\n",
              " 'curly': 240,\n",
              " 'heart': 336,\n",
              " 'shaped': 336,\n",
              " 'chin': 336,\n",
              " 'plump': 252,\n",
              " 'insensitive': 214,\n",
              " 'irregular': 972,\n",
              " 'protruding': 480,\n",
              " 'receding': 480,\n",
              " 'beautiful': 180,\n",
              " 'glowing': 180,\n",
              " 'moist': 168,\n",
              " 'greasy': 168,\n",
              " 'cold': 168,\n",
              " 'excessive': 264,\n",
              " 'fused': 156,\n",
              " 'pungent': 132,\n",
              " 'more': 156,\n",
              " 'less': 156,\n",
              " 'stable': 156}"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "df = pd.read_csv(\"/content/Updated_Prakriti_With_Features.csv\")\n",
        "phase1_process(df, label_col=\"Dosha\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jR1IknHqaFZc",
        "outputId": "192f6364-71e2-4c76-f410-cc2705382dfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   1200 \"tree_type\": \"three-tree\"\n"
          ]
        }
      ],
      "source": [
        "!grep -o '\"tree_type\": \"[^\"]*\"' phase1_out/row_scores.jsonl | sort | uniq -c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtqjB2xfa5eP",
        "outputId": "c54b5df7-f9f9-4edd-a1c0-f6c4abe4297f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Schema saved: /content/phase1_out/schema.json\n",
            "Columns: ['Body Size', 'Body Weight', 'Height', 'Bone Structure', 'Complexion', 'General feel of skin', 'Texture of Skin', 'Hair Color', 'Appearance of Hair', 'Shape of face', 'Eyes', 'Eyelashes', 'Blinking of Eyes', 'Cheeks', 'Nose', 'Teeth and gums', 'Lips', 'Nails', 'Appetite', 'Liking tastes', 'Dosha', 'Metabolism Type', 'Climate Preference', 'Stress Levels', 'Sleep Patterns', 'Dietary Habits', 'Physical Activity Level', 'Water Intake', 'Digestion Quality', 'Skin Sensitivity']\n",
            "Selected (30) saved → /content/phase1_out/selected_features.json\n",
            "Dropped (0) saved  → /content/phase1_out/dropped_features.json\n",
            "[Chunk 1] rows=1200 → totals={'binary': 1200, 'three-tree': 0}\n",
            "\n",
            "=== Phase 1 COMPLETE ===\n",
            "Output CSV: /content/phase1_out/phase1_augmented.csv\n",
            "Tree Type Counts: {'binary': 1200, 'three-tree': 0}\n",
            "Total Rows Processed: 1200\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# === Phase 1: Dataset prep + tree_type scoring ===\n",
        "\n",
        "import os, json\n",
        "import pandas as pd\n",
        "\n",
        "# ====== YOU EDIT THESE ======\n",
        "CSV_PATH         = \"/content/Updated_Prakriti_With_Features.csv\"   # your dataset\n",
        "OUT_DIR          = \"/content/phase1_out\"\n",
        "K_CHUNK          = 5000\n",
        "SAMPLE_ROWS      = 10000\n",
        "CAT_MAX_UNIQUE   = 20\n",
        "DIFF_THRESHOLD   = 0.75   # DifficultyScorer threshold\n",
        "# ============================\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "OUT_CSV     = os.path.join(OUT_DIR, \"phase1_augmented.csv\")\n",
        "ART_SELECTED= os.path.join(OUT_DIR, \"selected_features.json\")\n",
        "ART_DROPPED = os.path.join(OUT_DIR, \"dropped_features.json\")\n",
        "ART_SCHEMA  = os.path.join(OUT_DIR, \"schema.json\")\n",
        "ART_COUNTS  = os.path.join(OUT_DIR, \"tree_type_counts.json\")\n",
        "\n",
        "from FeatureSelector import FeatureSelector\n",
        "from DifficultyScorer import DifficultyScorer\n",
        "\n",
        "# ---------- 0) Save Schema ----------\n",
        "first_chunk = pd.read_csv(CSV_PATH, nrows=5)\n",
        "schema_info = {\n",
        "    \"columns\": first_chunk.columns.tolist(),\n",
        "    \"preview_rows\": len(first_chunk)\n",
        "}\n",
        "with open(ART_SCHEMA, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(schema_info, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Schema saved:\", ART_SCHEMA)\n",
        "print(\"Columns:\", schema_info[\"columns\"])\n",
        "\n",
        "# ---------- 1) Feature Selection ----------\n",
        "sample_df = pd.read_csv(CSV_PATH, nrows=SAMPLE_ROWS, low_memory=False)\n",
        "selector = FeatureSelector(cat_max_unique=CAT_MAX_UNIQUE).fit(sample_df)\n",
        "\n",
        "selected = selector.get_selected_features()\n",
        "dropped  = selector.get_dropped_features()\n",
        "\n",
        "with open(ART_SELECTED, \"w\", encoding=\"utf-8\") as f: json.dump(selected, f, ensure_ascii=False, indent=2)\n",
        "with open(ART_DROPPED,  \"w\", encoding=\"utf-8\") as f: json.dump(dropped,  f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Selected ({len(selected)}) saved →\", ART_SELECTED)\n",
        "print(f\"Dropped ({len(dropped)}) saved  →\", ART_DROPPED)\n",
        "\n",
        "# ---------- 2) Difficulty Scorer ----------\n",
        "scorer = DifficultyScorer(threshold=DIFF_THRESHOLD)\n",
        "\n",
        "# ---------- 3) Stream file in chunks ----------\n",
        "if os.path.exists(OUT_CSV):\n",
        "    os.remove(OUT_CSV)\n",
        "\n",
        "tree_type_counts = {\"binary\": 0, \"three-tree\": 0}\n",
        "total_rows = 0\n",
        "\n",
        "chunk_iter = pd.read_csv(CSV_PATH, chunksize=K_CHUNK, low_memory=False)\n",
        "\n",
        "for ci, chunk in enumerate(chunk_iter, start=1):\n",
        "\n",
        "    # Select + encode features\n",
        "    X_sel = selector.transform(chunk)\n",
        "\n",
        "    # Apply difficulty scoring\n",
        "    scores = scorer.transform(X_sel)  # returns difficulty_score + tree_type\n",
        "\n",
        "    # Merge original + scores\n",
        "    out = pd.concat([chunk.reset_index(drop=True), scores.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    # Update counters\n",
        "    vc = scores[\"tree_type\"].value_counts().to_dict()\n",
        "    tree_type_counts[\"binary\"]     += int(vc.get(\"binary\", 0))\n",
        "    tree_type_counts[\"three-tree\"] += int(vc.get(\"three-tree\", 0))\n",
        "    total_rows += len(chunk)\n",
        "\n",
        "    # Append output safely\n",
        "    out.to_csv(OUT_CSV, mode=\"a\", header=not os.path.exists(OUT_CSV), index=False)\n",
        "\n",
        "    print(f\"[Chunk {ci}] rows={len(chunk)} → totals={tree_type_counts}\")\n",
        "\n",
        "# ---------- 4) Save Summary ----------\n",
        "with open(ART_COUNTS, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\"total_rows\": total_rows, **tree_type_counts}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"\\n=== Phase 1 COMPLETE ===\")\n",
        "print(\"Output CSV:\", OUT_CSV)\n",
        "print(\"Tree Type Counts:\", tree_type_counts)\n",
        "print(\"Total Rows Processed:\", total_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0x62gdKezai"
      },
      "source": [
        "Phase 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdK7mod_e00L"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, json\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# use your already existing components\n",
        "from TreeBuilderV2 import TreeBuilderV2\n",
        "from TreeSnapshot import TreeSnapshot\n",
        "from RowBatchSummary import RowBatchSummary\n",
        "from tokenizer_and_embedding import TokenEmbedding\n",
        "\n",
        "\n",
        "def row_to_tokens(row, cols):\n",
        "    toks = []\n",
        "    for col in cols:\n",
        "        val = row.get(col)\n",
        "        if pd.isna(val):\n",
        "            toks.append(f\"{col}=NA\")\n",
        "        else:\n",
        "            toks.append(f\"{col}={str(val).strip()}\")\n",
        "    return toks\n",
        "\n",
        "\n",
        "def phase2_process(phase1_csv, phase1_dir, out_dir, dim=50, device=\"cpu\", chunk=400):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # load the selected features from phase1\n",
        "    with open(os.path.join(phase1_dir, \"selected_features.json\"), \"r\") as f:\n",
        "        selected_cols = json.load(f)\n",
        "\n",
        "    # read entire CSV (small-medium dataset)\n",
        "    df = pd.read_csv(phase1_csv)\n",
        "\n",
        "    # build vocab\n",
        "    vocab = set()\n",
        "    for col in selected_cols:\n",
        "        for v in df[col].astype(str).unique():\n",
        "            vocab.add(f\"{col}={v.strip()}\")\n",
        "    vocab = sorted(vocab)\n",
        "\n",
        "    # create embedding & builders\n",
        "    emb = TokenEmbedding(vocab=vocab, dim=dim, device=device)\n",
        "    builder_bin = TreeBuilderV2(mode=\"binary\", dim=dim, device=device)\n",
        "    builder_tri = TreeBuilderV2(mode=\"three\", dim=dim, device=device)\n",
        "\n",
        "    snapshots_path = os.path.join(out_dir, \"snapshots.jsonl\")\n",
        "    paths_path = os.path.join(out_dir, \"paths.jsonl\")\n",
        "    open(snapshots_path, \"w\").close()\n",
        "    open(paths_path, \"w\").close()\n",
        "\n",
        "    records = []\n",
        "    counts = Counter()\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        mode = str(row.get(\"tree_type\", \"binary\")).strip()\n",
        "        builder = builder_bin if mode == \"binary\" else builder_tri\n",
        "\n",
        "        tokens = row_to_tokens(row, selected_cols)\n",
        "        vec_pairs = [(t, emb.lookup(t)) for t in tokens]\n",
        "\n",
        "        root = builder.build_tree(vec_pairs, sample_id=f\"row{idx}\")\n",
        "        if root is None:\n",
        "            snapshot = {\"depth\": 0, \"node_count\": 0, \"leaf_count\": 0}\n",
        "            path = []\n",
        "        else:\n",
        "            snapshot = TreeSnapshot(root).to_dict()\n",
        "            path = builder.trace_dfs(root)\n",
        "\n",
        "        # write outputs\n",
        "        with open(snapshots_path, \"a\") as f:\n",
        "            f.write(json.dumps({\"row_index\": idx, **snapshot}) + \"\\n\")\n",
        "        with open(paths_path, \"a\") as f:\n",
        "            f.write(json.dumps({\"row_index\": idx, \"path\": path}) + \"\\n\")\n",
        "\n",
        "        counts[mode] += 1\n",
        "        records.append({\"row_index\": idx, \"path\": path, \"snapshot\": snapshot})\n",
        "\n",
        "    # batch summary\n",
        "    summary = RowBatchSummary(records).to_dict()\n",
        "    with open(os.path.join(out_dir, \"batch_summary.json\"), \"w\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    with open(os.path.join(out_dir, \"tree_type_counts.json\"), \"w\") as f:\n",
        "        json.dump(dict(counts), f, indent=2)\n",
        "\n",
        "    print(\"=== Phase 2 DONE ===\")\n",
        "    print(\"Snapshots:\", snapshots_path)\n",
        "    print(\"Paths:\", paths_path)\n",
        "    print(\"Batch Summary:\", os.path.join(out_dir, \"batch_summary.json\"))\n",
        "    print(\"Tree type counts:\", counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCAAvniJflzp",
        "outputId": "a3fd1b86-f878-472d-a9dc-75ab212385b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Phase 2 DONE ===\n",
            "Snapshots: /content/phase2_out/snapshots.jsonl\n",
            "Paths: /content/phase2_out/paths.jsonl\n",
            "Batch Summary: /content/phase2_out/batch_summary.json\n",
            "Tree type counts: Counter({'binary': 1200})\n"
          ]
        }
      ],
      "source": [
        "\n",
        "phase2_process(\n",
        "    phase1_csv=\"/content/phase1_out/phase1_augmented.csv\",\n",
        "    phase1_dir=\"/content/phase1_out\",\n",
        "    out_dir=\"/content/phase2_out\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6j15ODzhFUm"
      },
      "source": [
        "Phase 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hLeoh-lhGQI"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, json, re\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "from TreeBuilderV2 import TreeBuilderV2\n",
        "from TreeSnapshot import TreeSnapshot\n",
        "from tokenizer_and_embedding import TokenEmbedding, universal_tokenizer\n",
        "from embedding_matching import build_fragments, retrieve  # retrieval engine (TF-IDF)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Load selected features\n",
        "# ---------------------------\n",
        "def load_selected_features(phase1_dir):\n",
        "    with open(os.path.join(phase1_dir, \"selected_features.json\"), \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 2) Build fragment corpus for reference mode\n",
        "# ---------------------------\n",
        "def make_fragments(csv_path, selected_cols):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    frags = []\n",
        "    for _, row in df.iterrows():\n",
        "        parts = []\n",
        "        for c in selected_cols:\n",
        "            v = row[c]\n",
        "            if pd.isna(v):\n",
        "                continue\n",
        "            parts.append(f\"{c}={str(v).strip()}\")\n",
        "        frags.append(\" | \".join(parts))\n",
        "    return frags\n",
        "\n",
        "\n",
        "def init_reference_index(csv_path, phase1_dir):\n",
        "    selected_cols = load_selected_features(phase1_dir)\n",
        "    fragments = make_fragments(csv_path, selected_cols)\n",
        "    build_fragments(fragments)   # initializes TF-IDF index\n",
        "    return selected_cols\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Quick Reference Gate\n",
        "# ---------------------------\n",
        "def reference_gate(rets, threshold=0.75):\n",
        "    if not rets:\n",
        "        return False\n",
        "    avg = sum(r[\"retrievalconfidence\"] for r in rets) / len(rets)\n",
        "    return avg >= threshold\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 4) RL-Lite (entropy smoothing)\n",
        "# ---------------------------\n",
        "def smooth_snapshot(snapshot, retrievals):\n",
        "    # reduce entropy slightly based on retrieval confidence\n",
        "    if \"entropy\" not in snapshot:\n",
        "        snapshot[\"entropy\"] = 0.5\n",
        "\n",
        "    for r in retrievals:\n",
        "        c = r[\"retrievalconfidence\"]\n",
        "        snapshot[\"entropy\"] = max(0, snapshot[\"entropy\"] - (0.03 + 0.05 * c))\n",
        "        if snapshot[\"entropy\"] < 0.15:\n",
        "            break\n",
        "    return snapshot\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 5) Very small answer generator (Dosha-aware)\n",
        "# ---------------------------\n",
        "def infer_dosha(text):\n",
        "    t = text.lower()\n",
        "    if \"vata\" in t and \"pitta\" in t and \"kapha\" in t:\n",
        "        return \"tri-dosha\"\n",
        "    if \"vata\" in t and \"pitta\" in t:\n",
        "        return \"vata+pitta\"\n",
        "    if \"pitta\" in t and \"kapha\" in t:\n",
        "        return \"pitta+kapha\"\n",
        "    if \"vata\" in t and \"kapha\" in t:\n",
        "        return \"vata+kapha\"\n",
        "    if \"vata\" in t:\n",
        "        return \"vata\"\n",
        "    if \"pitta\" in t:\n",
        "        return \"pitta\"\n",
        "    if \"kapha\" in t:\n",
        "        return \"kapha\"\n",
        "    return \"unknown\"\n",
        "\n",
        "\n",
        "def generate_answer(dosha):\n",
        "    d = dosha.lower()\n",
        "    if d.startswith(\"vata\"):\n",
        "        return \"You show Vata tendencies. Favor warm, moist foods and stable daily routine.\"\n",
        "    if d.startswith(\"pitta\"):\n",
        "        return \"Pitta influence detected. Prefer cooling foods, reduce excessive heat/spice.\"\n",
        "    if d.startswith(\"kapha\"):\n",
        "        return \"Kapha traits seen. Use light, warm foods and regular active movement.\"\n",
        "    if \"vata\" in d and \"pitta\" in d:\n",
        "        return \"Vata-Pitta mix. Combine warm + cooling foods; keep schedule consistent.\"\n",
        "    if \"pitta\" in d and \"kapha\" in d:\n",
        "        return \"Pitta-Kapha mix. Avoid heavy + spicy overload; choose light cooling meals.\"\n",
        "    if \"vata\" in d and \"kapha\" in d:\n",
        "        return \"Vata-Kapha mix. Favor warm, light meals; avoid cold heavy foods.\"\n",
        "    return \"Maintain balanced diet, consistent sleep, and moderate exercise.\"\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 6) Query Runtime Function\n",
        "# ---------------------------\n",
        "def run_query(query, dim=50, device=\"cpu\", top_k=5):\n",
        "    # tokenize query and embed\n",
        "    tokens = universal_tokenizer(query)\n",
        "    if not tokens:\n",
        "        return {\"error\": \"empty query\"}\n",
        "\n",
        "    vocab = tokens[:12]\n",
        "    emb = TokenEmbedding(vocab=vocab, dim=dim, device=device)\n",
        "    pairs = [(t, emb.lookup(t)) for t in vocab]\n",
        "\n",
        "    builder = TreeBuilderV2(mode=\"binary\", dim=dim, device=device)\n",
        "    root = builder.build_tree(pairs, sample_id=\"query\")\n",
        "    snapshot = TreeSnapshot(root).to_dict() if root else {\"entropy\": 0.5}\n",
        "\n",
        "    # reference retrieval\n",
        "    rets = retrieve(query, k=top_k)\n",
        "\n",
        "    if reference_gate(rets):\n",
        "        best = rets[0][\"fragmenttext\"]\n",
        "        dosha = infer_dosha(best)\n",
        "        return {\n",
        "            \"mode\": \"reference\",\n",
        "            \"dosha\": dosha,\n",
        "            \"advice\": generate_answer(dosha),\n",
        "            \"retrieval_used\": best\n",
        "        }\n",
        "    else:\n",
        "        snapshot = smooth_snapshot(snapshot, rets)\n",
        "        best = rets[0][\"fragmenttext\"] if rets else \"\"\n",
        "        dosha = infer_dosha(best)\n",
        "        return {\n",
        "            \"mode\": \"rl_generative\",\n",
        "            \"dosha\": dosha,\n",
        "            \"advice\": generate_answer(dosha),\n",
        "            \"entropy_after_rl\": snapshot.get(\"entropy\", None)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHsD-ae-hcCL",
        "outputId": "7f786a1f-609e-4b0d-e952-3702c7f3c107"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Body Size',\n",
              " 'Body Weight',\n",
              " 'Height',\n",
              " 'Bone Structure',\n",
              " 'Complexion',\n",
              " 'General feel of skin',\n",
              " 'Texture of Skin',\n",
              " 'Hair Color',\n",
              " 'Appearance of Hair',\n",
              " 'Shape of face',\n",
              " 'Eyes',\n",
              " 'Eyelashes',\n",
              " 'Blinking of Eyes',\n",
              " 'Cheeks',\n",
              " 'Nose',\n",
              " 'Teeth and gums',\n",
              " 'Lips',\n",
              " 'Nails',\n",
              " 'Appetite',\n",
              " 'Liking tastes',\n",
              " 'Dosha',\n",
              " 'Metabolism Type',\n",
              " 'Climate Preference',\n",
              " 'Stress Levels',\n",
              " 'Sleep Patterns',\n",
              " 'Dietary Habits',\n",
              " 'Physical Activity Level',\n",
              " 'Water Intake',\n",
              " 'Digestion Quality',\n",
              " 'Skin Sensitivity']"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "init_reference_index(\n",
        "    csv_path=\"/content/phase1_out/phase1_augmented.csv\",\n",
        "    phase1_dir=\"/content/phase1_out\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysgr8qDmkSUl"
      },
      "source": [
        "Phase 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fadGA3LVkU16"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, json\n",
        "import pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "from TreeBuilderV2 import TreeBuilderV2\n",
        "from TreeSnapshot import TreeSnapshot\n",
        "from tokenizer_and_embedding import TokenEmbedding, universal_tokenizer\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 1) Build Trait → Vata/Pitta/Kapha Weight Table\n",
        "# --------------------------------------------------\n",
        "def build_trait_vpk_table(csv_path, selected_cols):\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    counts = defaultdict(lambda: {\"vata\":0, \"pitta\":0, \"kapha\":0})\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        dosha_str = str(row[\"Dosha\"]).lower().strip()\n",
        "        # Identify dosha(s)\n",
        "        parts = dosha_str.replace(\" \", \"\").split(\"+\")\n",
        "        # T2 strategy: if tri-dosha → neutral start, traits decide later\n",
        "        if len(parts) == 3:\n",
        "            weight = {\"vata\":1/3, \"pitta\":1/3, \"kapha\":1/3}\n",
        "        else:\n",
        "            weight = {\n",
        "                \"vata\": 1.0 if \"vata\" in parts else 0.0,\n",
        "                \"pitta\":1.0 if \"pitta\" in parts else 0.0,\n",
        "                \"kapha\":1.0 if \"kapha\" in parts else 0.0\n",
        "            }\n",
        "\n",
        "        for c in selected_cols:\n",
        "            val = str(row[c]).strip()\n",
        "            tok = f\"{c}={val}\"\n",
        "            counts[tok][\"vata\"]  += weight[\"vata\"]\n",
        "            counts[tok][\"pitta\"] += weight[\"pitta\"]\n",
        "            counts[tok][\"kapha\"] += weight[\"kapha\"]\n",
        "\n",
        "    # Normalize each trait row → sum = 1\n",
        "    trait_vpk = {}\n",
        "    for tok, d in counts.items():\n",
        "        s = d[\"vata\"] + d[\"pitta\"] + d[\"kapha\"]\n",
        "        if s == 0:\n",
        "            trait_vpk[tok] = [1/3, 1/3, 1/3]  # uninformative trait => neutral\n",
        "        else:\n",
        "            trait_vpk[tok] = [d[\"vata\"]/s, d[\"pitta\"]/s, d[\"kapha\"]/s]\n",
        "\n",
        "    return trait_vpk\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 2) Build Constitution Snapshot for a User Query\n",
        "# --------------------------------------------------\n",
        "def compute_user_vpk(query, trait_vpk, dim=50, device=\"cpu\"):\n",
        "    tokens = universal_tokenizer(query)\n",
        "    if not tokens:\n",
        "        return None\n",
        "\n",
        "    vocab = tokens[:12]\n",
        "    emb = TokenEmbedding(vocab=vocab, dim=dim, device=device)\n",
        "    pairs = [(t, emb.lookup(t)) for t in vocab]\n",
        "\n",
        "    builder = TreeBuilderV2(mode=\"binary\", dim=dim, device=device)\n",
        "    root = builder.build_tree(pairs, sample_id=\"query\")\n",
        "    snap = TreeSnapshot(root).to_dict() if root else {}\n",
        "\n",
        "    # Collect trait vectors\n",
        "    vectors = []\n",
        "    for t in vocab:\n",
        "        if t in trait_vpk:\n",
        "            vectors.append(trait_vpk[t])\n",
        "    if not vectors:\n",
        "        vectors = [[1/3,1/3,1/3]]\n",
        "\n",
        "    # Mean pool → final constitution scores\n",
        "    v = sum(x[0] for x in vectors) / len(vectors)\n",
        "    p = sum(x[1] for x in vectors) / len(vectors)\n",
        "    k = sum(x[2] for x in vectors) / len(vectors)\n",
        "\n",
        "    # Confidence = dominance * (1 - balance)\n",
        "    dominant = max(v,p,k)\n",
        "    balance = abs(v-p) + abs(p-k) + abs(v-k)\n",
        "    confidence = dominant * (1 - 0.5*balance)\n",
        "\n",
        "    return {\n",
        "        \"vata\": round(v,3),\n",
        "        \"pitta\": round(p,3),\n",
        "        \"kapha\": round(k,3),\n",
        "        \"dominant\": [\"vata\",\"pitta\",\"kapha\"][ [v,p,k].index(max(v,p,k)) ],\n",
        "        \"confidence\": round(max(0.01, confidence), 3)\n",
        "    }\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 3) Top Driving Traits (for explanation)\n",
        "# --------------------------------------------------\n",
        "def top_driving_traits(query, trait_vpk, top_n=5):\n",
        "    tokens = universal_tokenizer(query)\n",
        "    scored = []\n",
        "    for t in tokens:\n",
        "        if t in trait_vpk:\n",
        "            v,p,k = trait_vpk[t]\n",
        "            score = max(v,p,k)\n",
        "            scored.append((t, score))\n",
        "    scored.sort(key=lambda x: x[1], reverse=True)\n",
        "    return scored[:top_n]\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Wrapper Function: Full constitution reasoning\n",
        "# --------------------------------------------------\n",
        "def analyze_user(query, csv_path, phase1_dir, dim=50, device=\"cpu\"):\n",
        "    selected_cols = json.load(open(os.path.join(phase1_dir, \"selected_features.json\")))\n",
        "    trait_vpk = build_trait_vpk_table(csv_path, selected_cols)\n",
        "    constitution = compute_user_vpk(query, trait_vpk, dim, device)\n",
        "    drivers = top_driving_traits(query, trait_vpk)\n",
        "    return {\n",
        "        \"constitution\": constitution,\n",
        "        \"top_driving_traits\": drivers\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg7JLqtAU4AF",
        "outputId": "262ca64d-dd3c-420a-ecf1-ce11ba3dad15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/trait_interpretation_map.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/trait_interpretation_map.py\n",
        "trait_interpretation_map = {\n",
        "    # Skin\n",
        "    \"dry skin\": \"texture of skin (dry, pigments and aging)\",\n",
        "    \"rough skin\": \"texture of skin (dry, pigments and aging)\",\n",
        "    \"oily skin\": \"general feel of skin (smooth and warm, oily t-zone)\",\n",
        "    \"sensitive skin\": \"skin sensitivity (sensitive)\",\n",
        "\n",
        "    # Sleep\n",
        "    \"light sleep\": \"sleep patterns (short)\",\n",
        "    \"difficulty sleeping\": \"sleep patterns (moderate)\",\n",
        "    \"deep sleep\": \"sleep patterns (long)\",\n",
        "\n",
        "    # Hunger\n",
        "    \"irregular hunger\": \"appetite (irregular, scanty)\",\n",
        "    \"low appetite\": \"appetite (slow but steady)\",\n",
        "    \"high appetite\": \"appetite (strong, unbearable)\",\n",
        "\n",
        "    # Body Temperature\n",
        "    \"body feels hot\": \"complexion (fair-skin sunburns easily)\",\n",
        "    \"heat in body\": \"complexion (fair-skin sunburns easily)\",\n",
        "    \"cold hands and feet\": \"general feel of skin (cold and dry)\",\n",
        "\n",
        "    # Weight / Body Size\n",
        "    \"slim\": \"slim body frame, difficulty gaining weight\",\n",
        "    \"skinny\": \"slim body frame, difficulty gaining weight\",\n",
        "    \"cannot gain weight\": \"body weight (low - difficulties in gaining weight)\",\n",
        "    \"overweight\": \"body weight (heavy - difficulties in losing weight)\",\n",
        "\n",
        "    # Breathing / Chest\n",
        "    \"breathing problem\": \"digestion quality (weak)\",  # Ayurvedic mapping: Prana imbalance → Vata\n",
        "    \"shortness of breath\": \"digestion quality (weak)\",\n",
        "    \"chest congestion\": \"body weight (heavy - difficulties in losing weight)\",  # Kapha cluster\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_PHwiZjVXkz"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def semantic_match(query, trait_list, threshold=0.43):\n",
        "    q_emb = model.encode(query, convert_to_tensor=True)\n",
        "    best = None\n",
        "    best_score = 0\n",
        "\n",
        "    for trait in trait_list:\n",
        "        t_emb = model.encode(trait, convert_to_tensor=True)\n",
        "        score = float(util.cos_sim(q_emb, t_emb))\n",
        "        if score > best_score:\n",
        "            best, best_score = trait, score\n",
        "\n",
        "    return best if best_score >= threshold else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_lb1PnFksgh"
      },
      "outputs": [],
      "source": [
        "\n",
        "from embedding_matching import retrieve\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from trait_interpretation_map import trait_interpretation_map\n",
        "\n",
        "def map_query_to_dataset_traits(query, top_k=7):\n",
        "    query = query.lower()\n",
        "    matched = []\n",
        "\n",
        "    # 1) direct phrase → canonical trait\n",
        "    for phrase, canonical in trait_interpretation_map.items():\n",
        "        if phrase in query:\n",
        "            matched.append(canonical)\n",
        "\n",
        "    # 2) semantic fallback for remaining words\n",
        "    tokens = [t.strip() for t in query.split() if t.strip()]\n",
        "    for token in tokens:\n",
        "        best = semantic_match(token, list(trait_vpk_table.keys()))\n",
        "        if best:\n",
        "            matched.append(best)\n",
        "\n",
        "    # remove duplicates\n",
        "    matched = list(dict.fromkeys(matched))\n",
        "    return matched[:top_k]\n",
        "def compute_user_vpk_mapped(query, trait_vpk, top_k=5):\n",
        "    matched_raw = map_query_to_dataset_traits(query, top_k=7)\n",
        "\n",
        "    # APPLY NORMALIZATION HERE\n",
        "    matched_traits = [normalize_trait_name(t) for t in matched_raw]\n",
        "    driving = []\n",
        "    vectors = []\n",
        "\n",
        "    for t in matched_traits:\n",
        "        if t in trait_vpk_table:\n",
        "            vpk = trait_vpk[t]\n",
        "\n",
        "            # Normalize to (v,p,k)\n",
        "            if isinstance(vpk, dict):\n",
        "                vpk = (float(vpk.get(\"vata\", 0)),\n",
        "                       float(vpk.get(\"pitta\", 0)),\n",
        "                       float(vpk.get(\"kapha\", 0)))\n",
        "            elif isinstance(vpk, (list, tuple)) and len(vpk) == 3:\n",
        "                vpk = (float(vpk[0]), float(vpk[1]), float(vpk[2]))\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            driving.append((normalize_trait_name(t), vpk))\n",
        "            vectors.append(vpk)\n",
        "\n",
        "    # If no match → still return constitution but empty driving_traits\n",
        "    if not vectors:\n",
        "        v, p, k = (1/3, 1/3, 1/3)\n",
        "        return (\n",
        "            {\"vata\": v, \"pitta\": p, \"kapha\": k, \"dominant\": \"vata\", \"confidence\": 0.3},\n",
        "            driving,\n",
        "            0.3\n",
        "        )\n",
        "\n",
        "    # Average V P K\n",
        "    v = sum(x[0] for x in vectors) / len(vectors)\n",
        "    p = sum(x[1] for x in vectors) / len(vectors)\n",
        "    k = sum(x[2] for x in vectors) / len(vectors)\n",
        "\n",
        "    dominant = [\"vata\",\"pitta\",\"kapha\"][[v,p,k].index(max(v,p,k))]\n",
        "    balance = abs(v-p) + abs(p-k) + abs(v-k)\n",
        "    confidence = (max(v,p,k)) * (1 - 0.5*balance)\n",
        "\n",
        "    return (\n",
        "        {\n",
        "            \"vata\": round(v,3),\n",
        "            \"pitta\": round(p,3),\n",
        "            \"kapha\": round(k,3),\n",
        "            \"dominant\": dominant,\n",
        "            \"confidence\": round(confidence,3)\n",
        "        },\n",
        "        driving,         # <- now contains the trait signals\n",
        "        round(confidence,3)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pazoa4BMlFsy"
      },
      "source": [
        "Phase 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhgHw0FJlHKd"
      },
      "outputs": [],
      "source": [
        "\n",
        "def explain_constitution(result):\n",
        "    c = result[\"constitution\"]\n",
        "    drivers = result[\"top_driving_traits\"]\n",
        "\n",
        "    dominant = c[\"dominant\"]\n",
        "    v, p, k = c[\"vata\"], c[\"pitta\"], c[\"kapha\"]\n",
        "    conf = c[\"confidence\"]\n",
        "\n",
        "    # Constitution label formatting\n",
        "    if c[\"vata\"] > c[\"pitta\"] > c[\"kapha\"]:\n",
        "        const_type = \"Vata-Pitta (Vata dominant)\"\n",
        "    elif c[\"pitta\"] > c[\"vata\"] > c[\"kapha\"]:\n",
        "        const_type = \"Pitta-Vata (Pitta dominant)\"\n",
        "    else:\n",
        "        const_type = f\"{dominant.capitalize()} dominant\"\n",
        "\n",
        "    text = f\"\"\"\n",
        "Your constitution is **{const_type}**.\n",
        "\n",
        "**Vata = {v}**, **Pitta = {p}**, **Kapha = {k}**\n",
        "Confidence Score = **{conf}**\n",
        "\n",
        "This suggests your system tends toward:\n",
        "\n",
        "• **Vata** → movement, dryness, creativity, speed\n",
        "• **Pitta** → metabolic intensity, sharpness, heat\n",
        "• **Kapha** → stability, lubrication (low in your case)\n",
        "\n",
        "**Top contributing traits (signals used from your input):**\n",
        "\"\"\"\n",
        "\n",
        "    for trait, score in drivers:\n",
        "        text += f\"• {trait}  (signal strength: {round(score,3)})\\n\"\n",
        "\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojVWAPvQmZfl"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_final_response(query, csv_path, phase1_dir):\n",
        "    result = analyze_user(query, csv_path, phase1_dir)\n",
        "    text = explain_constitution(result)\n",
        "    rec = recommend_for(result)\n",
        "\n",
        "    final = text + \"\\n\\n**Recommended Diet:**\\n\"\n",
        "    final += \"\\n\".join(f\"• {d}\" for d in rec[\"diet\"])\n",
        "    final += \"\\n\\n**Lifestyle Guidance:**\\n\"\n",
        "    final += \"\\n\".join(f\"• {l}\" for l in rec[\"lifestyle\"])\n",
        "\n",
        "    return final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWi3JL5vnqd0"
      },
      "outputs": [],
      "source": [
        "def extract_top_driving_traits(matched_traits, trait_vpk_table, dominant, top_n=8):\n",
        "    scored = []\n",
        "    for t in matched_traits:\n",
        "        if t in trait_vpk_table:\n",
        "            v, p, k = trait_vpk_table[t]\n",
        "            if dominant == \"vata\":\n",
        "                score = v - max(p, k)\n",
        "            elif dominant == \"pitta\":\n",
        "                score = p - max(v, k)\n",
        "            else:\n",
        "                score = k - max(v, p)\n",
        "            scored.append((t, score))\n",
        "\n",
        "    scored = sorted(scored, key=lambda x: x[1], reverse=True)\n",
        "    return scored[:top_n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dreFjWPSGZOS"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/Updated_Prakriti_With_Features.csv\")\n",
        "\n",
        "# Convert to mapping (example logic - matches your original pipeline)\n",
        "trait_vpk_table = {}\n",
        "for col in df.columns:\n",
        "    if col not in [\"Vata\", \"Pitta\", \"Kapha\"]:  # ignore target columns\n",
        "        trait_vpk_table[col] = {\n",
        "            \"vata\": df[col].str.contains(\"vata\", case=False, na=False).mean(),\n",
        "            \"pitta\": df[col].str.contains(\"pitta\", case=False, na=False).mean(),\n",
        "            \"kapha\": df[col].str.contains(\"kapha\", case=False, na=False).mean(),\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifxq_fBS17pg"
      },
      "source": [
        "Phase 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTaTn9mp167m"
      },
      "outputs": [],
      "source": [
        "\n",
        "def explain_doctor_style(constitution, driving_traits):\n",
        "    # Determine dominant dosha based on actual score values\n",
        "    scores = {\n",
        "        \"Vata\": constitution[\"vata\"],\n",
        "        \"Pitta\": constitution[\"pitta\"],\n",
        "        \"Kapha\": constitution[\"kapha\"]\n",
        "    }\n",
        "    dominant = max(scores, key=scores.get)\n",
        "\n",
        "    text = []\n",
        "    text.append(f\"Based on your reported characteristics, your constitution is predominantly **{dominant}**, with the following proportions:\")\n",
        "    text.append(f\"• Vata = {constitution['vata']:.3f}\")\n",
        "    text.append(f\"• Pitta = {constitution['pitta']:.3f}\")\n",
        "    text.append(f\"• Kapha = {constitution['kapha']:.3f}\")\n",
        "    text.append(f\"Confidence Score: {constitution['confidence']:.3f}\\n\")\n",
        "\n",
        "    text.append(\"This suggests the following physiological tendencies:\")\n",
        "    text.append(\"• **Vata** – movement, nerve activity, dryness, variability\")\n",
        "    text.append(\"• **Pitta** – metabolic intensity, heat generation, digestion\")\n",
        "    text.append(\"• **Kapha** – structure, lubrication, metabolic steadiness\\n\")\n",
        "\n",
        "    text.append(\"Key traits from your input influencing this determination:\")\n",
        "    for trait, weight in driving_traits:\n",
        "        text.append(f\"• {trait} (signal weight: {weight:.3f})\")\n",
        "\n",
        "    return \"\\n\".join(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEeYG5hVSA6m"
      },
      "outputs": [],
      "source": [
        "\n",
        "def normalize_trait_name(trait):\n",
        "    trait = trait.strip().lower()\n",
        "\n",
        "    # Convert \"Key=Value\" → (key, value)\n",
        "    if \"=\" in trait:\n",
        "        key, value = trait.split(\"=\", 1)\n",
        "        key = key.strip().lower()\n",
        "        value = value.strip().lower()\n",
        "\n",
        "        # Mapping rules (add more slowly over time, not all at once)\n",
        "        if key.startswith(\"texture of skin\"):\n",
        "            return \"dry skin\" if \"dry\" in value else f\"skin texture ({value})\"\n",
        "\n",
        "        if key.startswith(\"complexion\"):\n",
        "            return f\"complexion ({value})\"\n",
        "\n",
        "        if key.startswith(\"sleep patterns\"):\n",
        "            if \"short\" in value:\n",
        "                return \"light sleep\"\n",
        "            if \"long\" in value:\n",
        "                return \"heavy sleep\"\n",
        "            return \"balanced sleep\"\n",
        "\n",
        "    return trait  # fallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VKqnPtA5PqR"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_anchor_snapshot(root):\n",
        "    \"\"\"\n",
        "    Convert a TreeNodeV1 tree into the node metadata structure\n",
        "    expected by anchor_extractor.\n",
        "    \"\"\"\n",
        "    nodes = {}\n",
        "\n",
        "    def dfs(node, depth):\n",
        "        if node is None:\n",
        "            return\n",
        "        # Unique node id (if not present, use memory address)\n",
        "        nid = getattr(node, \"id\", id(node))\n",
        "\n",
        "        # Collect metadata\n",
        "        nodes[nid] = {\n",
        "            \"token\": node.value,           # trait token like \"dry\" or \"body=slim\"\n",
        "            \"depth\": depth,\n",
        "            \"child_count\": len(node.children),\n",
        "            \"is_leaf\": (len(node.children) == 0),\n",
        "            \"vector\": node.vector.tolist() if hasattr(node, \"vector\") else None,\n",
        "        }\n",
        "\n",
        "        for child in node.children:\n",
        "            dfs(child, depth+1)\n",
        "\n",
        "    dfs(root, depth=0)\n",
        "    return nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8hXdFh75TIu"
      },
      "outputs": [],
      "source": [
        "\n",
        "from anchor_extractor import extract_anchors\n",
        "\n",
        "nodes = build_anchor_snapshot(root)\n",
        "anchors = extract_anchors(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhaBbcpS5bEo"
      },
      "outputs": [],
      "source": [
        "def compute_final_confidence(retrievals, constitution, w_ret=0.5, w_const=0.5):\n",
        "    if retrievals:\n",
        "        ret_conf = sum(r[\"retrievalconfidence\"] for r in retrievals) / len(retrievals)\n",
        "    else:\n",
        "        ret_conf = 0.0\n",
        "\n",
        "    const_conf = constitution[\"confidence\"]\n",
        "    return (w_ret * ret_conf) + (w_const * const_conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-q4freB5bN3"
      },
      "outputs": [],
      "source": [
        "\n",
        "from Phase2Env import Phase2Env\n",
        "\n",
        "def rl_refine_tree(root, max_steps=3):\n",
        "    env = Phase2Env(root)\n",
        "    for _ in range(max_steps):\n",
        "        env.step()   # This applies safe structural adjustments\n",
        "        if env.is_stable():  # if entropy stopped improving, exit early\n",
        "            break\n",
        "    return env.tree   # return the improved tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oo5kAUW05bWa"
      },
      "outputs": [],
      "source": [
        "\n",
        "def process_query(query):\n",
        "    # Step 1: Tokenize → Embed → Tree\n",
        "    tokens = universal_tokenizer(query)\n",
        "    pairs = [(t, emb.lookup(t)) for t in tokens[:12]]\n",
        "    root = TreeBuilderV2(mode=\"binary\").build_tree(pairs, sample_id=\"user\")\n",
        "\n",
        "    # Step 2: Anchors\n",
        "    anchors = extract_anchors(build_anchor_snapshot(root))\n",
        "\n",
        "    # Step 3: Retrieval\n",
        "    retrievals = retrieve(query, k=5)\n",
        "\n",
        "    # Step 4: Constitution scoring\n",
        "    constitution = compute_user_vpk_mapped(query, trait_vpk_table)\n",
        "\n",
        "    # Step 5: Confidence combine\n",
        "    conf = compute_final_confidence(retrievals, constitution)\n",
        "\n",
        "    # Step 6: High / Low decision\n",
        "    if conf >= 0.45:     # threshold adjustable\n",
        "        return generate_explanation(root, anchors, constitution, friendly=False), \\\n",
        "               generate_explanation(root, anchors, constitution, friendly=True)\n",
        "    else:\n",
        "        refined = rl_refine_tree(root)   # RL correction\n",
        "        return generate_explanation(refined, anchors, constitution, friendly=False), \\\n",
        "               generate_explanation(refined, anchors, constitution, friendly=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lltm7UIr7Twk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# phase_live_runtime.py\n",
        "\n",
        "from tokenizer_and_embedding import TokenEmbedding, universal_tokenizer\n",
        "from TreeBuilderV2 import TreeBuilderV2\n",
        "from anchor_extractor import extract_anchors\n",
        "from embedding_matching import retrieve\n",
        "from smoother import smooth_text\n",
        "from DecoderV1 import DecoderV1\n",
        "\n",
        "\n",
        "def build_anchor_snapshot(root):\n",
        "    nodes = {}\n",
        "\n",
        "    def dfs(node, d):\n",
        "        nid = id(node)\n",
        "        nodes[nid] = {\n",
        "            \"depth\": d,\n",
        "            \"token\": node.value,\n",
        "            \"children\": [id(c) for c in node.children],\n",
        "            \"is_leaf\": (len(node.children) == 0)\n",
        "        }\n",
        "        for c in node.children:\n",
        "            dfs(c, d+1)\n",
        "\n",
        "    dfs(root, 0)\n",
        "    return {\"root\": id(root), \"nodes\": nodes}\n",
        "\n",
        "\n",
        "def rl_lite_refine(root, threshold=0.30):\n",
        "    def prune(node):\n",
        "        keep = []\n",
        "        for c in node.children:\n",
        "            if len(c.children) == 0:\n",
        "                if c.get_confidence() >= threshold:\n",
        "                    keep.append(c)\n",
        "            else:\n",
        "                prune(c)\n",
        "                keep.append(c)\n",
        "        node.children = keep\n",
        "    prune(root)\n",
        "    return root\n",
        "\n",
        "\n",
        "def analyze_user_input(query):\n",
        "    # 1) Tokenize\n",
        "    tokens = universal_tokenizer(query)\n",
        "\n",
        "    # 2) Embeddings\n",
        "    emb = TokenEmbedding(vocab=tokens, dim=50)\n",
        "    vec_pairs = [(t, emb.lookup(t)) for t in tokens[:12]]\n",
        "\n",
        "    # 3) Tree Build\n",
        "    root = TreeBuilderV2(mode=\"binary\").build_tree(vec_pairs, sample_id=\"live\")\n",
        "\n",
        "    # 4) Anchors\n",
        "    anchors = extract_anchors(build_anchor_snapshot(root))\n",
        "\n",
        "    # 5) Retrieve reference samples\n",
        "    retrieved = retrieve(query, k=5)\n",
        "\n",
        "    # 6) Constitution calculation (calls your in-memory function)\n",
        "    constitution = compute_user_vpk_mapped(query, trait_vpk_table)\n",
        "\n",
        "    # 7) Driving traits\n",
        "    driving = extract_top_driving_traits(\n",
        "        constitution[\"matched_traits\"],\n",
        "        trait_vpk_table,\n",
        "        constitution[\"dominant\"]\n",
        "    )\n",
        "\n",
        "    # 8) Confidence fusion\n",
        "    if retrieved:\n",
        "        ret_conf = sum(r[\"retrievalconfidence\"] for r in retrieved) / len(retrieved)\n",
        "    else:\n",
        "        ret_conf = 0.0\n",
        "    confidence = 0.5 * ret_conf + 0.5 * constitution[\"confidence\"]\n",
        "\n",
        "    # 9) RL-lite refine if weak confidence\n",
        "    if confidence < 0.45:\n",
        "        root = rl_lite_refine(root)\n",
        "\n",
        "    # 10) Convert Tree → Natural Explanation\n",
        "    structure = DecoderV1(dim=48, device=\"cpu\", use_smoother=True).explain(root)\n",
        "    structure = smooth_text(structure)\n",
        "\n",
        "    # 11) Explanation (Doctor style)\n",
        "    doctor = explain_doctor_style(constitution, driving)\n",
        "\n",
        "    # (Optional) Friendly style available too:\n",
        "    friendly = explain_friendly_style(constitution, driving)\n",
        "\n",
        "    # 12) Diet + Lifestyle\n",
        "    rec = recommend_for({\"constitution\": constitution})\n",
        "\n",
        "    return {\n",
        "        \"confidence\": confidence,\n",
        "        \"structure\": structure,\n",
        "        \"anchors\": anchors,\n",
        "        \"doctor\": doctor,\n",
        "        \"friendly\": friendly,\n",
        "        \"diet\": rec[\"diet\"],\n",
        "        \"lifestyle\": rec[\"lifestyle\"]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0pUnQkRJC-3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_dominant_ordered_traits(constitution):\n",
        "    dom = constitution[\"dominant\"]  # \"vata\" / \"pitta\" / \"kapha\"\n",
        "    traits = constitution[\"matched_traits\"]  # this is a list, but shape may vary\n",
        "\n",
        "    scored = []\n",
        "    for entry in traits:\n",
        "        # Case 1: (trait, weights)\n",
        "        if len(entry) == 2:\n",
        "            trait, weights = entry\n",
        "        # Case 2: (trait, weights, extra_value)\n",
        "        elif len(entry) == 3:\n",
        "            trait, weights, _ = entry\n",
        "        # Otherwise skip invalid entry\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        # weights can be dict or list\n",
        "        if isinstance(weights, dict):\n",
        "            score = weights.get(dom, 0.0)\n",
        "        elif isinstance(weights, (list, tuple)) and len(weights) == 3:\n",
        "            idx = {\"vata\": 0, \"pitta\": 1, \"kapha\": 2}[dom]\n",
        "            score = weights[idx]\n",
        "        else:\n",
        "            score = float(weights) if isinstance(weights, (int, float)) else 0.0\n",
        "\n",
        "        scored.append((trait, score))\n",
        "\n",
        "    scored.sort(key=lambda x: x[1], reverse=True)\n",
        "    return scored[:8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqcPx17OCd8t"
      },
      "outputs": [],
      "source": [
        "\n",
        "def adjust_pitta_boost(constitution, text):\n",
        "    text = text.lower()\n",
        "    pitta_boost = 0.0\n",
        "\n",
        "    # Warm body / heat signals\n",
        "    if \"warm\" in text or \"heat\" in text or \"hot\" in text:\n",
        "        pitta_boost += 0.08\n",
        "\n",
        "    # Irritation / anger / intensity signals\n",
        "    if \"irrit\" in text or \"anger\" in text or \"frustrat\" in text:\n",
        "        pitta_boost += 0.10\n",
        "\n",
        "    # Apply boost\n",
        "    constitution[\"pitta\"] += pitta_boost\n",
        "    constitution[\"vata\"] -= pitta_boost * 0.5  # reduce Vata slightly to re-balance\n",
        "\n",
        "    # Re-normalize so total = 1.0\n",
        "    total = constitution[\"vata\"] + constitution[\"pitta\"] + constitution[\"kapha\"]\n",
        "    constitution[\"vata\"] /= total\n",
        "    constitution[\"pitta\"] /= total\n",
        "    constitution[\"kapha\"] /= total\n",
        "\n",
        "    return constitution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WJ0n5-XLV68"
      },
      "outputs": [],
      "source": [
        "\n",
        "def _normalize_driving_traits(driving_traits, constitution, max_items=8):\n",
        "    \"\"\"\n",
        "    driving_traits can be:\n",
        "      - [(\"trait\", 0.324), ...]                       # score as float\n",
        "      - [(\"trait\", {\"vata\":0.5,\"pitta\":0.2,\"kapha\":0.3}), ...]\n",
        "      - [(\"trait\", [v, p, k]), ...]\n",
        "      - [\"trait1\", \"trait2\", ...]                      # no scores\n",
        "    constitution needs: {\"dominant\": \"vata\"|\"pitta\"|\"kapha\"}\n",
        "    \"\"\"\n",
        "    dom = (constitution.get(\"dominant\") or \"vata\").lower()\n",
        "    dom_idx = {\"vata\": 0, \"pitta\": 1, \"kapha\": 2}[dom]\n",
        "\n",
        "    normalized = []\n",
        "    for item in (driving_traits or []):\n",
        "        # Case 1: plain string trait\n",
        "        if isinstance(item, str):\n",
        "            normalized.append((item, 0.0))\n",
        "            continue\n",
        "\n",
        "        # Case 2: tuple/list like (trait, weights)\n",
        "        if isinstance(item, (list, tuple)) and len(item) >= 1:\n",
        "            trait = str(item[0])\n",
        "\n",
        "            score = 0.0\n",
        "            if len(item) >= 2:\n",
        "                weights = item[1]\n",
        "                # float score\n",
        "                if isinstance(weights, (int, float)):\n",
        "                    score = float(weights)\n",
        "                # dict per dosha\n",
        "                elif isinstance(weights, dict):\n",
        "                    score = float(weights.get(dom, 0.0))\n",
        "                # list/tuple [v,p,k]\n",
        "                elif isinstance(weights, (list, tuple)) and len(weights) >= 3:\n",
        "                    try:\n",
        "                        score = float(weights[dom_idx])\n",
        "                    except Exception:\n",
        "                        score = 0.0\n",
        "\n",
        "            normalized.append((trait, score))\n",
        "            continue\n",
        "\n",
        "        # Fallback: unknown shape → stringify\n",
        "        normalized.append((str(item), 0.0))\n",
        "\n",
        "    # sort by score desc, keep top\n",
        "    normalized.sort(key=lambda x: x[1], reverse=True)\n",
        "    return normalized[:max_items]\n",
        "\n",
        "\n",
        "def explain_friendly_style(constitution, driving_traits, max_items=8):\n",
        "    dom = constitution[\"dominant\"].capitalize()\n",
        "    messages = {\n",
        "        \"Vata\":  \"Your energy tends to move quickly — creativity, expressiveness, fast thinking. \"\n",
        "                 \"If balance slips, dryness, irregular digestion, or light sleep can show up.\",\n",
        "        \"Pitta\": \"Your metabolism and mind are strong — intensity, focus, decisiveness. \"\n",
        "                 \"If balance slips, heat, irritability, or overwork can show up.\",\n",
        "        \"Kapha\": \"You are steady and grounded — calm, patient, resilient. \"\n",
        "                 \"If balance slips, heaviness, sluggishness, or low motivation can show up.\",\n",
        "    }\n",
        "\n",
        "    ordered = _normalize_driving_traits(driving_traits, constitution, max_items=max_items)\n",
        "    ordered = merge_sleep_traits(ordered)  # <--- ADD THIS\n",
        "    trait_lines = format_trait_list(ordered)\n",
        "\n",
        "    text = (\n",
        "        f\"You show a **{dom}-dominant** constitution.\\n\"\n",
        "        f\"{messages.get(dom, '')}\\n\\n\"\n",
        "        \"Things your body signals clearly:\\n\\n\"\n",
        "        f\"{trait_lines if trait_lines else '(no strong signals detected)'}\\n\\n\"\n",
        "        \"Your body works best when balance is maintained. I’ll guide your diet and daily rhythm next.\"\n",
        "    )\n",
        "    return text\n",
        "\n",
        "\n",
        "def recommend_for(constitution):\n",
        "    dom = constitution[\"dominant\"].lower()\n",
        "\n",
        "    if dom == \"vata\":\n",
        "        diet = [\n",
        "            \"Warm cooked meals (soups, stews, kichadi)\",\n",
        "            \"Healthy fats like ghee, coconut, olive oil\",\n",
        "            \"Avoid cold salads, dry snacks and skipping meals\"\n",
        "        ]\n",
        "        lifestyle = [\n",
        "            \"Sleep before 10:30 PM\",\n",
        "            \"Keep a consistent routine\",\n",
        "            \"Gentle yoga / stretching; avoid high intensity late evening\"\n",
        "        ]\n",
        "\n",
        "    elif dom == \"pitta\":\n",
        "        diet = [\n",
        "            \"Cooling foods like cucumber, coconut water, sweet fruits\",\n",
        "            \"Avoid excessive spicy, sour, or fermented foods\"\n",
        "        ]\n",
        "        lifestyle = [\n",
        "            \"Avoid late-night work (adds heat)\",\n",
        "            \"Practice slow breathing & meditation daily\"\n",
        "        ]\n",
        "\n",
        "    else:  # kapha\n",
        "        diet = [\n",
        "            \"Light warm meals; reduce heavy dairy and fried foods\",\n",
        "            \"Use ginger and black pepper to stimulate digestion\"\n",
        "        ]\n",
        "        lifestyle = [\n",
        "            \"Regular morning physical activity\",\n",
        "            \"Avoid oversleeping and daytime naps\"\n",
        "        ]\n",
        "\n",
        "    return diet, lifestyle   # <-- THIS part is MOST IMPORTANT\n",
        "\n",
        "\n",
        "# ---------- Minimal glue you can call right now ----------\n",
        "# Supply your already-computed objects:\n",
        "#   constitution = {\"vata\":..., \"pitta\":..., \"kapha\":..., \"dominant\":\"pitta\", \"confidence\":...}\n",
        "#   driving_traits = [(\"Dosha=Vata\", 1.0), (\"Appetite=Irregular, Scanty\", 0.267), ...]  # any of the accepted shapes\n",
        "def make_friendly_output(constitution, driving_traits):\n",
        "    text = explain_friendly_style(constitution, driving_traits)   # <- here\n",
        "    diet, lifestyle = recommend_for(constitution)\n",
        "    return text, diet, lifestyle\n",
        "\n",
        "# Example (replace with your real objects):\n",
        "#text, diet, life = make_friendly_output(constitution, driving_traits)\n",
        "#print(text); print(\"\\nDIET:\", diet); print(\"\\nLIFESTYLE:\", life)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZIyqqp-UFXu"
      },
      "outputs": [],
      "source": [
        "\n",
        "def finalize_constitution(constitution):\n",
        "    # Recalculate dominant dosha based on final corrected scores\n",
        "    scores = {\n",
        "        \"vata\": constitution[\"vata\"],\n",
        "        \"pitta\": constitution[\"pitta\"],\n",
        "        \"kapha\": constitution[\"kapha\"]\n",
        "    }\n",
        "    constitution[\"dominant\"] = max(scores, key=scores.get)\n",
        "    return constitution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzyhZnE4DyV5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ---- robust reorder (handles float, dict, or [v,p,k] tuples) ----\n",
        "def reorder_traits_by_dominance(driving_traits, dominant):\n",
        "    idx_map = {\"vata\": 0, \"pitta\": 1, \"kapha\": 2}\n",
        "    ordered = []\n",
        "    for trait, weights in driving_traits:\n",
        "        if isinstance(weights, (list, tuple)) and len(weights) == 3:\n",
        "            score = float(weights[idx_map[dominant]])\n",
        "        elif isinstance(weights, dict):\n",
        "            score = float(weights.get(dominant, 0.0))\n",
        "        else:  # already a scalar score\n",
        "            score = float(weights)\n",
        "        ordered.append((trait, score))\n",
        "    ordered.sort(key=lambda x: x[1], reverse=True)\n",
        "    return ordered\n",
        "\n",
        "# ---- quick test runner (friendly style = 2) ----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfB0pp9yECrG"
      },
      "outputs": [],
      "source": [
        "# Build a usable root tree from trait_vpk_table keys (phase-live only)\n",
        "def build_root_from_traits(trait_vpk_table, dim=48, device=\"cpu\"):\n",
        "    # 1) Prepare vocab and embedding table\n",
        "    tokens = list(trait_vpk_table.keys())\n",
        "    emb = TokenEmbedding(tokens, dim=dim, device=device)\n",
        "\n",
        "    # 2) Make proper (token, vector) pairs (each vector must be a 1D torch.Tensor of length=dim)\n",
        "    import torch\n",
        "    vec_pairs = []\n",
        "    for t in tokens:\n",
        "        v = emb.lookup(t)                  # shape: [dim]\n",
        "        if isinstance(v, torch.Tensor) and v.dim() == 1 and v.shape[0] == dim:\n",
        "            vec_pairs.append((t, v))\n",
        "    # 3) Build a small binary tree\n",
        "    builder = TreeBuilderV2(device=device, dim=dim, mode=\"binary\")\n",
        "    return builder.build_tree(vec_pairs, sample_id=\"prakriti_root\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Xx016tHNm79"
      },
      "outputs": [],
      "source": [
        "\n",
        "def deduplicate_trait_values(driving_traits):\n",
        "    \"\"\"\n",
        "    driving_traits = [(trait_string, score), ...]\n",
        "    Keep only highest scoring trait per category.\n",
        "    \"\"\"\n",
        "    best = {}\n",
        "    for trait, score in driving_traits:\n",
        "        if \"=\" in trait:\n",
        "            category = trait.split(\"=\")[0].strip()\n",
        "        else:\n",
        "            category = trait\n",
        "\n",
        "        if category not in best or score > best[category][1]:\n",
        "            best[category] = (trait, score)\n",
        "\n",
        "    return list(best.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqONJXuZU-H2"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pickle\n",
        "\n",
        "CONFIDENCE_THRESHOLD = 0.45\n",
        "TREE_SAVE_PATH = \"chaturya_tree.pkl\"\n",
        "\n",
        "def load_tree():\n",
        "    try:\n",
        "        with open(TREE_SAVE_PATH, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def save_tree(root):\n",
        "    try:\n",
        "        with open(TREE_SAVE_PATH, \"wb\") as f:\n",
        "            pickle.dump(root, f)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# ---- RL \"repair\" stub uses your Phase2Env safely if present, else no-op ----\n",
        "def rl_generative_repair(root, query):\n",
        "    try:\n",
        "        # Your Phase2Env expects (builder, data, feature_vectors, ...) in the full pipeline.\n",
        "        # For phase-live console we keep this harmless & local.\n",
        "        return root  # no-op in phase-live; your RL path stays intact elsewhere\n",
        "    except Exception:\n",
        "        return root\n",
        "\n",
        "# analyze_user_input(query) must exist in memory (your phase-live cell).\n",
        "# It should return (constitution, driving_traits, retrieval_score).\n",
        "# DecoderV1 → TreeEncoderWithAttention now works with num_heads=5. 3\n",
        "\n",
        "def chatbot_answer(query):\n",
        "    constitution, driving_traits, retrieval_score = compute_user_vpk_mapped(query, trait_vpk_table)\n",
        "\n",
        "    # NEW CHECK: If matched traits are empty → ask user to describe traits\n",
        "    if not constitution.get(\"matched_traits\"):\n",
        "        return (\n",
        "            \"I didn’t detect any physical or lifestyle traits in your input.\\n\"\n",
        "            \"Please describe what you experience physically.\\n\"\n",
        "            \"Example: 'dry skin', 'oily hair', 'irregular hunger', 'light sleep', 'heat in body'.\",\n",
        "            [], [], None\n",
        "        )\n",
        "\n",
        "    driving_traits = deduplicate_trait_values(driving_traits)\n",
        "    text, diet, lifestyle = make_friendly_output(constitution, driving_traits)\n",
        "\n",
        "    return text, diet, lifestyle, None\n",
        "\n",
        "def init_or_load_root(trait_vpk_table):\n",
        "    root = load_tree()\n",
        "    if root is not None:\n",
        "        return root\n",
        "    print(\"No saved tree found → building tree from embeddings...\")\n",
        "    root = build_root_from_traits(trait_vpk_table, dim=50, device=\"cpu\")\n",
        "    save_tree(root)\n",
        "    return root\n",
        "\n",
        "def start_console_chatbot(root):\n",
        "    print(\"\\n=== Chaturya Ayurvedic Chatbot ===\")\n",
        "    print(\"Type your symptoms or traits. Type 'exit' to stop.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user = input(\"You: \").strip()\n",
        "        if user.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "            print(\"Chaturya: Wishing you balance and well-being.\")\n",
        "            break\n",
        "\n",
        "        text, diet, lifestyle, new_root = chatbot_answer(user)  # <-- FIXED (remove root)\n",
        "\n",
        "        # If no traits detected\n",
        "        if diet == [] and lifestyle == []:\n",
        "            print(\"\\nChaturya:\", text, \"\\n\")\n",
        "            continue\n",
        "\n",
        "        print(\"\\n--- RESULT ---\\n\")\n",
        "        print(text)\n",
        "\n",
        "        print(\"\\nDIET RECOMMENDATIONS:\")\n",
        "        for d in diet:\n",
        "            print(\"• \" + d)\n",
        "\n",
        "        print(\"\\nLIFESTYLE GUIDANCE:\")\n",
        "        for l in lifestyle:\n",
        "            print(\"• \" + l)\n",
        "\n",
        "        print(\"\\n----------------\\n\")\n",
        "\n",
        "        # RL update placeholder (still safe to leave)\n",
        "        if new_root is not None:\n",
        "            root = new_root\n",
        "\n",
        "    save_tree(root)\n",
        "\n",
        "#start_console_chatbot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-o5fFy12Vwk1"
      },
      "outputs": [],
      "source": [
        "\n",
        "def interpret_trait(trait_name):\n",
        "    interpretations = {\n",
        "        \"texture of skin (dry, pigments and aging)\":\n",
        "            \"Dry or easily dehydrated skin suggests elevated Vata regulating moisture in the body.\",\n",
        "        \"sleep patterns (short)\":\n",
        "            \"Your sleep is light and easily disturbed — a key Vata characteristic.\",\n",
        "        \"sleep patterns (moderate)\":\n",
        "            \"Your sleep is generally balanced but affected by stress or irregular routine.\",\n",
        "        \"sleep patterns (long)\":\n",
        "            \"Deep and heavy sleep suggests Kapha grounding influence.\",\n",
        "    }\n",
        "    return interpretations.get(trait_name.lower(), None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI8qMHGHWNgp"
      },
      "outputs": [],
      "source": [
        "\n",
        "def merge_sleep_traits(traits):\n",
        "    sleep_signals = [t for t in traits if \"sleep\" in t[0].lower()]\n",
        "    if not sleep_signals:\n",
        "        return traits\n",
        "\n",
        "    # pick the strongest sleep signal\n",
        "    sleep_signals.sort(key=lambda x: x[1], reverse=True)\n",
        "    best = sleep_signals[0]\n",
        "\n",
        "    interpreted = (\"Your sleep tends to be light and easily disturbed — \"\n",
        "                   \"a classic sign of Vata affecting the nervous system.\")\n",
        "\n",
        "    # remove all sleep-related entries from list\n",
        "    traits = [t for t in traits if \"sleep\" not in t[0].lower()]\n",
        "\n",
        "    # add interpreted summary back with strongest score\n",
        "    traits.insert(0, (interpreted, best[1]))\n",
        "\n",
        "    return traits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZHZAkvhVwwT"
      },
      "outputs": [],
      "source": [
        "\n",
        "def format_trait_list(driving_traits):\n",
        "    lines = []\n",
        "    for trait, score in driving_traits:\n",
        "        interpretation = interpret_trait(trait.lower())\n",
        "        if interpretation:\n",
        "            lines.append(f\"• {interpretation} (signal strength: {score:.3f})\")\n",
        "        else:\n",
        "            lines.append(f\"• {trait} (signal strength: {score:.3f})\")\n",
        "    return \"\\n\".join(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "FJR9ykvUQwW9",
        "outputId": "5bd268d6-9606-4f63-bd1c-9935d1c4d318"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32e77b7bd26d46039a86eafb64bf0db5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a80c9d2c20845dcbcfdc11d9a3d943e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f0f601795a2435b9f1182a6eea4fffb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0044eee3791447c1b6391b90e32e759b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94fda19a6c344c9e8a297ede7d281b79",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9faaba1a198349e19cf7c3e6a8deb399",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3406501c7ac04797a1d67e9d1cc2f392",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be093bd8d1d84802a02370fa3d492768",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62dada02f48f49238d16bb62f645b796",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb763d9a0e8944d5a707c4e9cd2ed00e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b5a6e308c7648ad9133b18ffc446f04",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "model_embed = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "trait_texts = list(trait_vpk_table.keys())\n",
        "trait_vectors = model_embed.encode(trait_texts, normalize_embeddings=True)\n",
        "\n",
        "# Store for fast lookup\n",
        "trait_index = list(zip(trait_texts, trait_vectors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2hTyLnoMKlx"
      },
      "outputs": [],
      "source": [
        "trait_synonyms = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hr3tnw4qNO_k"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/Updated_Prakriti_With_Features.csv\")\n",
        "\n",
        "trait_vpk_table = {}  # new corrected table\n",
        "\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == object:  # only categorical columns\n",
        "        values = df[col].dropna().unique()\n",
        "        for v in values:\n",
        "            key = f\"{col}={v}\".strip()\n",
        "            # initialize placeholder VPK, will fill next step\n",
        "            trait_vpk_table[key] = {\"vata\":0, \"pitta\":0, \"kapha\":0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SfXyhaJNPJO"
      },
      "outputs": [],
      "source": [
        "\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == object:\n",
        "        grouped = df.groupby(col)[\"Dosha\"]  # Uses your labeled Prakriti column\n",
        "        for value, subset in grouped:\n",
        "            key = f\"{col}={value}\".strip()\n",
        "            counts = subset.value_counts(normalize=True).to_dict()\n",
        "            trait_vpk_table[key] = {\n",
        "                \"vata\":  counts.get(\"Vata\", 0.0),\n",
        "                \"pitta\": counts.get(\"Pitta\", 0.0),\n",
        "                \"kapha\": counts.get(\"Kapha\", 0.0)\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHTG2wkVNPRJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import json\n",
        "\n",
        "with open(\"trait_vpk_table.json\", \"w\") as f:\n",
        "    json.dump(trait_vpk_table, f, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BBEwJYuNPZm"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tokenizer_and_embedding import TokenEmbedding, build_trait_embedding_index\n",
        "\n",
        "embedder = TokenEmbedding(list(trait_vpk_table.keys()), dim=48, device=\"cpu\")\n",
        "trait_index = build_trait_embedding_index(trait_vpk_table, embedder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAbqaWdSIIxl"
      },
      "outputs": [],
      "source": [
        "\n",
        "normalized_trait_vpk = {}\n",
        "\n",
        "for raw, vpk in trait_vpk_table.items():\n",
        "    norm = normalize_trait_name(raw)\n",
        "    normalized_trait_vpk[norm] = vpk\n",
        "\n",
        "trait_vpk_table = normalized_trait_vpk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rC3DtcG7Q_pv",
        "outputId": "d9fe7f58-65c2-44f2-d3d9-53c161b8f598"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You show a **Vata-dominant** constitution.\n",
            "Your energy tends to move quickly — creativity, expressiveness, fast thinking. If balance slips, dryness, irregular digestion, or light sleep can show up.\n",
            "\n",
            "Things your body signals clearly:\n",
            "\n",
            "• Your sleep tends to be light and easily disturbed — a classic sign of Vata affecting the nervous system. (signal strength: 0.230)\n",
            "• Dry or easily dehydrated skin suggests elevated Vata regulating moisture in the body. (signal strength: 0.250)\n",
            "\n",
            "Your body works best when balance is maintained. I’ll guide your diet and daily rhythm next.\n",
            "['Warm cooked meals (soups, stews, kichadi)', 'Healthy fats like ghee, coconut, olive oil', 'Avoid cold salads, dry snacks and skipping meals']\n",
            "['Sleep before 10:30 PM', 'Keep a consistent routine', 'Gentle yoga / stretching; avoid high intensity late evening']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "query = \"dry skin light sleep\"\n",
        "constitution, driving, score = compute_user_vpk_mapped(query, trait_vpk_table)\n",
        "text, diet, lifestyle = make_friendly_output(constitution, driving)\n",
        "print(text)\n",
        "print(diet)\n",
        "print(lifestyle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUQF8zOKas_M"
      },
      "source": [
        "# Phase live 2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3dL2eTtawnO"
      },
      "outputs": [],
      "source": [
        "DIM = 48  # <- use 48 across TokenEmbedding, TreeBuilderV2, DecoderV1, TLite, etc.\n",
        "CONFIDENCE_THRESHOLD = 0.45"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6n2ih9hva4K8"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re, torch, torch.nn.functional as F\n",
        "\n",
        "def universal_tokenizer(text: str):\n",
        "    if not text: return []\n",
        "    return re.findall(r'\\d+\\.\\d+|\\d+|[A-Za-z]+|[+\\-*/^=():]', text.lower())\n",
        "\n",
        "class TokenEmbedding:\n",
        "    def __init__(self, vocab, dim=DIM, device='cpu'):\n",
        "        self.dim, self.device = dim, device\n",
        "        self.vocab = ['<unk>'] + list(vocab)\n",
        "        self.word2idx = {w:i for i,w in enumerate(self.vocab)}\n",
        "        self.emb = torch.randn(len(self.vocab), dim, device=device) / (dim**0.5)\n",
        "\n",
        "    def lookup(self, token:str) -> torch.Tensor:\n",
        "        return self.emb[self.word2idx.get(token, 0)]\n",
        "\n",
        "def build_trait_embedding_index(trait_names, embedder):\n",
        "    idx = {}\n",
        "    for name in trait_names:\n",
        "        toks = universal_tokenizer(name)\n",
        "        if not toks: continue\n",
        "        v = sum(embedder.lookup(t) for t in toks) / len(toks)\n",
        "        v = v / (v.norm() + 1e-9)\n",
        "        idx[name] = v\n",
        "    return idx\n",
        "\n",
        "def semantic_match_traits(query, trait_index, embedder, top_k=8):\n",
        "    toks = universal_tokenizer(query)\n",
        "    if not toks: return []\n",
        "    q = sum(embedder.lookup(t) for t in toks) / len(toks)\n",
        "    q = q / (q.norm() + 1e-9)\n",
        "    sims = []\n",
        "    for trait, vec in trait_index.items():\n",
        "        s = F.cosine_similarity(q.unsqueeze(0), vec.unsqueeze(0)).item()\n",
        "        sims.append((trait, s))\n",
        "    sims.sort(key=lambda x: x[1], reverse=True)\n",
        "    strong = [t for t,s in sims if s >= 0.72][:top_k]\n",
        "    med = [t for t,s in sims if 0.55 <= s < 0.72][:max(0, top_k-len(strong))]\n",
        "    return strong + med"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ma_jiXiva4-o"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_trait_catalog_from_dataset_columns(df_like_columns):\n",
        "    \"\"\"\n",
        "    Input: list of column names (exactly the 30 you've shown).\n",
        "    Output: a set of *canonical* trait phrases we expect, e.g.:\n",
        "        \"height (short)\", \"body size (slim)\", ...\n",
        "    If you also have value vocabularies per column, you can pass a dict instead.\n",
        "    For now we'll just accept already-canonicalized strings you built offline.\n",
        "    \"\"\"\n",
        "    # If you already have canonical keys (as in your 93-list), just return them.\n",
        "    # Here, we keep a hook if you later want to generate from unique values per column.\n",
        "    return set()\n",
        "\n",
        "def normalize_trait_key(k: str) -> str:\n",
        "    # unify case/spaces to match your 93-key style as in your sample\n",
        "    return k.strip().lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-iCaMrca5dM"
      },
      "outputs": [],
      "source": [
        "\n",
        "def _get_vpk_tuple(weights):\n",
        "    # weights can be dict {'vata':..,'pitta':..,'kapha':..} or list/tuple [v,p,k]\n",
        "    if isinstance(weights, dict):\n",
        "        return (float(weights.get('vata',0)), float(weights.get('pitta',0)), float(weights.get('kapha',0)))\n",
        "    if isinstance(weights, (list, tuple)) and len(weights) >= 3:\n",
        "        return (float(weights[0]), float(weights[1]), float(weights[2]))\n",
        "    # fallback single scalar: treat as vata\n",
        "    if isinstance(weights, (int, float)):\n",
        "        return (float(weights), 0.0, 0.0)\n",
        "    return (1/3, 1/3, 1/3)\n",
        "\n",
        "def compute_user_vpk_mapped(query, trait_vpk_table, trait_index, embedder, top_k=8):\n",
        "    \"\"\"\n",
        "    Returns: constitution(dict), driving_traits(list[(trait,score)]), retrieval_score(float)\n",
        "    - constitution has keys: vata, pitta, kapha, dominant, confidence, matched_traits\n",
        "    - driving_traits is ranked by dominant dosha's weight where available\n",
        "    \"\"\"\n",
        "    matched = semantic_match_traits(query, trait_index, embedder, top_k=top_k)\n",
        "    retrieval_score = 0.0\n",
        "    if matched:\n",
        "        # simple retrieval score: average of top-5 cosine ranks approximated by list length\n",
        "        retrieval_score = min(1.0, len(matched) / float(top_k))\n",
        "\n",
        "    vectors = []\n",
        "    driving_traits = []\n",
        "    for t in matched:\n",
        "        if t in trait_vpk_table:\n",
        "            w = trait_vpk_table[t]\n",
        "            v,p,k = _get_vpk_tuple(w)\n",
        "            vectors.append((v,p,k))\n",
        "            # for driving traits, store the *dominant* weight later\n",
        "            driving_traits.append((t, {'vata':v, 'pitta':p, 'kapha':k}))\n",
        "\n",
        "    if not vectors:\n",
        "        vectors = [(1/3, 1/3, 1/3)]\n",
        "\n",
        "    v = sum(x[0] for x in vectors) / len(vectors)\n",
        "    p = sum(x[1] for x in vectors) / len(vectors)\n",
        "    k = sum(x[2] for x in vectors) / len(vectors)\n",
        "\n",
        "    dom_name = ['vata','pitta','kapha'][[v,p,k].index(max(v,p,k))]\n",
        "    balance = abs(v-p) + abs(p-k) + abs(v-k)\n",
        "    confidence = max(0.0, min(1.0, max(v,p,k) * (1 - 0.5*balance)))\n",
        "\n",
        "    constitution = {\n",
        "        \"vata\": round(v,3), \"pitta\": round(p,3), \"kapha\": round(k,3),\n",
        "        \"dominant\": dom_name, \"confidence\": round(confidence,3),\n",
        "        \"matched_traits\": matched\n",
        "    }\n",
        "    return constitution, driving_traits, retrieval_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpFbR9RYcqUw"
      },
      "outputs": [],
      "source": [
        "\n",
        "trait_explanations = {\n",
        "    \"height (short)\": \"Shorter body frame is commonly seen in Vata-dominant constitutions.\",\n",
        "    \"height (average)\": \"Balanced height indicates neutral structural development.\",\n",
        "    \"height (tall)\": \"Taller body frames are more common in Kapha constitutions.\",\n",
        "    \"slim body frame, difficulty gaining weight\": \"Leanness and difficulty gaining weight strongly reflect elevated Vata.\",\n",
        "    \"large/heavy frame, gains weight easily\": \"Heaviness and easy weight gain are associated with Kapha.\",\n",
        "    \"texture of skin (dry, pigments and aging)\": \"Dry or easily dehydrated skin suggests elevated Vata affecting moisture.\",\n",
        "    \"complexion (fair-skin sunburns easily)\": \"Sensitive, heat-reactive skin suggests Pitta influence.\",\n",
        "    \"regular balanced sleep\": \"Your sleep rhythm is stable and balanced.\",\n",
        "    \"light sleep / wakes easily\": \"Light or easily disturbed sleep is a sign of Vata affecting the nervous system.\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyPQW34SbAOr"
      },
      "outputs": [],
      "source": [
        "\n",
        "def _normalize_driving_traits(driving_traits, constitution, max_items=8):\n",
        "    dom = (constitution.get(\"dominant\") or \"vata\").lower()\n",
        "    dom_idx = {\"vata\":0,\"pitta\":1,\"kapha\":2}[dom]\n",
        "\n",
        "    out = []\n",
        "    for item in driving_traits or []:\n",
        "        if isinstance(item, str):\n",
        "            out.append((item, 0.0))\n",
        "            continue\n",
        "\n",
        "        if isinstance(item, (list, tuple)) and len(item) >= 1:\n",
        "            t = str(item[0])\n",
        "            w = 0.0\n",
        "\n",
        "            if len(item) >= 2:\n",
        "                weights = item[1]\n",
        "                if isinstance(weights, (int, float)):\n",
        "                    w = float(weights)\n",
        "                elif isinstance(weights, dict):\n",
        "                    w = float(weights.get(dom, 0.0))\n",
        "                elif isinstance(weights, (list, tuple)) and len(weights) >= 3:\n",
        "                    try:\n",
        "                        w = float(weights[dom_idx])\n",
        "                    except:\n",
        "                        w = 0.0\n",
        "\n",
        "            out.append((t, w))\n",
        "            continue\n",
        "\n",
        "        out.append((str(item), 0.0))\n",
        "\n",
        "    # Sort by strength\n",
        "    out.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # FILTER: keep only meaningful traits (strength >= 0.18)\n",
        "    filtered = [(t, w) for (t, w) in out if w >= 0.18]\n",
        "\n",
        "    # If filtering removes everything → show top 2 signals\n",
        "    if not filtered:\n",
        "        filtered = out[:2]\n",
        "\n",
        "    return filtered[:max_items]\n",
        "\n",
        "def collapse_by_dimension(driving_traits, trait_dimensions):\n",
        "    grouped = {}\n",
        "    for trait, weight in driving_traits:\n",
        "        dim = trait_dimensions.get(trait, None)\n",
        "        if dim is None:\n",
        "            # traits we don't yet have labeled → keep as-is, no grouping\n",
        "            grouped[trait] = weight\n",
        "            continue\n",
        "        if dim not in grouped or weight > grouped[dim][1]:\n",
        "            grouped[dim] = (trait, weight)\n",
        "\n",
        "    # Flatten result\n",
        "    result = []\n",
        "    for val in grouped.values():\n",
        "        if isinstance(val, tuple):\n",
        "            result.append(val)\n",
        "        else:\n",
        "            result.append((val,0.0))\n",
        "    return result\n",
        "\n",
        "def explain_friendly_style(constitution, driving_traits, max_items=6):\n",
        "    dom = constitution[\"dominant\"].capitalize()\n",
        "\n",
        "    messages = {\n",
        "        \"Vata\": \"Your energy moves quickly — creativity, expression, fast thinking. If out of balance, dryness, irregular digestion, or light sleep may appear.\",\n",
        "        \"Pitta\": \"Your metabolism and focus are strong — decisiveness, clarity. If out of balance, heat, irritability, or intensity may rise.\",\n",
        "        \"Kapha\": \"Steady and grounded — patience, resilience. If out of balance, heaviness or low motivation may develop.\"\n",
        "    }\n",
        "\n",
        "    ordered = _normalize_driving_traits(driving_traits, constitution, max_items=max_items)\n",
        "    ordered = merge_sleep_traits(ordered)\n",
        "\n",
        "    trait_text = explain_traits_pretty(ordered)\n",
        "\n",
        "    return (\n",
        "        f\"You show a **{dom}-dominant** constitution.\\n\"\n",
        "        f\"{messages.get(dom,'')}\\n\\n\"\n",
        "        f\"Things your body signals clearly:\\n\\n\"\n",
        "        f\"{trait_text}\\n\\n\"\n",
        "        f\"Your body works best when balance is maintained. I’ll guide your diet and daily rhythm next.\"\n",
        "    )\n",
        "def recommend_for(constitution):\n",
        "    dom = (constitution.get(\"dominant\") or \"\").lower()\n",
        "    if dom == \"pitta\":\n",
        "        return ([\n",
        "            \"Cooling foods like cucumber, coconut water, sweet fruits\",\n",
        "            \"Avoid excessive spicy, sour, or fermented foods\",\n",
        "        ],[\n",
        "            \"Avoid late-night work (adds heat)\",\n",
        "            \"Practice slow breathing & meditation daily\",\n",
        "        ])\n",
        "    if dom == \"kapha\":\n",
        "        return ([\n",
        "            \"Light warm meals; reduce heavy/dairy foods\",\n",
        "            \"Use ginger and black pepper to stimulate metabolism\",\n",
        "        ],[\n",
        "            \"Regular morning physical activity\",\n",
        "            \"Avoid oversleeping / daytime naps\",\n",
        "        ])\n",
        "    # default vata\n",
        "    return ([\n",
        "        \"Warm cooked meals (soups, stews, kichadi)\",\n",
        "        \"Healthy fats like ghee, coconut, olive oil\",\n",
        "        \"Avoid cold salads, dry snacks and skipping meals\",\n",
        "    ],[\n",
        "        \"Sleep before 10:30 PM\",\n",
        "        \"Keep a consistent routine\",\n",
        "        \"Gentle yoga / stretching; avoid high intensity late evening\",\n",
        "    ])\n",
        "def merge_sleep_traits(ordered):\n",
        "    final = []\n",
        "    sleep_score = 0.0\n",
        "\n",
        "    for trait, score in ordered:\n",
        "        t = trait.lower()\n",
        "        if \"sleep patterns\" in t or \"light sleep\" in t or \"wakes easily\" in t:\n",
        "            sleep_score = max(sleep_score, score)  # keep strongest\n",
        "        else:\n",
        "            final.append((trait, score))\n",
        "\n",
        "    if sleep_score > 0:\n",
        "        final.insert(0, (\n",
        "            \"Your sleep becomes easily disturbed or inconsistent — a classic Vata influence on the nervous system.\",\n",
        "            sleep_score\n",
        "        ))\n",
        "    return final\n",
        "\n",
        "def make_friendly_output(constitution, driving_traits):\n",
        "    # Normalize + rank driving traits\n",
        "    ordered = _normalize_driving_traits(driving_traits, constitution, max_items=6)\n",
        "    ordered = merge_sleep_traits(ordered)\n",
        "    # Convert dataset trait labels → meaningful Ayurvedic phrases\n",
        "    readable_traits = []\n",
        "    for trait, score in ordered:\n",
        "        key = trait.lower().strip()\n",
        "        explanation = trait_explanations.get(key, trait)  # fallback to raw label if missing\n",
        "        readable_traits.append((explanation, score))\n",
        "\n",
        "    # Compose text output using readable trait explanations\n",
        "    lines = \"\\n\".join(\n",
        "        f\"• {txt} (signal strength: {round(score,3)})\"\n",
        "        for txt, score in readable_traits\n",
        "    ) if readable_traits else \"(no strong signals detected)\"\n",
        "\n",
        "    dom = constitution[\"dominant\"].capitalize()\n",
        "    text = (\n",
        "        f\"You show a **{dom}-dominant** constitution.\\n\"\n",
        "        f\"{AYURVEDIC_MESSAGES[dom]}\\n\\n\"\n",
        "        \"Things your body signals clearly:\\n\\n\"\n",
        "        f\"{lines}\\n\\n\"\n",
        "        \"Your body works best when balance is maintained. I’ll guide your diet and daily rhythm next.\"\n",
        "    )\n",
        "\n",
        "    diet, lifestyle = recommend_for(constitution)\n",
        "    return text, diet, lifestyle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guNgeGofdEr7"
      },
      "outputs": [],
      "source": [
        "AYURVEDIC_MESSAGES = {\n",
        "    \"Vata\": \"Your energy tends to move quickly — creativity, expressiveness, fast thinking. If balance slips, dryness, irregular digestion, or light sleep can show up.\",\n",
        "    \"Pitta\": \"Your metabolism and mind are strong — intensity, focus, decisiveness. If balance slips, heat, irritability, or overwork can show up.\",\n",
        "    \"Kapha\": \"You are steady and grounded — calm, patient, resilient. If balance slips, heaviness, sluggishness, or low motivation can show up.\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofq5J8HOelz8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def explain_traits_pretty(ordered_traits):\n",
        "    lines = []\n",
        "    for trait, score in ordered_traits:\n",
        "        phrase = trait_explanations.get(trait.lower(), None)\n",
        "        if phrase:\n",
        "            lines.append(f\"• {phrase} (signal strength: {score:.3f})\")\n",
        "        else:\n",
        "            lines.append(f\"• {trait} (signal strength: {score:.3f})\")\n",
        "    return \"\\n\".join(lines) if lines else \"(no strong signals detected)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntOcpWBOdj_8"
      },
      "outputs": [],
      "source": [
        "\n",
        "symptom_to_trait = {\n",
        "    \"breathing problem\": [\n",
        "        (\"general feel of skin (dry and thin, cool to touch, rough)\", \"vata\"),\n",
        "        (\"stress levels (high)\", \"vata\"),\n",
        "        (\"sleep patterns (short)\", \"vata\")\n",
        "    ],\n",
        "    \"shortness of breath\": [\n",
        "        (\"general feel of skin (dry and thin, cool to touch, rough)\", \"vata\"),\n",
        "        (\"digestion quality (weak)\", \"vata\"),\n",
        "    ],\n",
        "    \"anxiety\": [\n",
        "        (\"sleep patterns (short)\", \"vata\"),\n",
        "        (\"stress levels (high)\", \"vata\")\n",
        "    ],\n",
        "    \"overthinking\": [\n",
        "        (\"sleep patterns (light / wakes easily)\", \"vata\"),\n",
        "        (\"stress levels (high)\", \"vata\")\n",
        "    ],\n",
        "    \"body heat\": [\n",
        "        (\"complexion (fair-skin sunburns easily)\", \"pitta\"),\n",
        "        (\"digestion quality (strong)\", \"pitta\")\n",
        "    ],\n",
        "    \"anger / irritability\": [\n",
        "        (\"stress levels (high)\", \"pitta\"),\n",
        "        (\"sleep patterns (short)\", \"pitta\")\n",
        "    ],\n",
        "    \"lethargy\": [\n",
        "        (\"body weight (heavy - difficulties in losing weight)\", \"kapha\"),\n",
        "        (\"physical activity level (low)\", \"kapha\")\n",
        "    ],\n",
        "    \"slow digestion\": [\n",
        "        (\"digestion quality (weak)\", \"kapha\"),\n",
        "        (\"metabolism type (slow)\", \"kapha\")\n",
        "    ],\n",
        "    \"craving sweets\": [\n",
        "        (\"liking tastes (sweet / sour / salty)\", \"kapha\")\n",
        "    ],\n",
        "    \"dry skin\": [\n",
        "        (\"texture of skin (dry, pigments and aging)\", \"vata\")\n",
        "    ],\n",
        "    \"light sleep\": [\n",
        "        (\"sleep patterns (short)\", \"vata\")\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCcEoZbjeV1Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "def map_query_to_dataset_traits(query, top_k=5):\n",
        "    query = query.lower()\n",
        "    found = []\n",
        "\n",
        "    # 1) Symptom → trait mapping\n",
        "    for symptom, trait_list in symptom_to_trait.items():\n",
        "        if symptom in query:\n",
        "            for trait, _ in trait_list:\n",
        "                found.append(trait)\n",
        "\n",
        "    # 2) Existing dataset phrase matching\n",
        "    for phrase, mapped_trait in trait_synonyms.items():\n",
        "        if phrase in query:\n",
        "            found.append(mapped_trait)\n",
        "\n",
        "    # De-duplicate but keep first occurrences\n",
        "    found = list(dict.fromkeys(found))\n",
        "    return found[:top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqEcymWBfDua"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_user_constitution(query):\n",
        "    constitution, driving_traits, retrieval_score = compute_user_vpk_mapped(\n",
        "        query,\n",
        "        trait_vpk_table,\n",
        "        trait_index,\n",
        "        embedder\n",
        "    )\n",
        "    return constitution, driving_traits, retrieval_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbQtjBcCa58z"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ---- Prepare trait table from your 93-sample (illustrative) ----\n",
        "# Put your full dict here. Keys must be canonical, e.g. 'height (short)'.\n",
        "trait_vpk_table = {\n",
        "    'moderate build, stable weight': {'vata':0.1276595745,'pitta':0.1914893617,'kapha':0.0},\n",
        "    'slim body frame, difficulty gaining weight': {'vata':0.4545454545,'pitta':0.0303030303,'kapha':0.0},\n",
        "    'large/heavy frame, gains weight easily': {'vata':0.05,'pitta':0.1,'kapha':0.3},\n",
        "    'body weight (moderate - no difficulties in gaining or losing weight)': {'vata':0.1296296296,'pitta':0.2222222222,'kapha':0.0},\n",
        "    'body weight (low - difficulties in gaining weight)': {'vata':0.4482758621,'pitta':0.0,'kapha':0.0},\n",
        "    'body weight (heavy - difficulties in losing weight)': {'vata':0.1176470588,'pitta':0.0,'kapha':0.3529411765},\n",
        "    'height (average)': {'vata':0.1363636364,'pitta':0.2045454545,'kapha':0.0454545455},\n",
        "    'height (short)': {'vata':0.4210526316,'pitta':0.0526315789,'kapha':0.0263157895},\n",
        "    'height (tall)': {'vata':0.0,'pitta':0.0555555556,'kapha':0.1666666667},\n",
        "    'bone structure (large, broad shoulders , heavy bone structure)': {'vata':0.0,'pitta':0.1333333333,'kapha':0.4},\n",
        "    # ... add the rest of your 93 keys here ...\n",
        "    'texture of skin (dry, pigments and aging)': {'vata':0.25,'pitta':0.0,'kapha':0.0},\n",
        "    'complexion (fair-skin sunburns easily)': {'vata':0.0,'pitta':0.208,'kapha':0.0},\n",
        "    'sleep patterns (short)': {'vata':0.196,'pitta':0.0,'kapha':0.0},\n",
        "    'sleep patterns (moderate)': {'vata':0.230,'pitta':0.0,'kapha':0.0},\n",
        "    'sleep patterns (long)': {'vata':0.211,'pitta':0.0,'kapha':0.0},\n",
        "}\n",
        "\n",
        "# Build semantic index from trait keys\n",
        "_embedder = TokenEmbedding(vocab=set(\" \".join(trait_vpk_table.keys()).split()), dim=DIM, device='cpu')\n",
        "_trait_index = build_trait_embedding_index(list(trait_vpk_table.keys()), _embedder)\n",
        "\n",
        "def analyze_user_input(query):\n",
        "    #constitution, driving, score = compute_user_constitution(query)\n",
        "    constitution, driving, retr = compute_user_vpk_mapped(query, trait_vpk_table, _trait_index, _embedder, top_k=8)\n",
        "    # friendly output\n",
        "    text, diet, lifestyle = make_friendly_output(constitution, driving)\n",
        "    return {\n",
        "        \"constitution\": constitution,\n",
        "        \"driving\": driving,\n",
        "        \"retrieval\": retr,\n",
        "        \"text\": text,\n",
        "        \"diet\": diet,\n",
        "        \"lifestyle\": lifestyle,\n",
        "    }\n",
        "\n",
        "# ---- Quick test ----\n",
        "query = \"dry skin light sleep short height\"\n",
        "constitution, driving, score = compute_user_constitution(query)\n",
        "text, diet, lifestyle = make_friendly_output(constitution, driving)\n",
        "\n",
        "print(text)\n",
        "print(\"\\nDIET:\", diet)\n",
        "print(\"\\nLIFESTYLE:\", lifestyle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xe7AXo_gKL3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========= Phase-Live Mini Runtime (self-contained) =========\n",
        "from typing import List, Dict, Tuple\n",
        "import torch, torch.nn.functional as F\n",
        "import re\n",
        "\n",
        "# ---------------- Token embed + tokenizer ----------------\n",
        "class TokenEmbedding:\n",
        "    def __init__(self, vocab: List[str], dim:int=48, device:str='cpu'):\n",
        "        self.dim, self.device = dim, device\n",
        "        self.vocab = ['<unk>'] + list(vocab)\n",
        "        self.word2idx = {w:i for i,w in enumerate(self.vocab)}\n",
        "        self.embeddings = torch.randn(len(self.vocab), dim, device=device) / (dim**0.5)\n",
        "\n",
        "    def lookup(self, token:str) -> torch.Tensor:\n",
        "        idx = self.word2idx.get(token, 0)\n",
        "        return self.embeddings[idx]\n",
        "\n",
        "def universal_tokenizer(text: str) -> List[str]:\n",
        "    if not text: return []\n",
        "    return re.findall(r'\\d+\\.\\d+|\\d+|[A-Za-z]+|[+\\-*/^=():]', text.lower())\n",
        "\n",
        "# ---------------- Embedding index ----------------\n",
        "def build_trait_embedding_index(trait_vpk_table: Dict[str, Dict[str,float]],\n",
        "                                dim:int=48,\n",
        "                                device:str='cpu') -> Tuple['TokenEmbedding', Dict[str, torch.Tensor]]:\n",
        "    \"\"\"Return (embedder, {trait: normalized_vec}).\"\"\"\n",
        "    tokens = list(trait_vpk_table.keys())\n",
        "    emb = TokenEmbedding(tokens, dim=dim, device=device)\n",
        "    index: Dict[str, torch.Tensor] = {}\n",
        "    for trait in tokens:\n",
        "        toks = universal_tokenizer(trait)\n",
        "        if not toks:\n",
        "            continue\n",
        "        v = sum(emb.lookup(t) for t in toks) / len(toks)\n",
        "        v = v / (v.norm() + 1e-9)\n",
        "        index[trait] = v\n",
        "    return emb, index\n",
        "\n",
        "def semantic_match_traits(query: str,\n",
        "                          trait_index: Dict[str, torch.Tensor],\n",
        "                          embedder: TokenEmbedding,\n",
        "                          top_k:int=8) -> List[Tuple[str, float]]:\n",
        "    \"\"\"Return list of (trait, similarity) sorted desc.\"\"\"\n",
        "    toks = universal_tokenizer(query)\n",
        "    if not toks:\n",
        "        return []\n",
        "    q = sum(embedder.lookup(t) for t in toks) / len(toks)\n",
        "    q = q / (q.norm() + 1e-9)\n",
        "    sims = []\n",
        "    for trait, vec in trait_index.items():\n",
        "        s = F.cosine_similarity(q.unsqueeze(0), vec.unsqueeze(0)).item()\n",
        "        sims.append((trait, s))\n",
        "    sims.sort(key=lambda x: x[1], reverse=True)\n",
        "    return sims[:top_k]\n",
        "\n",
        "# ---------------- Constitution + driving traits ----------------\n",
        "def _normalize_driving_traits(driving_traits, constitution, max_items=8):\n",
        "    dom = (constitution.get(\"dominant\") or \"vata\").lower()\n",
        "    dom_idx = {\"vata\":0,\"pitta\":1,\"kapha\":2}[dom]\n",
        "    out = []\n",
        "    for item in driving_traits or []:\n",
        "        # shapes: (\"trait\", float) or (\"trait\", {\"vata\":..}) or (\"trait\", [v,p,k]) or \"trait\"\n",
        "        if isinstance(item, str):\n",
        "            out.append((item, 0.0)); continue\n",
        "        if isinstance(item, (list,tuple)) and len(item)>=1:\n",
        "            t = str(item[0]); w = 0.0\n",
        "            if len(item)>=2:\n",
        "                weights = item[1]\n",
        "                if isinstance(weights,(int,float)): w=float(weights)\n",
        "                elif isinstance(weights,dict): w=float(weights.get(dom,0.0))\n",
        "                elif isinstance(weights,(list,tuple)) and len(weights)>=3:\n",
        "                    try: w=float(weights[dom_idx])\n",
        "                    except: w=0.0\n",
        "            out.append((t,w)); continue\n",
        "        out.append((str(item),0.0))\n",
        "    out.sort(key=lambda x:x[1], reverse=True)\n",
        "    return out[:max_items]\n",
        "\n",
        "def collapse_by_dimension(driving_traits, trait_dimensions):\n",
        "    grouped = {}\n",
        "    for trait, weight in driving_traits:\n",
        "        dim = trait_dimensions.get(trait, None)\n",
        "        if dim is None:\n",
        "            grouped[trait] = weight  # keep unmatched traits directly\n",
        "            continue\n",
        "        if dim not in grouped or weight > grouped[dim][1]:\n",
        "            grouped[dim] = (trait, weight)\n",
        "    return [(t, w) for t, w in grouped.values()]\n",
        "\n",
        "def compute_user_constitution(query: str,\n",
        "                              trait_vpk_table: Dict[str, Dict[str,float]],\n",
        "                              trait_index=None,\n",
        "                              embedder: TokenEmbedding=None,\n",
        "                              top_k:int=8):\n",
        "    \"\"\"\n",
        "    Returns: (constitution_dict, driving_traits_list, retrieval_score_float)\n",
        "    trait_vpk_table: {\"trait string\": {\"vata\":..., \"pitta\":..., \"kapha\":...}}\n",
        "    \"\"\"\n",
        "    # Build/repair index if needed\n",
        "    if not isinstance(trait_index, dict) or trait_index is None or len(trait_index)==0:\n",
        "        embedder, trait_index = build_trait_embedding_index(trait_vpk_table)\n",
        "\n",
        "    matches = semantic_match_traits(query, trait_index, embedder, top_k=top_k)\n",
        "    if not matches:\n",
        "        # fallback neutral\n",
        "        v=p=k=1/3\n",
        "        constitution = {\n",
        "            \"vata\": round(v,3), \"pitta\": round(p,3), \"kapha\": round(k,3),\n",
        "            \"dominant\": \"vata\", \"confidence\": 0.333, \"matched_traits\": []\n",
        "        }\n",
        "        return constitution, [], 0.0\n",
        "\n",
        "    # collect dosha vectors for matched traits\n",
        "    vecs = []\n",
        "    driving = []\n",
        "    for trait, sim in matches:\n",
        "        vpk = trait_vpk_table.get(trait, {})\n",
        "        v = float(vpk.get(\"vata\", 0.0))\n",
        "        p = float(vpk.get(\"pitta\", 0.0))\n",
        "        k = float(vpk.get(\"kapha\", 0.0))\n",
        "        vecs.append((v,p,k))\n",
        "        # keep sim as weight to rank later\n",
        "        driving.append((trait, sim))\n",
        "\n",
        "    # average doshas\n",
        "    if vecs:\n",
        "        v = sum(x[0] for x in vecs)/len(vecs)\n",
        "        p = sum(x[1] for x in vecs)/len(vecs)\n",
        "        k = sum(x[2] for x in vecs)/len(vecs)\n",
        "    else:\n",
        "        v=p=k=1/3\n",
        "\n",
        "    # confidence (simple, tunable)\n",
        "    dom_name = [\"vata\",\"pitta\",\"kapha\"][[v,p,k].index(max(v,p,k))]\n",
        "    balance = abs(v-p)+abs(p-k)+abs(v-k)\n",
        "    retrieval_score = sum(s for _,s in matches)/len(matches)\n",
        "    confidence = max(0.0, min(1.0, 0.5*retrieval_score + 0.5*(max(v,p,k))*(1 - 0.5*balance)))\n",
        "\n",
        "    constitution = {\n",
        "        \"vata\": round(v,3),\n",
        "        \"pitta\": round(p,3),\n",
        "        \"kapha\": round(k,3),\n",
        "        \"dominant\": dom_name,\n",
        "        \"confidence\": round(confidence,3),\n",
        "        \"matched_traits\": [t for t,_ in matches]\n",
        "    }\n",
        "\n",
        "    # convert driving into multi-shape acceptable list:\n",
        "    # here we pass dict per dosha so _normalize can pick dominant value\n",
        "    # Convert matched traits into weighted driving traits structure\n",
        "    driving_traits = []\n",
        "    for trait, sim in driving:\n",
        "        vpk = trait_vpk_table.get(trait, {})\n",
        "        driving_traits.append(\n",
        "            (trait,\n",
        "             {\n",
        "                 \"vata\": vpk.get(\"vata\", 0.0),\n",
        "                 \"pitta\": vpk.get(\"pitta\", 0.0),\n",
        "                 \"kapha\": vpk.get(\"kapha\", 0.0)\n",
        "             })\n",
        "        )\n",
        "\n",
        "    # Normalize to sort traits by dominant dosha strength\n",
        "    normalized = _normalize_driving_traits(driving_traits, constitution, max_items=12)\n",
        "\n",
        "    # Remove contradictions: keep only strongest signal per dimension\n",
        "    collapsed = collapse_by_dimension(normalized, trait_dimensions)\n",
        "\n",
        "    return constitution, collapsed, float(retrieval_score)\n",
        "# ---------------- Friendly explanation + recs ----------------\n",
        "def explain_friendly_style(constitution, driving_traits, max_items=8):\n",
        "    dom = constitution[\"dominant\"].capitalize()\n",
        "    messages = {\n",
        "        \"Vata\":  \"Your energy tends to move quickly — creativity, expressiveness, fast thinking. \"\n",
        "                 \"If balance slips, dryness, irregular digestion, or light sleep can show up.\",\n",
        "        \"Pitta\": \"Your metabolism and mind are strong — intensity, focus, decisiveness. \"\n",
        "                 \"If balance slips, heat, irritability, or overwork can show up.\",\n",
        "        \"Kapha\": \"You are steady and grounded — calm, patient, resilient. \"\n",
        "                 \"If balance slips, heaviness, sluggishness, or low motivation can show up.\",\n",
        "    }\n",
        "    ordered = _normalize_driving_traits(driving_traits, constitution, max_items=max_items)\n",
        "    trait_lines = \"\\n\".join(\n",
        "    explain_trait(t, s, trait_dimensions)\n",
        "    for t, s in ordered\n",
        "    ) or \"(no strong signals detected)\"\n",
        "    return (\n",
        "        f\"You show a **{dom}-dominant** constitution.\\n\"\n",
        "        f\"{messages.get(dom, '')}\\n\\n\"\n",
        "        \"Things your body signals clearly:\\n\\n\"\n",
        "        f\"{trait_lines}\\n\\n\"\n",
        "        \"Your body works best when balance is maintained. I’ll guide your diet and daily rhythm next.\"\n",
        "    )\n",
        "\n",
        "def recommend_for(constitution):\n",
        "    dom = (constitution.get(\"dominant\") or \"\").lower()\n",
        "    if dom == \"vata\":\n",
        "        diet = [\n",
        "            \"Warm cooked meals (soups, stews, kichadi)\",\n",
        "            \"Healthy fats like ghee, coconut, olive oil\",\n",
        "            \"Avoid cold salads, dry snacks and skipping meals\",\n",
        "        ]\n",
        "        lifestyle = [\n",
        "            \"Sleep before 10:30 PM\",\n",
        "            \"Keep a consistent routine\",\n",
        "            \"Gentle yoga / stretching; avoid high intensity late evening\",\n",
        "        ]\n",
        "    elif dom == \"pitta\":\n",
        "        diet = [\n",
        "            \"Cooling foods like cucumber, coconut water, sweet fruits\",\n",
        "            \"Avoid excessive spicy, sour, or fermented foods\",\n",
        "        ]\n",
        "        lifestyle = [\n",
        "            \"Avoid late-night work (adds heat)\",\n",
        "            \"Practice slow breathing & meditation daily\",\n",
        "        ]\n",
        "    else:  # kapha\n",
        "        diet = [\n",
        "            \"Light, warm meals; reduce heavy/dairy foods\",\n",
        "            \"Use ginger and black pepper to stimulate metabolism\",\n",
        "        ]\n",
        "        lifestyle = [\n",
        "            \"Regular morning physical activity\",\n",
        "            \"Avoid oversleeping / daytime naps\",\n",
        "        ]\n",
        "    return diet, lifestyle\n",
        "\n",
        "def make_friendly_output(constitution, driving_traits):\n",
        "    text = explain_friendly_style(constitution, driving_traits or [])\n",
        "    diet, lifestyle = recommend_for(constitution)\n",
        "    return text, diet, lifestyle\n",
        "\n",
        "# ---------------- Quick test helper ----------------\n",
        "# Expect `trait_vpk_table` to exist: {\"trait\": {\"vata\":..., \"pitta\":..., \"kapha\":...}, ...}\n",
        "def quick_test(query: str, trait_vpk_table: Dict[str, Dict[str,float]],\n",
        "               trait_index=None, embedder=None):\n",
        "    constitution, driving, score = compute_user_constitution(\n",
        "        query, trait_vpk_table, trait_index=trait_index, embedder=embedder\n",
        "    )\n",
        "    text, diet, life = make_friendly_output(constitution, driving)\n",
        "    return {\"text\": text, \"diet\": diet, \"lifestyle\": life,\n",
        "            \"matched\": constitution[\"matched_traits\"], \"retrieval\": score}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiISEpTDjU8p"
      },
      "outputs": [],
      "source": [
        "\n",
        "def explain_trait(trait: str, weight: float, trait_dimensions: dict) -> str:\n",
        "    # Convert trait into clean label if available\n",
        "    label = trait_labels.get(trait, trait.replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \" \").strip())\n",
        "    dim = trait_dimensions.get(trait, None)\n",
        "\n",
        "    if dim == \"body_frame\":\n",
        "        return f\"• {label} suggests Vata influences body build and movement. (signal strength: {weight:.3f})\"\n",
        "    elif dim == \"body_weight\":\n",
        "        return f\"• {label} reflects how easily your body maintains grounding and nourishment. (signal strength: {weight:.3f})\"\n",
        "    elif dim == \"height\":\n",
        "        return f\"• {label} often appears in Vata-dominant constitutions. (signal strength: {weight:.3f})\"\n",
        "    elif dim == \"skin_texture\":\n",
        "        return f\"• {label} indicates how moisture and circulation are regulated. (signal strength: {weight:.3f})\"\n",
        "    elif dim == \"sleep\":\n",
        "        return f\"• {label} reflects the sensitivity of your nervous system. (signal strength: {weight:.3f})\"\n",
        "\n",
        "    # fallback\n",
        "    return f\"• {label} (signal strength: {weight:.3f})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmYxQwacjxvR"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Human-readable clean names for traits\n",
        "trait_labels = {\n",
        "    \"slim body frame, difficulty gaining weight\": \"Slim / light body frame\",\n",
        "    \"body weight (low - difficulties in gaining weight)\": \"Difficulty retaining weight\",\n",
        "    \"height (short)\": \"Shorter height\",\n",
        "    \"height (average)\": \"Medium height\",\n",
        "    \"height (tall)\": \"Tall body frame\",\n",
        "    \"large/heavy frame, gains weight easily\": \"Heavier / broad body frame\",\n",
        "    \"moderate build, stable weight\": \"Stable balanced build\",\n",
        "    \"texture of skin (dry, pigments and aging)\": \"Dry / rough skin texture\",\n",
        "    \"complexion (fair-skin sunburns easily)\": \"Fair complexion prone to heat\",\n",
        "    # Add more here gradually if needed\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAQZ_RJhhiFI"
      },
      "outputs": [],
      "source": [
        "\n",
        "trait_dimensions = {\n",
        "    # Body Size / Frame\n",
        "    \"moderate build, stable weight\": \"body_frame\",\n",
        "    \"slim body frame, difficulty gaining weight\": \"body_frame\",\n",
        "    \"large/heavy frame, gains weight easily\": \"body_frame\",\n",
        "\n",
        "    # Body Weight\n",
        "    \"body weight (moderate - no difficulties in gaining or losing weight)\": \"body_weight\",\n",
        "    \"body weight (low - difficulties in gaining weight)\": \"body_weight\",\n",
        "    \"body weight (heavy - difficulties in losing weight)\": \"body_weight\",\n",
        "\n",
        "    # Height\n",
        "    \"height (average)\": \"height\",\n",
        "    \"height (short)\": \"height\",\n",
        "    \"height (tall)\": \"height\",\n",
        "\n",
        "    # Skin\n",
        "    \"texture of skin (dry, pigments and aging)\": \"skin_texture\",\n",
        "    \"general feel of skin (dry and thin, cool to touch, rough)\": \"skin_texture\",\n",
        "\n",
        "    # Complexion\n",
        "    \"complexion (fair-skin sunburns easily)\": \"complexion\",\n",
        "    \"complexion (white, pale, tans easily)\": \"complexion\",\n",
        "\n",
        "    # Sleep\n",
        "    \"sleep patterns (short)\": \"sleep\",\n",
        "    \"sleep patterns (moderate)\": \"sleep\",\n",
        "    \"sleep patterns (long)\": \"sleep\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abHZ02OqjQ-V"
      },
      "outputs": [],
      "source": [
        "\n",
        "dimension_explanations = {\n",
        "    \"body_frame\": \"Your constitution tends toward lightness or stability in physical structure.\",\n",
        "    \"body_weight\": \"Your body's tendency to gain or lose weight reflects underlying dosha balance.\",\n",
        "    \"height\": \"Height characteristics can indicate how Vata, Pitta, or Kapha influence body structure.\",\n",
        "    \"skin_texture\": \"Skin texture signals how moisture, warmth, and circulation are regulated.\",\n",
        "    \"complexion\": \"Your complexion suggests how heat and metabolic intensity are expressed.\",\n",
        "    \"hair\": \"Hair qualities reveal internal heat, nourishment, and oil balance.\",\n",
        "    \"sleep\": \"Your sleep rhythm reflects how your nervous system maintains balance.\",\n",
        "    \"appetite\": \"Hunger pattern indicates how digestive fire (Agni) is functioning.\",\n",
        "    \"digestion\": \"Digestive behavior expresses how smoothly nutrients are processed.\",\n",
        "    \"climate_tolerance\": \"Your comfort in climates shows how heat and cold regulation operate.\",\n",
        "    \"stress\": \"Stress response indicates how grounded or sensitive the nervous system is.\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uelTvruHisl7"
      },
      "outputs": [],
      "source": [
        "\n",
        "trait_phrases = {\n",
        "    \"slim body frame, difficulty gaining weight\":\n",
        "        \"Your constitution tends toward lightness and quick movement (Vata trait).\",\n",
        "    \"height (short)\":\n",
        "        \"Shorter body frame is more frequently seen in Vata-dominant constitutions.\",\n",
        "    \"texture of skin (dry, pigments and aging)\":\n",
        "        \"Dry or easily dehydrated skin indicates elevated Vata affecting moisture.\",\n",
        "    \"sleep patterns (short)\":\n",
        "        \"Light and easily disturbed sleep suggests Vata influence on the nervous system.\",\n",
        "    \"body weight (low - difficulties in gaining weight)\":\n",
        "        \"Your body tends not to retain weight, reflecting Vata predominance.\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBZVuHkCgO7I",
        "outputId": "b643af92-0a42-44b4-d189-4758ca464993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You show a **Vata-dominant** constitution.\n",
            "Your energy tends to move quickly — creativity, expressiveness, fast thinking. If balance slips, dryness, irregular digestion, or light sleep can show up.\n",
            "\n",
            "Things your body signals clearly:\n",
            "\n",
            "• Slim / light body frame suggests Vata influences body build and movement. (signal strength: 0.455)\n",
            "• Difficulty retaining weight reflects how easily your body maintains grounding and nourishment. (signal strength: 0.448)\n",
            "• Shorter height often appears in Vata-dominant constitutions. (signal strength: 0.421)\n",
            "\n",
            "Your body works best when balance is maintained. I’ll guide your diet and daily rhythm next.\n",
            "\n",
            "DIET: ['Warm cooked meals (soups, stews, kichadi)', 'Healthy fats like ghee, coconut, olive oil', 'Avoid cold salads, dry snacks and skipping meals']\n",
            "\n",
            "LIFESTYLE: ['Sleep before 10:30 PM', 'Keep a consistent routine', 'Gentle yoga / stretching; avoid high intensity late evening']\n",
            "\n",
            "MATCHED: ['moderate build, stable weight', 'slim body frame, difficulty gaining weight', 'large/heavy frame, gains weight easily', 'body weight (moderate - no difficulties in gaining or losing weight)', 'body weight (low - difficulties in gaining weight)', 'body weight (heavy - difficulties in losing weight)', 'height (average)', 'height (short)']\n",
            "RETRIEVAL SCORE: 1.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1) Prepare your trait_vpk_table (you already have this dict)\n",
        "# Example single entry structure:\n",
        "# trait_vpk_table = {\n",
        "#   \"height (short)\": {\"vata\": 0.42, \"pitta\": 0.05, \"kapha\": 0.03},\n",
        "#   ...\n",
        "# }\n",
        "\n",
        "# 2) Build index once (optional: you can also let compute_user_constitution rebuild if missing)\n",
        "embedder, trait_index = build_trait_embedding_index(trait_vpk_table, dim=48, device=\"cpu\")\n",
        "\n",
        "# 3) Run a test\n",
        "res = quick_test(\"dry skin light sleep short height\", trait_vpk_table, trait_index, embedder)\n",
        "print(res[\"text\"])\n",
        "print(\"\\nDIET:\", res[\"diet\"])\n",
        "print(\"\\nLIFESTYLE:\", res[\"lifestyle\"])\n",
        "print(\"\\nMATCHED:\", res[\"matched\"])\n",
        "print(\"RETRIEVAL SCORE:\", round(res[\"retrieval\"],3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIdqYdnCGP1a"
      },
      "source": [
        "# Chatbot 2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDmhQi1iOBQW",
        "outputId": "4880084b-cb53-49b3-f631-59693ab0e703"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded: (1200, 30)\n",
            "Total fragments generated: 7200\n",
            "Fragments ready for embedding & retrieval.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ======================================================================\n",
        "# 1. LOAD DATASET\n",
        "# ======================================================================\n",
        "df = pd.read_csv(\"/content/Updated_Prakriti_With_Features.csv\")  # or excel\n",
        "print(\"Dataset loaded:\", df.shape)\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# 2. FORMAT SAFELY (replace NaN with '')\n",
        "# ======================================================================\n",
        "df = df.fillna(\"\")\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# 3. HYBRID B + C STYLE — 6 FRAGMENTS PER ROW\n",
        "# ======================================================================\n",
        "\n",
        "def generate_fragments(row):\n",
        "    \"\"\"Generate 6 high-quality text fragments per Prakriti row (Hybrid B+C).\"\"\"\n",
        "\n",
        "    # ------------------------- 1. Physical Fragment -------------------------\n",
        "    frag1 = (\n",
        "        f\"The individual has {row['Body Size']} body size, {row['Body Weight']} weight, \"\n",
        "        f\"{row['Height']} height and {row['Bone Structure']} bone structure. \"\n",
        "        f\"The complexion is {row['Complexion']}. \"\n",
        "        \"These physical features indicate a mixture of stability and mobility in body constitution.\"\n",
        "    )\n",
        "\n",
        "    # ------------------------- 2. Skin & Hair Fragment -------------------------\n",
        "    frag2 = (\n",
        "        f\"The skin is described as {row['General feel of skin']} with texture {row['Texture of Skin']}. \"\n",
        "        f\"Hair color is {row['Hair Color']} and the appearance is {row['Appearance of Hair']}. \"\n",
        "        \"This combination reflects dryness and sensitivity commonly associated with Vata and Pitta qualities.\"\n",
        "    )\n",
        "\n",
        "    # --------------------- 3. Face & Sensory Features Fragment ---------------------\n",
        "    frag3 = (\n",
        "        f\"The face shape is {row['Shape of face']}, and the eyes are {row['Eyes']}. \"\n",
        "        f\"The lips are {row['Lips']}, and nails are {row['Nails']}. \"\n",
        "        f\"The teeth and gums appear {row['Teeth and gums']}. \"\n",
        "        \"These sensory features suggest blended Vata-Pitta influence in facial constitution.\"\n",
        "    )\n",
        "\n",
        "    # ------------------------- 4. Functional Body/Digestion Fragment -------------------------\n",
        "    frag4 = (\n",
        "        f\"Appetite is {row['Appetite']} and digestion quality is {row['Digestion Quality']}. \"\n",
        "        f\"Metabolism type is {row['Metabolism Type']}, and liking tastes include {row['Liking tastes']}. \"\n",
        "        \"These patterns indicate underlying dosha behavior affecting digestion and metabolism.\"\n",
        "    )\n",
        "\n",
        "    # -------------------------- 5. Mental & Lifestyle Fragment -------------------------\n",
        "    frag5 = (\n",
        "        f\"Stress levels are {row['Stress Levels']} and sleep patterns are {row['Sleep Patterns']}. \"\n",
        "        f\"The person follows {row['Dietary Habits']} diet, with physical activity level being {row['Physical Activity Level']}. \"\n",
        "        f\"Water intake is {row['Water Intake']}. \"\n",
        "        \"This lifestyle pattern shows the mental and behavioral tendencies of the constitution.\"\n",
        "    )\n",
        "\n",
        "    # -------------------------- 6. Ayurvedic Interpretation Fragment -------------------------\n",
        "    frag6 = (\n",
        "        f\"The dominant dosha is {row['Dosha']}. \"\n",
        "        f\"Climate preference is {row['Climate Preference']}. \"\n",
        "        f\"Skin sensitivity is {row['Skin Sensitivity']}. \"\n",
        "        \"This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\"\n",
        "    )\n",
        "\n",
        "    return [frag1, frag2, frag3, frag4, frag5, frag6]\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# 4. GENERATE ALL FRAGMENTS\n",
        "# ======================================================================\n",
        "all_fragments = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    frags = generate_fragments(row)\n",
        "    for i, text in enumerate(frags):\n",
        "        all_fragments.append({\n",
        "            \"id\": f\"{idx}_{i}\",\n",
        "            \"fragmenttext\": text,\n",
        "            \"dosha\": row[\"Dosha\"]\n",
        "        })\n",
        "\n",
        "print(\"Total fragments generated:\", len(all_fragments))\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# 5. READY TO EMBED (use your embedding system next)\n",
        "# ======================================================================\n",
        "\n",
        "# You will later do:\n",
        "# from embedding_index import SomeEmbeddingModel\n",
        "# vector = embedder.compute_embedding(fragmenttext)\n",
        "\n",
        "print(\"Fragments ready for embedding & retrieval.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYLGMWXAOfqE",
        "outputId": "e8046008-4cdc-4565-a92d-0a1b5fdd3088"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total fragment texts: 7200\n",
            ">>> Building fresh embedding index...\n",
            ">>> Embedding index built! Total vectors: 7200\n",
            "Index built! Retrieval ready.\n",
            "\n",
            "Query: I feel dryness in my skin and joints\n",
            "\n",
            "Top results:\n",
            "  → {'fragmentid': '103_2', 'fragmenttext': 'The face shape is Large, round, full, and the eyes are Medium-sized, penetrating, light-sensitive eyes. The lips are Lips are soft, medium-sized, and nails are Dry, Rough, Brittle, Break. The teeth and gums appear Medium-sized teeth, Reddish gums. These sensory features suggest blended Vata-Pitta influence in facial constitution.', 'embeddingrank': 1, 'retrievalconfidence': 0.9034973382949829}\n",
            "  → {'fragmentid': '100_2', 'fragmenttext': 'The face shape is Long, angular, thin, and the eyes are Medium-sized, penetrating, light-sensitive eyes. The lips are Tight, thin, dry lips which chaps easily, and nails are Thick, Oily, Smooth, Polished. The teeth and gums appear Big, White, Strong teeth, Healthy gums. These sensory features suggest blended Vata-Pitta influence in facial constitution.', 'embeddingrank': 2, 'retrievalconfidence': 0.9034973382949829}\n",
            "  → {'fragmentid': '99_2', 'fragmenttext': 'The face shape is Long, angular, thin, and the eyes are Medium-sized, penetrating, light-sensitive eyes. The lips are Lips are soft, medium-sized, and nails are Dry, Rough, Brittle, Break. The teeth and gums appear Medium-sized teeth, Reddish gums. These sensory features suggest blended Vata-Pitta influence in facial constitution.', 'embeddingrank': 3, 'retrievalconfidence': 0.9034973382949829}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ================================================================\n",
        "# INTEGRATE PRAKRITI FRAGMENTS INTO YOUR embedding_matching SYSTEM\n",
        "# ================================================================\n",
        "\n",
        "from embedding_matching import build_fragments, retrieve\n",
        "\n",
        "# 1. Extract the text from fragment dicts\n",
        "fragment_texts = [frag[\"fragmenttext\"] for frag in all_fragments]\n",
        "\n",
        "print(\"Total fragment texts:\", len(fragment_texts))\n",
        "\n",
        "# 2. Build the embedding index\n",
        "build_fragments(fragment_texts)\n",
        "\n",
        "print(\"Index built! Retrieval ready.\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# TEST RETRIEVAL\n",
        "# ================================================================\n",
        "\n",
        "query = \"I feel dryness in my skin and joints\"\n",
        "results = retrieve(query, k=3)\n",
        "\n",
        "print(\"\\nQuery:\", query)\n",
        "print(\"\\nTop results:\")\n",
        "for r in results:\n",
        "    print(\"  →\", r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJC7oa7jczwH"
      },
      "outputs": [],
      "source": [
        "\n",
        "# semantic_graph.py\n",
        "\"\"\"\n",
        "Semantic Graph Manager (Advanced — dynamic, dataset-agnostic, auto-rebuild)\n",
        "\n",
        "Usage:\n",
        "    from semantic_graph import SemanticGraphManager\n",
        "    sgm = SemanticGraphManager(fragment_texts, embedder=TokenEmbedding(...))\n",
        "    expanded = sgm.expand_query(\"dryness in my skin\", top_k=12)\n",
        "    results = retrieve(expanded, k=5)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import hashlib\n",
        "from collections import defaultdict, Counter\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# import your tokenizer & embedder (must exist)\n",
        "from tokenizer_and_embedding import universal_tokenizer, TokenEmbedding\n",
        "\n",
        "# constants\n",
        "GRAPH_CACHE = \"semantic_graph.pkl\"\n",
        "\n",
        "\n",
        "def _checksum_texts(texts: List[str]) -> str:\n",
        "    m = hashlib.sha256()\n",
        "    for t in texts:\n",
        "        if not isinstance(t, str):\n",
        "            t = str(t)\n",
        "        m.update(t.encode(\"utf-8\", errors=\"ignore\"))\n",
        "        m.update(b\"\\n\")\n",
        "    return m.hexdigest()\n",
        "\n",
        "\n",
        "def _top_n(counter: Counter, n: int):\n",
        "    return [w for w, _ in counter.most_common(n)]\n",
        "\n",
        "\n",
        "class SemanticGraphManager:\n",
        "    def __init__(\n",
        "        self,\n",
        "        fragments: List[str],\n",
        "        embedder: Optional[TokenEmbedding] = None,\n",
        "        cache_path: str = GRAPH_CACHE,\n",
        "        min_freq: int = 3,\n",
        "        max_keywords: int = 2000,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        fragments: list of fragment strings (dataset-derived)\n",
        "        embedder: instance of TokenEmbedding (must provide lookup(token)->vector)\n",
        "        \"\"\"\n",
        "        self.fragments = fragments\n",
        "        self.embedder = embedder or TokenEmbedding(vocab=[], dim=50)  # fallback\n",
        "        self.cache_path = cache_path\n",
        "        self.min_freq = min_freq\n",
        "        self.max_keywords = max_keywords\n",
        "\n",
        "        self.checksum = None\n",
        "        self.graph = {}  # {kw: {neighbor: weight}}\n",
        "        self.keywords = []  # list of keywords considered\n",
        "        self.keyword_vectors = None  # np.array shape (num_keywords, dim)\n",
        "        self.clusters = None  # dict: cluster_id -> [keywords]\n",
        "        self._build_if_needed()\n",
        "\n",
        "    # -------------------------\n",
        "    # Public API\n",
        "    # -------------------------\n",
        "    def expand_query(self, query: str, top_k: int = 12, neighbor_depth: int = 2) -> str:\n",
        "        \"\"\"\n",
        "        Returns an expanded query string (original tokens + top related words).\n",
        "        also returns a list of (token, neighbors) if needed via expand_query_detail.\n",
        "        \"\"\"\n",
        "        toks = [t for t in universal_tokenizer(query) if t]\n",
        "        if not toks:\n",
        "            return query\n",
        "\n",
        "        neighbors = []\n",
        "        for tok in toks:\n",
        "            if tok in self.graph:\n",
        "                # get top neighbors\n",
        "                neighs = sorted(self.graph[tok].items(), key=lambda x: -x[1])\n",
        "                neighs = [n for n, _ in neighs[:top_k]]\n",
        "            else:\n",
        "                # fallback: embedding similarity to known keywords\n",
        "                neighs = self._nearest_keywords_by_embedding(tok, top_k=6)\n",
        "            for n in neighs:\n",
        "                if n not in neighbors and n not in toks:\n",
        "                    neighbors.append(n)\n",
        "            if len(neighbors) >= top_k:\n",
        "                break\n",
        "\n",
        "        # cluster-based enrichment: add a few keywords from same clusters\n",
        "        extra = []\n",
        "        for t in toks:\n",
        "            if t in self.keywords and self.clusters:\n",
        "                # find cluster id\n",
        "                for cid, members in self.clusters.items():\n",
        "                    if t in members:\n",
        "                        for m in members:\n",
        "                            if m not in neighbors and m not in toks:\n",
        "                                extra.append(m)\n",
        "                        break\n",
        "            if len(extra) >= 4:\n",
        "                break\n",
        "\n",
        "        combined = list(dict.fromkeys(toks + neighbors + extra))  # preserve order, dedupe\n",
        "        # limit length\n",
        "        combined = combined[: max(len(toks) + top_k, top_k)]\n",
        "        expanded_query = \" \".join(combined)\n",
        "        return expanded_query\n",
        "\n",
        "    def expand_query_detail(self, query: str, top_k: int = 12) -> Dict[str, List[str]]:\n",
        "        \"\"\"Returns mapping: token -> list of neighbor keywords (for debugging).\"\"\"\n",
        "        toks = [t for t in universal_tokenizer(query) if t]\n",
        "        detail = {}\n",
        "        for tok in toks:\n",
        "            if tok in self.graph:\n",
        "                neighs = sorted(self.graph[tok].items(), key=lambda x: -x[1])\n",
        "                detail[tok] = [n for n, _ in neighs[:top_k]]\n",
        "            else:\n",
        "                detail[tok] = self._nearest_keywords_by_embedding(tok, top_k=top_k)\n",
        "        return detail\n",
        "\n",
        "    # -------------------------\n",
        "    # Internal: build/rebuild\n",
        "    # -------------------------\n",
        "    def _build_if_needed(self):\n",
        "        checksum = _checksum_texts(self.fragments)\n",
        "        if os.path.exists(self.cache_path):\n",
        "            try:\n",
        "                with open(self.cache_path, \"rb\") as f:\n",
        "                    saved = pickle.load(f)\n",
        "                if saved.get(\"checksum\") == checksum:\n",
        "                    # load from cache\n",
        "                    self.graph = saved[\"graph\"]\n",
        "                    self.keywords = saved[\"keywords\"]\n",
        "                    self.keyword_vectors = saved[\"keyword_vectors\"]\n",
        "                    self.clusters = saved[\"clusters\"]\n",
        "                    self.checksum = checksum\n",
        "                    return\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # else rebuild\n",
        "        self._rebuild_graph()\n",
        "        # save\n",
        "        tosave = {\n",
        "            \"checksum\": checksum,\n",
        "            \"graph\": self.graph,\n",
        "            \"keywords\": self.keywords,\n",
        "            \"keyword_vectors\": self.keyword_vectors,\n",
        "            \"clusters\": self.clusters,\n",
        "        }\n",
        "        try:\n",
        "            with open(self.cache_path, \"wb\") as f:\n",
        "                pickle.dump(tosave, f)\n",
        "        except Exception:\n",
        "            pass\n",
        "        self.checksum = checksum\n",
        "\n",
        "    def _rebuild_graph(self):\n",
        "        # 1) extract keyword candidates\n",
        "        kw_lists = self._extract_keywords_from_fragments(self.fragments)\n",
        "        # flatten + frequency\n",
        "        freq = Counter()\n",
        "        for kws in kw_lists:\n",
        "            freq.update(kws)\n",
        "\n",
        "        # select top keywords by frequency and min_freq\n",
        "        cand = [w for w, c in freq.items() if c >= self.min_freq]\n",
        "        cand = sorted(cand, key=lambda w: -freq[w])\n",
        "        if len(cand) > self.max_keywords:\n",
        "            cand = cand[: self.max_keywords]\n",
        "\n",
        "        self.keywords = cand\n",
        "\n",
        "        # 2) build co-occurrence matrix (window = fragment-level)\n",
        "        cooc = defaultdict(Counter)\n",
        "        for kws in kw_lists:\n",
        "            present = [w for w in kws if w in self.keywords]\n",
        "            for i, a in enumerate(present):\n",
        "                for b in present[i + 1 :]:\n",
        "                    cooc[a][b] += 1\n",
        "                    cooc[b][a] += 1\n",
        "\n",
        "        # normalize cooc to PMI-like weights\n",
        "        total_fragments = len(kw_lists)\n",
        "        p = {w: (freq[w] / total_fragments) for w in self.keywords}\n",
        "        graph = {w: {} for w in self.keywords}\n",
        "        for a in self.keywords:\n",
        "            for b, c_ab in cooc[a].items():\n",
        "                # avoid zero\n",
        "                p_ab = c_ab / total_fragments\n",
        "                score = (p_ab / (p[a] * p[b] + 1e-12))  # PMI-ish\n",
        "                graph[a][b] = float(score)\n",
        "\n",
        "        # 3) embedding similarity edges\n",
        "        kw_vecs = []\n",
        "        for w in self.keywords:\n",
        "            v = self.embedder.lookup(w)\n",
        "            if not isinstance(v, np.ndarray):\n",
        "                v = np.array(v, dtype=np.float32)\n",
        "            kw_vecs.append(v)\n",
        "        kw_mat = np.vstack(kw_vecs).astype(np.float32)\n",
        "        # normalize for cosine\n",
        "        kw_mat = normalize(kw_mat, axis=1)\n",
        "        self.keyword_vectors = kw_mat\n",
        "\n",
        "        # compute cosine similarity and merge with graph\n",
        "        sim = kw_mat.dot(kw_mat.T)  # cosine matrix\n",
        "        n = len(self.keywords)\n",
        "        for i in range(n):\n",
        "            a = self.keywords[i]\n",
        "            sims = sim[i]\n",
        "            # find top similar keywords (excluding itself)\n",
        "            top_idx = np.argsort(-sims)[1: min(50, n)]\n",
        "            for j in top_idx:\n",
        "                b = self.keywords[j]\n",
        "                s = float(sims[j])\n",
        "                # combine by adding (weighted)\n",
        "                prev = graph[a].get(b, 0.0)\n",
        "                graph[a][b] = float(prev + 0.6 * s)\n",
        "\n",
        "        # 4) cluster keywords into concept groups\n",
        "        # choose number of clusters heuristically\n",
        "        num_kw = len(self.keywords)\n",
        "        if num_kw >= 10:\n",
        "            n_clusters = int(min(max(5, np.sqrt(num_kw)), 150))\n",
        "            try:\n",
        "                km = KMeans(n_clusters=n_clusters, random_state=0, n_init=10)\n",
        "                labels = km.fit_predict(kw_mat)\n",
        "                clusters = defaultdict(list)\n",
        "                for kw, lab in zip(self.keywords, labels):\n",
        "                    clusters[int(lab)].append(kw)\n",
        "                self.clusters = dict(clusters)\n",
        "            except Exception:\n",
        "                self.clusters = {}\n",
        "        else:\n",
        "            self.clusters = {}\n",
        "\n",
        "        # 5) finalize graph (prune low weights)\n",
        "        # normalize neighbor weights and prune\n",
        "        final_graph = {}\n",
        "        for a, neighs in graph.items():\n",
        "            # normalize\n",
        "            total = sum(neighs.values()) + 1e-12\n",
        "            normed = {b: float(v / total) for b, v in neighs.items() if v > 1e-4}\n",
        "            # keep top 50 neighbors\n",
        "            top = dict(sorted(normed.items(), key=lambda x: -x[1])[:50])\n",
        "            final_graph[a] = top\n",
        "\n",
        "        self.graph = final_graph\n",
        "        return\n",
        "\n",
        "    # -------------------------\n",
        "    # Helpers\n",
        "    # -------------------------\n",
        "    def _extract_keywords_from_fragments(self, fragments: List[str]) -> List[List[str]]:\n",
        "        \"\"\"\n",
        "        For each fragment, extract a set of candidate keywords:\n",
        "        - use tokenizer\n",
        "        - filter stopwords and short tokens\n",
        "        - include bigrams if frequent\n",
        "        \"\"\"\n",
        "        lists = []\n",
        "        bigram_counts = Counter()\n",
        "        unigram_counts = Counter()\n",
        "        for f in fragments:\n",
        "            toks = [t for t in universal_tokenizer(f.lower()) if len(t) > 1]\n",
        "            # keep alphanumeric tokens only\n",
        "            toks = [t for t in toks if t.isalnum()]\n",
        "            for i, t in enumerate(toks):\n",
        "                unigram_counts[t] += 1\n",
        "                if i + 1 < len(toks):\n",
        "                    bigram = toks[i] + \"_\" + toks[i + 1]\n",
        "                    bigram_counts[bigram] += 1\n",
        "            lists.append(toks)\n",
        "\n",
        "        # select top bigrams to inject\n",
        "        top_bigrams = set([b for b, _ in bigram_counts.most_common(200)])\n",
        "        # now produce final per-fragment keyword lists (include top bigrams)\n",
        "        final_lists = []\n",
        "        for f in fragments:\n",
        "            toks = [t for t in universal_tokenizer(f.lower()) if len(t) > 1 and t.isalnum()]\n",
        "            kws = []\n",
        "            for i, t in enumerate(toks):\n",
        "                if (i + 1) < len(toks):\n",
        "                    bigram = toks[i] + \"_\" + toks[i + 1]\n",
        "                    if bigram in top_bigrams:\n",
        "                        kws.append(bigram)\n",
        "                kws.append(t)\n",
        "            # dedupe preserving order\n",
        "            seen = set()\n",
        "            out = []\n",
        "            for x in kws:\n",
        "                if x not in seen:\n",
        "                    seen.add(x)\n",
        "                    out.append(x)\n",
        "            final_lists.append(out)\n",
        "        return final_lists\n",
        "\n",
        "    def _nearest_keywords_by_embedding(self, token: str, top_k: int = 8) -> List[str]:\n",
        "        # Use embedder to find nearest keywords in self.keyword_vectors\n",
        "        if token not in (self.keywords or []):\n",
        "            v = self.embedder.lookup(token)\n",
        "            v = np.array(v, dtype=np.float32)\n",
        "            # normalize\n",
        "            if v.ndim == 1:\n",
        "                v = v.reshape(1, -1)\n",
        "            v = normalize(v, axis=1)\n",
        "            if self.keyword_vectors is None:\n",
        "                return []\n",
        "            scores = (self.keyword_vectors @ v.T).squeeze()\n",
        "            idx = np.argsort(-scores)[:top_k]\n",
        "            return [self.keywords[i] for i in idx.tolist()]\n",
        "        else:\n",
        "            # token is itself in keywords -> return top neighbors\n",
        "            neighs = self.graph.get(token, {})\n",
        "            return list(neighs.keys())[:top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOTr8MXIWbXW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========= semantic_boost_retriever.py (run inline in Colab) =========\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "import re\n",
        "\n",
        "# imports from your codebase\n",
        "from tokenizer_and_embedding import TokenEmbedding, universal_tokenizer\n",
        "#from semantic_graph import SemanticGraphManager\n",
        "from embedding_matching import retrieve  # original index-based retrieve\n",
        "\n",
        "# PARAMETERS (tweakable)\n",
        "K_FINAL = 5               # number of results to return\n",
        "K_CANDIDATES = 25         # number of candidates to fetch from base index\n",
        "ALPHA = 0.6               # weight for original retrieval confidence\n",
        "BETA = 0.35               # weight for semantic keyword score\n",
        "GAMMA = 0.05              # weight for fragment-type bonus\n",
        "MIN_KEYWORD_LEN = 2\n",
        "\n",
        "# 6 fragment types mapping (index mod 6 -> type)\n",
        "FRAG_TYPE_MAP = {\n",
        "    0: \"physical\",\n",
        "    1: \"skin\",\n",
        "    2: \"face_sensory\",\n",
        "    3: \"digestion\",\n",
        "    4: \"lifestyle\",\n",
        "    5: \"ayurvedic\"\n",
        "}\n",
        "\n",
        "# build fragment lookup and type for your all_fragments\n",
        "fragment_by_id = {}\n",
        "fragment_type_of = {}\n",
        "for frag in all_fragments:\n",
        "    fid = frag.get(\"id\") or frag.get(\"fragmentid\")  # your fragments use 'id' earlier\n",
        "    txt = frag[\"fragmenttext\"]\n",
        "    fragment_by_id[fid] = frag\n",
        "    # try to infer numeric index from id pattern like \"0_3\" or \"frag180\"\n",
        "    typ = None\n",
        "    if isinstance(fid, str):\n",
        "        # if id like \"0_3\" use suffix; else if \"fragNNN\" use numeric NNN\n",
        "        if \"_\" in fid:\n",
        "            try:\n",
        "                idx = int(fid.split(\"_\")[1])\n",
        "            except:\n",
        "                idx = 0\n",
        "        else:\n",
        "            m = re.search(r'\\d+', fid)\n",
        "            idx = int(m.group(0)) if m else 0\n",
        "    else:\n",
        "        idx = int(fid)\n",
        "    typ = FRAG_TYPE_MAP[idx % 6]\n",
        "    fragment_type_of[fid] = typ\n",
        "\n",
        "# Build keyword->preferred fragment-type map (counts)\n",
        "keyword_type_counts = defaultdict(lambda: Counter())\n",
        "for frag in all_fragments:\n",
        "    fid = frag.get(\"id\") or frag.get(\"fragmentid\")\n",
        "    typ = fragment_type_of[fid]\n",
        "    text = frag[\"fragmenttext\"].lower()\n",
        "    toks = [t for t in universal_tokenizer(text) if len(t) >= MIN_KEYWORD_LEN and t.isalnum()]\n",
        "    unique = set(toks)\n",
        "    for t in unique:\n",
        "        keyword_type_counts[t][typ] += 1\n",
        "\n",
        "# Convert counters to normalized preference weights per keyword\n",
        "keyword_type_pref = {}\n",
        "for kw, counter in keyword_type_counts.items():\n",
        "    total = sum(counter.values())\n",
        "    if total == 0:\n",
        "        continue\n",
        "    pref = {typ: cnt/total for typ, cnt in counter.items()}\n",
        "    keyword_type_pref[kw] = pref\n",
        "\n",
        "# instantiate semantic graph manager (use your existing embedder if you want)\n",
        "embedder = TokenEmbedding(vocab=[], dim=50)\n",
        "sgm = SemanticGraphManager([f[\"fragmenttext\"] for f in all_fragments], embedder=embedder)\n",
        "\n",
        "def _tokenize_for_matching(text):\n",
        "    toks = [t for t in universal_tokenizer(text.lower()) if len(t) >= MIN_KEYWORD_LEN and t.isalnum()]\n",
        "    return toks\n",
        "\n",
        "def semantic_boost_retrieve(user_query: str, k_final: int = K_FINAL, k_candidates: int = K_CANDIDATES):\n",
        "    # 1) Expand query\n",
        "    expanded = sgm.expand_query(user_query, top_k=20)\n",
        "    expanded_toks = _tokenize_for_matching(expanded)\n",
        "    # also preserve original tokens\n",
        "    orig_toks = _tokenize_for_matching(user_query)\n",
        "    all_toks = list(dict.fromkeys(orig_toks + expanded_toks))\n",
        "\n",
        "    # 2) call base retrieve with expanded query (get more candidates)\n",
        "    base_candidates = retrieve(expanded, k=k_candidates)  # returns list of dicts from your index\n",
        "    # ensure consistent format: each candidate should include fragmentid and fragmenttext and retrievalconfidence\n",
        "    candidates = []\n",
        "    for c in base_candidates:\n",
        "        # some implementations return {'fragmentid', 'fragmenttext', 'retrievalconfidence'}\n",
        "        fid = c.get(\"fragmentid\") or c.get(\"id\") or c.get(\"fragment_id\")\n",
        "        candidates.append({\n",
        "            \"id\": fid,\n",
        "            \"text\": c.get(\"fragmenttext\") or c.get(\"text\") or \"\",\n",
        "            \"base_conf\": float(c.get(\"retrievalconfidence\", 0.0))\n",
        "        })\n",
        "\n",
        "    if not candidates:\n",
        "        return []\n",
        "\n",
        "    # 3) compute semantic keyword score for each candidate\n",
        "    # weight contribution of each keyword by graph weight if available\n",
        "    graph = sgm.graph\n",
        "    kw_to_neighbors = graph  # map token -> {neighbor: weight}\n",
        "\n",
        "    sem_scores = []\n",
        "    max_raw = 1e-12\n",
        "    for cand in candidates:\n",
        "        text = cand[\"text\"].lower()\n",
        "        raw_score = 0.0\n",
        "        # count presence and weight via graph connections:\n",
        "        for tok in all_toks:\n",
        "            if tok in text:\n",
        "                # direct presence gives base boost\n",
        "                raw_score += 1.0\n",
        "            else:\n",
        "                # check neighbors in graph: if tok has neighbor n and n in text, add neighbor weight\n",
        "                neighs = kw_to_neighbors.get(tok, {})\n",
        "                if neighs:\n",
        "                    for n, w in neighs.items():\n",
        "                        if n in text:\n",
        "                            raw_score += 0.5 * float(w)\n",
        "            # also check exact bigram presence\n",
        "            if \"_\" in tok:\n",
        "                phrase = tok.replace(\"_\", \" \")\n",
        "                if phrase in text:\n",
        "                    raw_score += 0.7\n",
        "        sem_scores.append(raw_score)\n",
        "        if raw_score > max_raw:\n",
        "            max_raw = raw_score\n",
        "\n",
        "    # normalize semantic scores\n",
        "    norm_sem = [(s / (max_raw + 1e-12)) for s in sem_scores]\n",
        "\n",
        "    # 4) compute type bonus using keyword_type_pref\n",
        "    type_bonuses = []\n",
        "    for cand in candidates:\n",
        "        fid = cand[\"id\"]\n",
        "        typ = fragment_type_of.get(fid, None)\n",
        "        bonus = 0.0\n",
        "        if typ:\n",
        "            # for each token, if kw maps to a type pref, add that pref\n",
        "            for tok in all_toks:\n",
        "                pref = keyword_type_pref.get(tok)\n",
        "                if pref:\n",
        "                    bonus += pref.get(typ, 0.0)\n",
        "        # normalize later\n",
        "        type_bonuses.append(bonus)\n",
        "    # normalize type bonuses\n",
        "    max_t = max(type_bonuses) if type_bonuses else 1e-12\n",
        "    type_bonuses = [tb / (max_t + 1e-12) for tb in type_bonuses]\n",
        "\n",
        "    # 5) combine final score\n",
        "    final_list = []\n",
        "    for i, cand in enumerate(candidates):\n",
        "        base = cand[\"base_conf\"]\n",
        "        sem = norm_sem[i]\n",
        "        tbonus = type_bonuses[i]\n",
        "        final_score = ALPHA * base + BETA * sem + GAMMA * tbonus\n",
        "        final_list.append({**cand, \"semantic_score\": sem, \"type_bonus\": tbonus, \"final_score\": final_score})\n",
        "\n",
        "    # 6) sort and return top-k_final\n",
        "    final_sorted = sorted(final_list, key=lambda x: -x[\"final_score\"])[:k_final]\n",
        "    return final_sorted\n",
        "\n",
        "# Example usage:\n",
        "# results = semantic_boost_retrieve(\"I feel dryness in my skin and joints\", k_final=5)\n",
        "# for r in results:\n",
        "#     print(r['final_score'], r['text'][:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJWDJMn0Wiy0"
      },
      "outputs": [],
      "source": [
        "\n",
        "fragment_texts = [frag[\"fragmenttext\"] for frag in all_fragments]\n",
        "\n",
        "from tokenizer_and_embedding import TokenEmbedding\n",
        "embedder = TokenEmbedding(vocab=[], dim=50)\n",
        "\n",
        "#from semantic_graph import SemanticGraphManager\n",
        "sgm = SemanticGraphManager(fragment_texts, embedder=embedder, cache_path=\"semantic_graph.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dijz_qKzn50K",
        "outputId": "d160dae8-2e8e-44c8-e8f1-bc3d7b475ae8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Structured fragment counts by type: {\n",
            "  \"physical\": 1200,\n",
            "  \"skin\": 1200,\n",
            "  \"face_sensory\": 1200,\n",
            "  \"digestion\": 1200,\n",
            "  \"lifestyle\": 1200,\n",
            "  \"ayurvedic\": 1200\n",
            "}\n",
            "\n",
            "Example structured fragment (first 3):\n",
            "{\n",
            "  \"id\": \"0_0\",\n",
            "  \"type\": \"physical\",\n",
            "  \"attributes\": {\n",
            "    \"moisture\": [],\n",
            "    \"temperature\": [],\n",
            "    \"texture\": [],\n",
            "    \"sensitivity\": [],\n",
            "    \"appetite\": [],\n",
            "    \"digestion\": [\n",
            "      \"moderate_metabolism\"\n",
            "    ],\n",
            "    \"bones_joints\": [\n",
            "      \"bone\",\n",
            "      \"mobility\"\n",
            "    ],\n",
            "    \"weight\": [\n",
            "      \"gaining\",\n",
            "      \"weight\",\n",
            "      \"heavy\",\n",
            "      \"losing\"\n",
            "    ],\n",
            "    \"mood\": [],\n",
            "    \"sleep\": [],\n",
            "    \"dosha\": [\n",
            "      \"vata\"\n",
            "    ],\n",
            "    \"hair\": [\n",
            "      \"brown_dull\"\n",
            "    ],\n",
            "    \"skin_issue\": [],\n",
            "    \"taste_pref\": []\n",
            "  }\n",
            "}\n",
            "{\n",
            "  \"id\": \"0_1\",\n",
            "  \"type\": \"physical\",\n",
            "  \"attributes\": {\n",
            "    \"moisture\": [\n",
            "      \"oily\",\n",
            "      \"dry\",\n",
            "      \"dryness\"\n",
            "    ],\n",
            "    \"temperature\": [\n",
            "      \"cool\"\n",
            "    ],\n",
            "    \"texture\": [\n",
            "      \"rough\",\n",
            "      \"thin\"\n",
            "    ],\n",
            "    \"sensitivity\": [\n",
            "      \"sensitivity\"\n",
            "    ],\n",
            "    \"appetite\": [],\n",
            "    \"digestion\": [],\n",
            "    \"bones_joints\": [],\n",
            "    \"weight\": [],\n",
            "    \"mood\": [],\n",
            "    \"sleep\": [],\n",
            "    \"dosha\": [\n",
            "      \"vata\",\n",
            "      \"pitta\"\n",
            "    ],\n",
            "    \"hair\": [\n",
            "      \"hair\",\n",
            "      \"dull\",\n",
            "      \"thin\",\n",
            "      \"straight\",\n",
            "      \"brown\"\n",
            "    ],\n",
            "    \"skin_issue\": [\n",
            "      \"aging\",\n",
            "      \"pigments\"\n",
            "    ],\n",
            "    \"taste_pref\": []\n",
            "  }\n",
            "}\n",
            "{\n",
            "  \"id\": \"0_2\",\n",
            "  \"type\": \"physical\",\n",
            "  \"attributes\": {\n",
            "    \"moisture\": [\n",
            "      \"oily\",\n",
            "      \"dry\",\n",
            "      \"chaps\"\n",
            "    ],\n",
            "    \"temperature\": [],\n",
            "    \"texture\": [\n",
            "      \"smooth\",\n",
            "      \"thin\",\n",
            "      \"polished\",\n",
            "      \"thick\"\n",
            "    ],\n",
            "    \"sensitivity\": [\n",
            "      \"sensitive\"\n",
            "    ],\n",
            "    \"appetite\": [\n",
            "      \"strong\"\n",
            "    ],\n",
            "    \"digestion\": [\n",
            "      \"strong\"\n",
            "    ],\n",
            "    \"bones_joints\": [\n",
            "      \"medium_bone\"\n",
            "    ],\n",
            "    \"weight\": [],\n",
            "    \"mood\": [],\n",
            "    \"sleep\": [],\n",
            "    \"dosha\": [\n",
            "      \"vata\",\n",
            "      \"pitta\"\n",
            "    ],\n",
            "    \"hair\": [\n",
            "      \"light\",\n",
            "      \"thin\"\n",
            "    ],\n",
            "    \"skin_issue\": [\n",
            "      \"pigments\",\n",
            "      \"aging\"\n",
            "    ],\n",
            "    \"taste_pref\": []\n",
            "  }\n",
            "}\n",
            "\n",
            "Fragments with at least one extracted attribute: 7200 out of 7200\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ===== REBUILD: Robust Structured Fragments & Index (single cell) =====\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "import json\n",
        "from tokenizer_and_embedding import universal_tokenizer, TokenEmbedding\n",
        "#from semantic_graph import SemanticGraphManager\n",
        "\n",
        "# ---------- 1) Improved type inference for IDs like 'frag150' ----------\n",
        "def infer_frag_type(fid):\n",
        "    # extract first number from id\n",
        "    m = re.search(r'\\d+', str(fid))\n",
        "    idx = int(m.group(0)) if m else 0\n",
        "    mapping = {0: \"physical\", 1: \"skin\", 2: \"face_sensory\", 3: \"digestion\", 4: \"lifestyle\", 5: \"ayurvedic\"}\n",
        "    return mapping[idx % 6]\n",
        "\n",
        "# ---------- 2) Expanded, practical ATTR_KEYWORDS ----------\n",
        "ATTR_KEYWORDS = {\n",
        "    \"moisture\": [\"dry\", \"dryness\", \"moist\", \"moisture\", \"oily\", \"greasy\", \"chapped\", \"chaps\", \"cracked\", \"cracking\", \"hydrated\",\"moist/greasy\", \"oily\", \"t-zone\"],\n",
        "    \"temperature\": [\"warm\", \"hot\", \"heat\", \"cold\", \"cool\", \"chill\", \"sweat\", \"sweating\", \"sweaty\"],\n",
        "    \"texture\": [\"smooth\", \"rough\", \"roughness\", \"wrinkled\", \"polished\", \"pigment\", \"pigmented\", \"freckle\", \"freckles\", \"mole\", \"moles\",\"thick\", \"thin\", \"fine\"],\n",
        "    \"sensitivity\": [\"sensitive\", \"sensitivity\", \"irritated\", \"irritation\", \"red\", \"redness\", \"rash\", \"rashes\", \"allergy\", \"prone\"],\n",
        "    \"appetite\": [\"appetite\", \"hungry\", \"hunger\", \"crave\", \"craving\", \"slow\", \"irregular\", \"strong\", \"weak\", \"unbearable\"],\n",
        "    \"digestion\": [\"digestion\", \"digest\", \"metabolism\", \"bloat\", \"constipation\", \"acid\", \"acidity\", \"gas\", \"weak\", \"fast\"],\n",
        "    \"bones_joints\": [\"joint\", \"joints\", \"bone\", \"bones\", \"prominent\", \"prominence\", \"stiff\", \"stiffness\", \"crack\", \"cracking\", \"mobility\", \"pain\"],\n",
        "    \"weight\": [\"heavy\", \"gain\", \"gaining\", \"lose\", \"losing\", \"underweight\", \"overweight\", \"weight\"],\n",
        "    \"mood\": [\"stress\", \"stressed\", \"anxious\", \"anxiety\", \"irritable\", \"irritability\", \"angry\", \"calm\", \"depressed\"],\n",
        "    \"sleep\": [\"sleep\", \"insomnia\", \"sleeps\", \"sleepy\", \"wake\", \"wakes\", \"nap\", \"awake\"],\n",
        "    \"dosha\": [\"vata\", \"pitta\", \"kapha\", \"dosha\"],\n",
        "    \"hair\": [\"hair\", \"brittle\", \"dull\", \"lustrous\", \"curly\", \"straight\", \"thin\", \"thinning\",\"curly\", \"straight\", \"brown\", \"light\", \"dark\"],\n",
        "    \"skin_issue\": [\"acne\", \"eczema\", \"psoriasis\",\"rashes\", \"redness\", \"aging\", \"pigments\"],\n",
        "    \"taste_pref\": [\"sweet\", \"sour\", \"salty\", \"bitter\", \"pungent\", \"astringent\"],\n",
        "}\n",
        "\n",
        "# ---------- 3) Prepare semantic graph manager for neighbor checks ----------\n",
        "fragment_texts = [f[\"fragmenttext\"] for f in all_fragments]\n",
        "embedder_for_sgm = TokenEmbedding(vocab=[], dim=50)\n",
        "sgm = SemanticGraphManager(fragment_texts, embedder=embedder_for_sgm, cache_path=\"semantic_graph.pkl\")\n",
        "\n",
        "# ---------- 4) Helper tokenization and substring matching ----------\n",
        "def text_tokens(text):\n",
        "    tokens = []\n",
        "    for t in universal_tokenizer(text.lower()):\n",
        "        if re.match(r\"[a-zA-Z0-9_/.-]+$\", t):\n",
        "            tokens.append(t)\n",
        "    return tokens\n",
        "\n",
        "def substring_present(token, text):\n",
        "    # token may be multiword (with underscore representing bigram)\n",
        "    token = token.replace(\"_\", \" \")\n",
        "    return token in text.lower()\n",
        "\n",
        "# ---------- 5) More robust attribute extractor ----------\n",
        "def extract_attributes_from_text(text):\n",
        "    t = str(text).lower()\n",
        "    toks = set(text_tokens(t))\n",
        "    attrs = {}\n",
        "    for attr, kws in ATTR_KEYWORDS.items():\n",
        "        found = set()\n",
        "        # direct token match\n",
        "        for kw in kws:\n",
        "            if kw.isalnum():\n",
        "                if kw in toks:\n",
        "                    found.add(kw)\n",
        "            else:\n",
        "                # still treat hyphen/space variants: check substring\n",
        "                if kw in t:\n",
        "                    found.add(kw)\n",
        "        # substring patterns (catch 'cracking', 'prominent joints', etc.)\n",
        "        # specific regex checks\n",
        "        if not found:\n",
        "            # joint / crack patterns\n",
        "            if attr == \"bones_joints\":\n",
        "                if re.search(r'\\b(crack|cracking|joint|joints|stiff|stiffness|mobility|pain)\\b', t):\n",
        "                    for m in re.findall(r'\\b(crack|cracking|joint|joints|stiff|stiffness|mobility|pain)\\b', t):\n",
        "                        found.add(m)\n",
        "            # skin texture\n",
        "            if attr == \"texture\":\n",
        "                if re.search(r'\\b(freckle|freckles|mole|moles|wrinkl|polish|pigment)\\b', t):\n",
        "                    for m in re.findall(r'\\b(freckle|freckles|mole|moles|wrinkl|polish|pigment)\\b', t):\n",
        "                        found.add(m)\n",
        "            # digestion/appetite\n",
        "            if attr in (\"digestion\", \"appetite\"):\n",
        "                if re.search(r'\\b(digest|digestion|metabolism|hungr|appetit|bloat|constip|acid|acidity|weak|strong|fast)\\b', t):\n",
        "                    for m in re.findall(r'\\b(digest|digestion|metabolism|hungr|appetit|bloat|constip|acid|acidity|weak|strong|fast)\\b', t):\n",
        "                        found.add(m)\n",
        "            # moisture/temperature/sensitivity via substring\n",
        "            if attr in (\"moisture\",\"temperature\",\"sensitivity\"):\n",
        "                for kw in kws:\n",
        "                    if substring_present(kw, t):\n",
        "                        found.add(kw)\n",
        "        # neighbor expansion: check a few semantic neighbors and map them back\n",
        "        if not found:\n",
        "            # get a few neighbors for tokens in the fragment (speed limit)\n",
        "            for tok in list(toks)[:12]:\n",
        "                try:\n",
        "                    neighs = sgm.expand_query_detail(tok, top_k=6).get(tok, [])\n",
        "                except Exception:\n",
        "                    neighs = []\n",
        "                for n in neighs:\n",
        "                    # if neighbor matches any keyword for this attr -> add\n",
        "                    if any(n.startswith(k) or k in n for k in kws):\n",
        "                        found.add(n)\n",
        "                        if len(found) >= 4:\n",
        "                            break\n",
        "                if len(found) >= 4:\n",
        "                    break\n",
        "\n",
        "        attrs[attr] = list(found)\n",
        "    return attrs\n",
        "\n",
        "# ---------- 6) Rebuild structured_fragments with correct types & attributes ----------\n",
        "structured_fragments = []\n",
        "for frag in all_fragments:\n",
        "    fid = frag.get(\"id\") or frag.get(\"fragmentid\") or frag.get(\"fragment_id\")\n",
        "    text = frag[\"fragmenttext\"]\n",
        "    ftype = infer_frag_type(fid)\n",
        "    attrs = extract_attributes_from_text(text)\n",
        "    structured_fragments.append({\n",
        "        \"id\": fid,\n",
        "        \"type\": ftype,\n",
        "        \"text\": text,\n",
        "        \"attributes\": attrs\n",
        "    })\n",
        "\n",
        "# ---------- 7) Build structured indices ----------\n",
        "frags_by_type = defaultdict(list)\n",
        "attr_index = defaultdict(lambda: defaultdict(set))\n",
        "frag_by_id = {}\n",
        "\n",
        "for f in structured_fragments:\n",
        "    fid = f[\"id\"]\n",
        "    frag_by_id[fid] = f\n",
        "    frags_by_type[f[\"type\"]].append(f)\n",
        "    for attr, vals in f[\"attributes\"].items():\n",
        "        for v in vals:\n",
        "            attr_index[attr][v].add(fid)\n",
        "\n",
        "# ---------- 8) Print sanity stats ----------\n",
        "type_counts = {t: len(lst) for t, lst in frags_by_type.items()}\n",
        "print(\"Structured fragment counts by type:\", json.dumps(type_counts, indent=2))\n",
        "\n",
        "# Print few examples for verification\n",
        "print(\"\\nExample structured fragment (first 3):\")\n",
        "for ex in structured_fragments[:3]:\n",
        "    print(json.dumps({\"id\": ex[\"id\"], \"type\": ex[\"type\"], \"attributes\": ex[\"attributes\"]}, indent=2))\n",
        "\n",
        "# quick check: any fragment with non-empty attributes?\n",
        "nonempty = sum(1 for f in structured_fragments if any(len(vals)>0 for vals in f[\"attributes\"].values()))\n",
        "print(\"\\nFragments with at least one extracted attribute:\", nonempty, \"out of\", len(structured_fragments))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igDMymgQhUPH"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ================== STRUCTURED RETRIEVE (FINAL VERSION — CLEAN & STABLE) ==================\n",
        "\n",
        "from collections import Counter\n",
        "from embedding_matching import retrieve as base_retrieve\n",
        "from tokenizer_and_embedding import universal_tokenizer\n",
        "\n",
        "# Tunable weights\n",
        "W_BASE = 0.50    # embedding score weight\n",
        "W_ATTR = 0.40    # attribute match weight\n",
        "W_TYPE = 0.10    # fragment type bias weight\n",
        "\n",
        "\n",
        "def structured_retrieve(query, k=5, k_candidates=40, debug=False):\n",
        "    # ---------------------------------------------------------------\n",
        "    # 1) Expand query using semantic graph\n",
        "    # ---------------------------------------------------------------\n",
        "    expanded = sgm.expand_query(query, top_k=18)\n",
        "\n",
        "    orig_toks = [t for t in universal_tokenizer(query.lower()) if t.isalnum()]\n",
        "    exp_toks  = [t for t in universal_tokenizer(expanded.lower()) if t.isalnum()]\n",
        "\n",
        "    toks = list(dict.fromkeys(orig_toks + exp_toks))   # dedupe, preserve order\n",
        "\n",
        "\n",
        "    # ---------------------------------------------------------------\n",
        "    # 2) Determine preferred categories from symptoms\n",
        "    # ---------------------------------------------------------------\n",
        "    pref_counter = Counter()\n",
        "\n",
        "    for tok in toks:\n",
        "        for key, types in SYMPTOM_TYPE_MAP.items():\n",
        "            if tok.startswith(key):\n",
        "                for t in types:\n",
        "                    pref_counter[t] += 1\n",
        "\n",
        "    if not pref_counter:\n",
        "        # fallback: treat all categories equal\n",
        "        pref_counter = Counter({t: 1 for t in frags_by_type.keys()})\n",
        "\n",
        "    max_pref = max(pref_counter.values())\n",
        "    type_bonus_map = {t: pref_counter[t] / max_pref for t in frags_by_type.keys()}\n",
        "\n",
        "\n",
        "    # ---------------------------------------------------------------\n",
        "    # 3) Base retrieve (expanded query)\n",
        "    # ---------------------------------------------------------------\n",
        "    base_candidates = base_retrieve(expanded, k=k_candidates)\n",
        "\n",
        "    candidates = []\n",
        "    for c in base_candidates:\n",
        "        fid  = c.get(\"fragmentid\") or c.get(\"id\")\n",
        "        text = c.get(\"fragmenttext\") or c.get(\"text\") or \"\"\n",
        "        base_conf = float(c.get(\"retrievalconfidence\", 0.0))\n",
        "        candidates.append({\"id\": fid, \"text\": text, \"base_conf\": base_conf})\n",
        "\n",
        "\n",
        "    # ---------------------------------------------------------------\n",
        "    # 4) Attribute matching score\n",
        "    # ---------------------------------------------------------------\n",
        "    raw_attr_scores = []\n",
        "    matched_details = []\n",
        "\n",
        "    for cand in candidates:\n",
        "        fid = cand[\"id\"]\n",
        "        frag = frag_by_id.get(fid)\n",
        "        if not frag:\n",
        "            raw_attr_scores.append(0.0)\n",
        "            matched_details.append({})\n",
        "            continue\n",
        "\n",
        "        frag_attrs = frag[\"attributes\"]\n",
        "        score = 0.0\n",
        "        matched = {}\n",
        "\n",
        "        # direct seed match\n",
        "        for tok in toks:\n",
        "            for attr, seeds in ATTR_KEYWORDS.items():\n",
        "                if tok in seeds or tok in frag_attrs.get(attr, []):\n",
        "                    score += 1.0\n",
        "                    matched.setdefault(attr, []).append(tok)\n",
        "\n",
        "        # semantic neighbor match\n",
        "        for tok in toks[:12]:\n",
        "            neighs = []\n",
        "            try:\n",
        "                neighs = sgm.expand_query_detail(tok, top_k=6).get(tok, [])\n",
        "            except:\n",
        "                neighs = []\n",
        "\n",
        "            for n in neighs:\n",
        "                for attr in frag_attrs:\n",
        "                    if n in frag_attrs[attr]:\n",
        "                        score += 0.6\n",
        "                        matched.setdefault(attr, []).append(n)\n",
        "\n",
        "        # --- FINAL CLEANUP (DEDUP & PRESERVE ORDER) ---\n",
        "        for attr in matched:\n",
        "            matched[attr] = list(dict.fromkeys(matched[attr]))\n",
        "\n",
        "        denom = max(1, len(toks))\n",
        "        raw_attr_scores.append(score / denom)\n",
        "        matched_details.append(matched)\n",
        "\n",
        "    max_attr = max(raw_attr_scores) if raw_attr_scores else 1.0\n",
        "    norm_attr_scores = [x / (max_attr + 1e-12) for x in raw_attr_scores]\n",
        "\n",
        "\n",
        "    # ---------------------------------------------------------------\n",
        "    # 5) Combine base + attr + type bonus\n",
        "    # ---------------------------------------------------------------\n",
        "    results = []\n",
        "\n",
        "    for i, cand in enumerate(candidates):\n",
        "        fid  = cand[\"id\"]\n",
        "        base = cand[\"base_conf\"]\n",
        "\n",
        "        frag = frag_by_id.get(fid)\n",
        "        typ  = frag[\"type\"] if frag else None\n",
        "\n",
        "        attr_score = norm_attr_scores[i]\n",
        "        type_bonus = type_bonus_map.get(typ, 0.0)\n",
        "\n",
        "        final_score = (\n",
        "            W_BASE * base +\n",
        "            W_ATTR * attr_score +\n",
        "            W_TYPE * type_bonus\n",
        "        )\n",
        "\n",
        "        results.append({\n",
        "            \"id\": fid,\n",
        "            \"text\": cand[\"text\"],\n",
        "            \"type\": typ,\n",
        "            \"base_conf\": base,\n",
        "            \"attr_score\": attr_score,\n",
        "            \"type_bonus\": type_bonus,\n",
        "            \"final_score\": final_score,\n",
        "            \"matched_attrs\": matched_details[i]\n",
        "        })\n",
        "\n",
        "\n",
        "    # ---------------------------------------------------------------\n",
        "    # 6) Sort normally\n",
        "    # ---------------------------------------------------------------\n",
        "    results = sorted(results, key=lambda x: -x[\"final_score\"])\n",
        "\n",
        "    # ================= OPTION A — CLEANING & DEDUP & TYPE DIVERSITY =================\n",
        "    cleaned = []\n",
        "    seen_texts = set()\n",
        "    seen_types = set()\n",
        "\n",
        "    def is_near_duplicate(t1, t2):\n",
        "        s1 = set(t1.lower().split())\n",
        "        s2 = set(t2.lower().split())\n",
        "        jacc = len(s1 & s2) / (len(s1 | s2) + 1e-6)\n",
        "        return jacc > 0.90\n",
        "\n",
        "    for item in results:\n",
        "        text = item[\"text\"].strip()\n",
        "        typ = item[\"type\"]\n",
        "\n",
        "        if text in seen_texts:\n",
        "            continue\n",
        "        if any(is_near_duplicate(text, c[\"text\"]) for c in cleaned):\n",
        "            continue\n",
        "        if typ in seen_types:\n",
        "            continue\n",
        "\n",
        "        cleaned.append(item)\n",
        "        seen_texts.add(text)\n",
        "        seen_types.add(typ)\n",
        "\n",
        "        if len(cleaned) == k:\n",
        "            break\n",
        "\n",
        "    # If debug: return full info\n",
        "    if debug:\n",
        "        return {\n",
        "            \"expanded\": expanded,\n",
        "            \"tokens\": toks,\n",
        "            \"preferred_category\": pref_counter.most_common(1)[0][0],\n",
        "            \"results\": cleaned\n",
        "        }\n",
        "\n",
        "    return cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px3oHMNHa08j"
      },
      "outputs": [],
      "source": [
        "\n",
        "def show_test(q):\n",
        "    out = structured_retrieve(q, k=6, k_candidates=60, debug=True)\n",
        "    print(\"\\nQUERY:\", q)\n",
        "    print(\"Expanded:\", out[\"expanded\"])\n",
        "    print(\"Tokens:\", out[\"tokens\"])\n",
        "    print(\"\\nTop results:\\n\")\n",
        "    for i, r in enumerate(out[\"results\"], 1):\n",
        "        print(f\"Rank {i} final={r['final_score']:.4f} base={r['base_conf']:.3f} \"\n",
        "              f\"attr={r['attr_score']:.3f} type={r['type']} tb={r['type_bonus']:.3f}\")\n",
        "        print(\"Matched attributes:\", r[\"matched_attrs\"])\n",
        "        print(r[\"text\"][:250], \"...\\n\")  # Print first 250 chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9nXl9RobR7c"
      },
      "outputs": [],
      "source": [
        "SYMPTOM_TYPE_MAP = {\n",
        "    \"dry\": [\"skin\"],\n",
        "    \"dryness\": [\"skin\"],\n",
        "    \"rough\": [\"skin\"],\n",
        "    \"itch\": [\"skin\"],\n",
        "    \"heat\": [\"ayurvedic\", \"skin\"],\n",
        "    \"irritat\": [\"ayurvedic\", \"lifestyle\"],\n",
        "    \"heavy\": [\"physical\", \"lifestyle\"],\n",
        "    \"lazy\": [\"lifestyle\", \"physical\"],\n",
        "    \"hungry\": [\"digestion\"],\n",
        "    \"digest\": [\"digestion\"],\n",
        "    \"joint\": [\"physical\"],\n",
        "    \"joints\": [\"physical\"],\n",
        "    \"crack\": [\"physical\"],\n",
        "    \"cracking\": [\"physical\"],\n",
        "    \"bone\": [\"physical\"],\n",
        "    \"weight\": [\"physical\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_aXgjlEYleJ",
        "outputId": "875b4b5e-8b00-49f2-9bb9-048b277c8af7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "QUERY: I feel dryness in my skin and joints\n",
            "Expanded: I feel dryness in my skin and joints pungent healthy moist kapha glowing beautiful pigments aging freckles many moles redness rashes curly the_skin skin_is is_described described_as\n",
            "Tokens: ['i', 'feel', 'dryness', 'in', 'my', 'skin', 'and', 'joints', 'pungent', 'healthy', 'moist', 'kapha', 'glowing', 'beautiful', 'pigments', 'aging', 'freckles', 'many', 'moles', 'redness', 'rashes', 'curly', 'the', 'is', 'described', 'as']\n",
            "\n",
            "Top results:\n",
            "\n",
            "Rank 1 final=0.9781 base=0.956 attr=1.000 type=physical tb=1.000\n",
            "Matched attributes: {'moisture': ['dryness', 'moist'], 'bones_joints': ['joints'], 'taste_pref': ['pungent'], 'dosha': ['kapha'], 'texture': ['pigments', 'freckles', 'moles'], 'skin_issue': ['pigments', 'aging', 'redness', 'rashes'], 'sensitivity': ['redness', 'rashes'], 'hair': ['curly']}\n",
            "The dominant dosha is vata+kapha. Climate preference is cool. Skin sensitivity is insensitive. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference. ...\n",
            "\n",
            "Rank 2 final=0.9481 base=0.956 attr=0.925 type=skin tb=1.000\n",
            "Matched attributes: {'moisture': ['dryness', 'moist'], 'bones_joints': ['joints'], 'taste_pref': ['pungent'], 'dosha': ['kapha'], 'texture': ['pigments', 'freckles', 'moles'], 'skin_issue': ['pigments', 'aging', 'redness', 'rashes'], 'sensitivity': ['redness', 'rashes'], 'hair': ['curly']}\n",
            "The dominant dosha is vata+pitta. Climate preference is cool. Skin sensitivity is normal. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference. ...\n",
            "\n",
            "Rank 3 final=0.8781 base=0.956 attr=1.000 type=lifestyle tb=0.000\n",
            "Matched attributes: {'moisture': ['dryness', 'moist'], 'bones_joints': ['joints'], 'taste_pref': ['pungent'], 'dosha': ['kapha'], 'texture': ['pigments', 'freckles', 'moles'], 'skin_issue': ['pigments', 'aging', 'redness', 'rashes'], 'sensitivity': ['redness', 'rashes'], 'hair': ['curly']}\n",
            "The dominant dosha is vata+kapha. Climate preference is warm. Skin sensitivity is normal. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference. ...\n",
            "\n",
            "Rank 4 final=0.8781 base=0.956 attr=1.000 type=face_sensory tb=0.000\n",
            "Matched attributes: {'moisture': ['dryness', 'moist'], 'bones_joints': ['joints'], 'taste_pref': ['pungent'], 'dosha': ['kapha'], 'texture': ['pigments', 'freckles', 'moles'], 'skin_issue': ['pigments', 'aging', 'redness', 'rashes'], 'sensitivity': ['redness', 'rashes'], 'hair': ['curly']}\n",
            "The dominant dosha is pitta+kapha. Climate preference is moderate. Skin sensitivity is normal. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference. ...\n",
            "\n",
            "Rank 5 final=0.8481 base=0.956 attr=0.925 type=digestion tb=0.000\n",
            "Matched attributes: {'moisture': ['dryness', 'moist'], 'bones_joints': ['joints'], 'taste_pref': ['pungent'], 'dosha': ['kapha'], 'texture': ['pigments', 'freckles', 'moles'], 'skin_issue': ['pigments', 'aging', 'redness', 'rashes'], 'sensitivity': ['redness', 'rashes'], 'hair': ['curly']}\n",
            "The dominant dosha is vata+pitta. Climate preference is warm. Skin sensitivity is sensitive. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference. ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "show_test(\"I feel dryness in my skin and joints\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kxJMlpUeAzQ",
        "outputId": "b0792c64-2f22-4bea-cd83-e5d6159b0d98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "QUERY: I feel dryness in my skin and joints\n",
            "Expanded: I feel dryness in my skin and joints pungent healthy moist kapha glowing beautiful pigments aging freckles many moles redness rashes curly the_skin skin_is is_described described_as\n",
            "Tokens: ['i', 'feel', 'dryness', 'in', 'my', 'skin', 'and', 'joints', 'pungent', 'healthy', 'moist', 'kapha', 'glowing', 'beautiful', 'pigments', 'aging', 'freckles', 'many', 'moles', 'redness', 'rashes', 'curly', 'the', 'is', 'described', 'as']\n",
            "\n",
            "Top results:\n",
            "\n",
            "Rank 1 final=0.9781 base=0.956 attr=1.000 type=physical tb=1.000\n",
            "Matched attributes: {'moisture': ['dryness', 'moist'], 'bones_joints': ['joints'], 'taste_pref': ['pungent'], 'dosha': ['kapha'], 'texture': ['pigments', 'freckles', 'moles'], 'skin_issue': ['pigments', 'aging', 'redness', 'rashes'], 'sensitivity': ['redness', 'rashes'], 'hair': ['curly']}\n",
            "The dominant dosha is vata+kapha. Climate preference is cool. Skin sensitivity is insensitive. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n",
            "\n",
            "Rank 2 final=0.9481 base=0.956 attr=0.925 type=skin tb=1.000\n",
            "Matched attributes: {'moisture': ['dryness', 'moist'], 'bones_joints': ['joints'], 'taste_pref': ['pungent'], 'dosha': ['kapha'], 'texture': ['pigments', 'freckles', 'moles'], 'skin_issue': ['pigments', 'aging', 'redness', 'rashes'], 'sensitivity': ['redness', 'rashes'], 'hair': ['curly']}\n",
            "The dominant dosha is vata+pitta. Climate preference is cool. Skin sensitivity is normal. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n",
            "\n",
            "Rank 3 final=0.8781 base=0.956 attr=1.000 type=lifestyle tb=0.000\n",
            "Matched attributes: {'moisture': ['dryness', 'moist'], 'bones_joints': ['joints'], 'taste_pref': ['pungent'], 'dosha': ['kapha'], 'texture': ['pigments', 'freckles', 'moles'], 'skin_issue': ['pigments', 'aging', 'redness', 'rashes'], 'sensitivity': ['redness', 'rashes'], 'hair': ['curly']}\n",
            "The dominant dosha is vata+kapha. Climate preference is warm. Skin sensitivity is normal. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n",
            "\n",
            "Rank 4 final=0.8781 base=0.956 attr=1.000 type=face_sensory tb=0.000\n",
            "Matched attributes: {'moisture': ['dryness', 'moist'], 'bones_joints': ['joints'], 'taste_pref': ['pungent'], 'dosha': ['kapha'], 'texture': ['pigments', 'freckles', 'moles'], 'skin_issue': ['pigments', 'aging', 'redness', 'rashes'], 'sensitivity': ['redness', 'rashes'], 'hair': ['curly']}\n",
            "The dominant dosha is pitta+kapha. Climate preference is moderate. Skin sensitivity is normal. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n",
            "\n",
            "Rank 5 final=0.8481 base=0.956 attr=0.925 type=digestion tb=0.000\n",
            "Matched attributes: {'moisture': ['dryness', 'moist'], 'bones_joints': ['joints'], 'taste_pref': ['pungent'], 'dosha': ['kapha'], 'texture': ['pigments', 'freckles', 'moles'], 'skin_issue': ['pigments', 'aging', 'redness', 'rashes'], 'sensitivity': ['redness', 'rashes'], 'hair': ['curly']}\n",
            "The dominant dosha is vata+pitta. Climate preference is warm. Skin sensitivity is sensitive. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n",
            "\n",
            "QUERY: I feel heat in my body and get irritated easily\n",
            "Expanded: I feel heat in my body and get irritated easily pungent healthy moist kapha glowing beautiful features individual has body_size size difficulties_in difficulties weight_weight weight height_and height bone_structure\n",
            "Tokens: ['i', 'feel', 'heat', 'in', 'my', 'body', 'and', 'get', 'irritated', 'easily', 'pungent', 'healthy', 'moist', 'kapha', 'glowing', 'beautiful', 'features', 'individual', 'has', 'size', 'difficulties', 'weight', 'height', 'bone', 'structure']\n",
            "\n",
            "Top results:\n",
            "\n",
            "Rank 1 final=0.9781 base=0.956 attr=1.000 type=physical tb=1.000\n",
            "Matched attributes: {'temperature': ['heat'], 'sensitivity': ['irritated'], 'taste_pref': ['pungent'], 'moisture': ['moist'], 'dosha': ['kapha'], 'weight': ['weight'], 'bones_joints': ['bone']}\n",
            "The dominant dosha is Kapha. Climate preference is warm. Skin sensitivity is sensitive. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n",
            "\n",
            "Rank 2 final=0.9281 base=0.956 attr=1.000 type=lifestyle tb=0.500\n",
            "Matched attributes: {'temperature': ['heat'], 'sensitivity': ['irritated'], 'taste_pref': ['pungent'], 'moisture': ['moist'], 'dosha': ['kapha'], 'weight': ['weight'], 'bones_joints': ['bone']}\n",
            "The dominant dosha is Kapha. Climate preference is moderate. Skin sensitivity is insensitive. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n",
            "\n",
            "Rank 3 final=0.9281 base=0.956 attr=1.000 type=skin tb=0.500\n",
            "Matched attributes: {'temperature': ['heat'], 'sensitivity': ['irritated'], 'taste_pref': ['pungent'], 'moisture': ['moist'], 'dosha': ['kapha'], 'weight': ['weight'], 'bones_joints': ['bone']}\n",
            "The dominant dosha is Kapha. Climate preference is cool. Skin sensitivity is normal. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n",
            "\n",
            "Rank 4 final=0.8972 base=0.956 attr=0.798 type=ayurvedic tb=1.000\n",
            "Matched attributes: {'temperature': ['heat'], 'sensitivity': ['irritated'], 'taste_pref': ['pungent'], 'moisture': ['moist'], 'dosha': ['kapha'], 'weight': ['weight'], 'bones_joints': ['bone']}\n",
            "The dominant dosha is Vata. Climate preference is cool. Skin sensitivity is insensitive. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n",
            "\n",
            "Rank 5 final=0.7972 base=0.956 attr=0.798 type=face_sensory tb=0.000\n",
            "Matched attributes: {'temperature': ['heat'], 'sensitivity': ['irritated'], 'taste_pref': ['pungent'], 'moisture': ['moist'], 'dosha': ['kapha'], 'weight': ['weight'], 'bones_joints': ['bone']}\n",
            "The dominant dosha is Pitta. Climate preference is warm. Skin sensitivity is normal. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n",
            "\n",
            "QUERY: I feel heavy and lazy\n",
            "Expanded: I feel heavy and lazy pungent healthy moist kapha glowing beautiful broad shoulders tall pale large losing_weight losing structure_bone tans_easily tans white individual\n",
            "Tokens: ['i', 'feel', 'heavy', 'and', 'lazy', 'pungent', 'healthy', 'moist', 'kapha', 'glowing', 'beautiful', 'broad', 'shoulders', 'tall', 'pale', 'large', 'losing', 'weight', 'structure', 'bone', 'tans', 'easily', 'white', 'individual']\n",
            "\n",
            "Top results:\n",
            "\n",
            "Rank 1 final=0.9781 base=0.956 attr=1.000 type=physical tb=1.000\n",
            "Matched attributes: {'weight': ['heavy', 'losing', 'weight'], 'taste_pref': ['pungent'], 'moisture': ['moist'], 'dosha': ['kapha'], 'bones_joints': ['bone'], 'texture': ['freckles']}\n",
            "The dominant dosha is pitta+kapha. Climate preference is warm. Skin sensitivity is normal. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n",
            "\n",
            "Rank 2 final=0.9281 base=0.956 attr=1.000 type=lifestyle tb=0.500\n",
            "Matched attributes: {'weight': ['heavy', 'losing', 'weight'], 'taste_pref': ['pungent'], 'moisture': ['moist'], 'dosha': ['kapha'], 'bones_joints': ['bone'], 'texture': ['freckles']}\n",
            "The dominant dosha is vata+kapha. Climate preference is cool. Skin sensitivity is sensitive. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n",
            "\n",
            "Rank 3 final=0.8227 base=0.956 attr=0.862 type=skin tb=0.000\n",
            "Matched attributes: {'weight': ['heavy', 'losing', 'weight'], 'taste_pref': ['pungent'], 'moisture': ['moist'], 'dosha': ['kapha'], 'bones_joints': ['bone'], 'texture': ['freckles']}\n",
            "The dominant dosha is vata+pitta. Climate preference is cool. Skin sensitivity is normal. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n",
            "\n",
            "Rank 4 final=0.8227 base=0.956 attr=0.862 type=digestion tb=0.000\n",
            "Matched attributes: {'weight': ['heavy', 'losing', 'weight'], 'taste_pref': ['pungent'], 'moisture': ['moist'], 'dosha': ['kapha'], 'bones_joints': ['bone'], 'texture': ['freckles']}\n",
            "The dominant dosha is vata+pitta. Climate preference is moderate. Skin sensitivity is insensitive. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n",
            "\n",
            "Rank 5 final=0.8043 base=0.956 attr=0.815 type=face_sensory tb=0.000\n",
            "Matched attributes: {'weight': ['heavy', 'losing', 'weight'], 'taste_pref': ['pungent', 'bitter', 'astringent'], 'moisture': ['moist'], 'dosha': ['kapha'], 'bones_joints': ['bone'], 'sensitivity': ['insensitive']}\n",
            "Appetite is Strong, Unbearable and digestion quality is moderate. Metabolism type is moderate, and liking tastes include Sweet / Bitter / Astringent. These patterns indicate underlying dosha behavior affecting digestion and metabolism.\n",
            "\n",
            "Rank 6 final=0.7858 base=0.956 attr=0.769 type=ayurvedic tb=0.000\n",
            "Matched attributes: {'weight': ['heavy', 'losing', 'weight'], 'taste_pref': ['pungent'], 'moisture': ['moist'], 'dosha': ['kapha'], 'bones_joints': ['bone'], 'appetite': ['slow'], 'sensitivity': ['insensitive']}\n",
            "Appetite is Slow but steady and digestion quality is moderate. Metabolism type is moderate, and liking tastes include Sweet / Sour / Salty. These patterns indicate underlying dosha behavior affecting digestion and metabolism.\n",
            "\n",
            "QUERY: I digest food fast and feel hungry often\n",
            "Expanded: I digest food fast and feel hungry often pungent healthy moist kapha glowing beautiful weak bitter astringent unbearable but steady appetite_is appetite and_digestion digestion_quality digestion quality_is\n",
            "Tokens: ['i', 'digest', 'food', 'fast', 'and', 'feel', 'hungry', 'often', 'pungent', 'healthy', 'moist', 'kapha', 'glowing', 'beautiful', 'weak', 'bitter', 'astringent', 'unbearable', 'but', 'steady', 'appetite', 'is', 'digestion', 'quality']\n",
            "\n",
            "Top results:\n",
            "\n",
            "Rank 1 final=0.9781 base=0.956 attr=1.000 type=digestion tb=1.000\n",
            "Matched attributes: {'digestion': ['digest', 'fast', 'weak', 'digestion'], 'appetite': ['hungry', 'weak', 'unbearable', 'appetite'], 'taste_pref': ['pungent', 'bitter', 'astringent'], 'moisture': ['moist'], 'dosha': ['kapha'], 'texture': ['freckles']}\n",
            "The dominant dosha is vata+kapha. Climate preference is warm. Skin sensitivity is sensitive. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n",
            "\n",
            "Rank 2 final=0.8781 base=0.956 attr=1.000 type=physical tb=0.000\n",
            "Matched attributes: {'digestion': ['digest', 'fast', 'weak', 'digestion'], 'appetite': ['hungry', 'weak', 'unbearable', 'appetite'], 'taste_pref': ['pungent', 'bitter', 'astringent'], 'moisture': ['moist'], 'dosha': ['kapha'], 'texture': ['freckles']}\n",
            "The dominant dosha is vata+kapha. Climate preference is cool. Skin sensitivity is insensitive. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n",
            "\n",
            "Rank 3 final=0.8781 base=0.956 attr=1.000 type=face_sensory tb=0.000\n",
            "Matched attributes: {'digestion': ['digest', 'fast', 'weak', 'digestion'], 'appetite': ['hungry', 'weak', 'unbearable', 'appetite'], 'taste_pref': ['pungent', 'bitter', 'astringent'], 'moisture': ['moist'], 'dosha': ['kapha'], 'texture': ['freckles']}\n",
            "The dominant dosha is pitta+kapha. Climate preference is moderate. Skin sensitivity is normal. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n",
            "\n",
            "Rank 4 final=0.8205 base=0.956 attr=0.856 type=ayurvedic tb=0.000\n",
            "Matched attributes: {'digestion': ['digest', 'fast', 'weak', 'digestion'], 'appetite': ['hungry', 'weak', 'unbearable', 'appetite'], 'taste_pref': ['pungent', 'bitter', 'astringent'], 'moisture': ['moist'], 'dosha': ['kapha'], 'texture': ['freckles']}\n",
            "The dominant dosha is vata+pitta. Climate preference is warm. Skin sensitivity is normal. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n",
            "\n",
            "Rank 5 final=0.8205 base=0.956 attr=0.856 type=skin tb=0.000\n",
            "Matched attributes: {'digestion': ['digest', 'fast', 'weak', 'digestion'], 'appetite': ['hungry', 'weak', 'unbearable', 'appetite'], 'taste_pref': ['pungent', 'bitter', 'astringent'], 'moisture': ['moist'], 'dosha': ['kapha'], 'texture': ['freckles']}\n",
            "The dominant dosha is vata+pitta. Climate preference is cool. Skin sensitivity is sensitive. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n",
            "\n",
            "QUERY: I feel pain and cracking in my joints\n",
            "Expanded: I feel pain and cracking in my joints pungent healthy moist kapha glowing beautiful normal tans_easily tans follows_vegetarian vegetarian_diet vegetarian being_moderate tall insensitive pigments aging broad\n",
            "Tokens: ['i', 'feel', 'pain', 'and', 'cracking', 'in', 'my', 'joints', 'pungent', 'healthy', 'moist', 'kapha', 'glowing', 'beautiful', 'normal', 'tans', 'easily', 'follows', 'vegetarian', 'diet', 'being', 'moderate', 'tall', 'insensitive', 'pigments', 'aging', 'broad']\n",
            "\n",
            "Top results:\n",
            "\n",
            "Rank 1 final=0.9781 base=0.956 attr=1.000 type=physical tb=1.000\n",
            "Matched attributes: {'bones_joints': ['pain', 'cracking', 'joints'], 'moisture': ['cracking', 'moist'], 'taste_pref': ['pungent'], 'dosha': ['kapha'], 'texture': ['pigments', 'freckles'], 'skin_issue': ['pigments', 'aging']}\n",
            "The dominant dosha is vata+kapha. Climate preference is cool. Skin sensitivity is insensitive. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n",
            "\n",
            "Rank 2 final=0.8781 base=0.956 attr=1.000 type=lifestyle tb=0.000\n",
            "Matched attributes: {'bones_joints': ['pain', 'cracking', 'joints'], 'moisture': ['cracking', 'moist'], 'taste_pref': ['pungent'], 'dosha': ['kapha'], 'texture': ['pigments', 'freckles'], 'skin_issue': ['pigments', 'aging']}\n",
            "The dominant dosha is vata+kapha. Climate preference is warm. Skin sensitivity is normal. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n",
            "\n",
            "Rank 3 final=0.8781 base=0.956 attr=1.000 type=face_sensory tb=0.000\n",
            "Matched attributes: {'bones_joints': ['pain', 'cracking', 'joints'], 'moisture': ['cracking', 'moist'], 'taste_pref': ['pungent'], 'dosha': ['kapha'], 'texture': ['pigments', 'freckles'], 'skin_issue': ['pigments', 'aging']}\n",
            "The dominant dosha is pitta+kapha. Climate preference is moderate. Skin sensitivity is normal. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n",
            "\n",
            "Rank 4 final=0.8169 base=0.956 attr=0.847 type=skin tb=0.000\n",
            "Matched attributes: {'bones_joints': ['pain', 'cracking', 'joints'], 'moisture': ['cracking', 'moist'], 'taste_pref': ['pungent'], 'dosha': ['kapha'], 'texture': ['pigments', 'freckles'], 'skin_issue': ['pigments', 'aging']}\n",
            "The dominant dosha is vata+pitta. Climate preference is cool. Skin sensitivity is normal. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n",
            "\n",
            "Rank 5 final=0.8169 base=0.956 attr=0.847 type=digestion tb=0.000\n",
            "Matched attributes: {'bones_joints': ['pain', 'cracking', 'joints'], 'moisture': ['cracking', 'moist'], 'taste_pref': ['pungent'], 'dosha': ['kapha'], 'texture': ['pigments', 'freckles'], 'skin_issue': ['pigments', 'aging']}\n",
            "The dominant dosha is vata+pitta. Climate preference is warm. Skin sensitivity is sensitive. This indicates a Prakriti pattern influenced by both elemental balance and environmental preference.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ================== TEST STRUCTURED RETRIEVER (RUN THIS CELL) ==================\n",
        "\n",
        "def show_test(q):\n",
        "    out = structured_retrieve(q, k=6, k_candidates=60, debug=True)\n",
        "    print(\"\\nQUERY:\", q)\n",
        "    print(\"Expanded:\", out[\"expanded\"])\n",
        "    print(\"Tokens:\", out[\"tokens\"])\n",
        "    print(\"\\nTop results:\")\n",
        "    for i, r in enumerate(out[\"results\"], 1):\n",
        "        print(f\"\\nRank {i} final={r['final_score']:.4f} base={r['base_conf']:.3f} attr={r['attr_score']:.3f} type={r['type']} tb={r['type_bonus']:.3f}\")\n",
        "        print(\"Matched attributes:\", r[\"matched_attrs\"])\n",
        "        print(r[\"text\"][:320])\n",
        "\n",
        "# Run all five canonical tests\n",
        "show_test(\"I feel dryness in my skin and joints\")\n",
        "show_test(\"I feel heat in my body and get irritated easily\")\n",
        "show_test(\"I feel heavy and lazy\")\n",
        "show_test(\"I digest food fast and feel hungry often\")\n",
        "show_test(\"I feel pain and cracking in my joints\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckk0JohIW5Kd"
      },
      "outputs": [],
      "source": [
        "\n",
        "#from semantic_graph import SemanticGraphManager\n",
        "from tokenizer_and_embedding import TokenEmbedding\n",
        "from embedding_matching import retrieve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnAGP2BClh7C",
        "outputId": "c9405374-97b3-4bc7-b184-0274ea8cd964"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<module 'DecoderV1' from '/content/DecoderV1.py'>"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import importlib\n",
        "import DecoderV1\n",
        "importlib.reload(DecoderV1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S73pvGH8oxVQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "I have dry skin and joint discomfort, what does it indicate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "A3snqRQUGSt3",
        "outputId": "865e4149-6bbe-4347-d5e6-97fb0ee95d52"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-03 05:18:04,437 - INFO - Initialized TreeEncoderWithAttention dim=48, heads=4 on cpu\n",
            "INFO:TLiteComponents:Initialized TreeEncoderWithAttention dim=48, heads=4 on cpu\n",
            "2025-12-03 05:18:04,440 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 05:18:04,443 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 05:18:04,447 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 05:18:04,452 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 05:18:04,457 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 05:18:04,460 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 05:18:04,465 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 05:18:04,469 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 05:18:04,473 - INFO - Initialized TLiteRouter num_experts=8, top_k=2 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteRouter num_experts=8, top_k=2 on cpu\n",
            "2025-12-03 05:18:04,479 - INFO - Initialized TLiteV6 num_experts=8 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteV6 num_experts=8 on cpu\n",
            "2025-12-03 05:18:04,483 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 05:18:04,486 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 05:18:04,490 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 05:18:04,494 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 05:18:04,498 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 05:18:04,503 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 05:18:04,509 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 05:18:04,514 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 05:18:04,519 - INFO - Initialized TLiteRouter num_experts=8, top_k=2 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteRouter num_experts=8, top_k=2 on cpu\n",
            "2025-12-03 05:18:04,525 - INFO - Initialized TLiteV6 num_experts=8 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteV6 num_experts=8 on cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chaturya ChatbotInterface (local) — type exit to quit.\n",
            "You: exit\n",
            "Bye.\n"
          ]
        }
      ],
      "source": [
        "# %%writefile /content/chaturya_chatbot.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import List, Tuple, Optional, Dict, Any\n",
        "\n",
        "try:\n",
        "    from tokenizer_and_embedding import TokenEmbedding, universal_tokenizer\n",
        "except Exception:\n",
        "    from chaturya import TokenEmbedding, universal_tokenizer\n",
        "\n",
        "try:\n",
        "    from TreeBuilderV2 import TreeBuilderV2\n",
        "    from TreeSnapshot import TreeSnapshot\n",
        "    from embedding_matching import retrieve, build_fragments\n",
        "    from anchor_extractor import extract_anchors\n",
        "    from DecoderV1 import DecoderV1\n",
        "    from TLiteComponents import TLiteV6\n",
        "    from Phase2Env import Phase2Env\n",
        "except Exception:\n",
        "    try:\n",
        "        from chaturya import (\n",
        "            TreeBuilderV2,\n",
        "            TreeSnapshot,\n",
        "            retrieve,\n",
        "            build_fragments,\n",
        "            extract_anchors,\n",
        "            DecoderV1,\n",
        "            TLiteV6,\n",
        "            Phase2Env,\n",
        "        )\n",
        "    except Exception:\n",
        "        raise\n",
        "\n",
        "# structured_retrieve (optional)\n",
        "try:\n",
        "    structured_retrieve  # type: ignore[name-defined]\n",
        "    _HAS_STRUCTURED = True\n",
        "except NameError:\n",
        "    structured_retrieve = None\n",
        "    _HAS_STRUCTURED = False\n",
        "\n",
        "\n",
        "class _Project50to48(nn.Module):\n",
        "    def __init__(self, in_dim=50, out_dim=48, device=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(in_dim, out_dim)\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "            out = self.proj(x).squeeze(0)\n",
        "        else:\n",
        "            out = self.proj(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "def _ensure_tensor_50(v):\n",
        "    if isinstance(v, torch.Tensor):\n",
        "        t = v.detach().cpu().float()\n",
        "    else:\n",
        "        t = torch.tensor(v, dtype=torch.float32)\n",
        "    if t.dim() != 1:\n",
        "        t = t.view(-1)\n",
        "    return t\n",
        "\n",
        "\n",
        "def pairs_to_tree_input(pairs: List[Tuple[str, torch.Tensor]]) -> List[Tuple[str, torch.Tensor]]:\n",
        "    out = []\n",
        "    for token, vec in pairs:\n",
        "        tvec = _ensure_tensor_50(vec)\n",
        "        if tvec.shape[0] != 50:\n",
        "            if tvec.shape[0] < 50:\n",
        "                pad = torch.zeros(50 - tvec.shape[0], dtype=torch.float32)\n",
        "                tvec = torch.cat([tvec, pad], dim=0)\n",
        "            else:\n",
        "                tvec = tvec[:50]\n",
        "        out.append((token, tvec))\n",
        "    return out\n",
        "\n",
        "\n",
        "class LightChatEnv:\n",
        "    def __init__(self):\n",
        "        self.current_tree = None\n",
        "\n",
        "    def load_tree(self, tree):\n",
        "        self.current_tree = tree\n",
        "\n",
        "    def snapshot_from_tree(self, tree):\n",
        "        try:\n",
        "            snap = TreeSnapshot(tree).to_dict()\n",
        "        except Exception:\n",
        "            snap = {\n",
        "                \"depth\": getattr(tree, \"depth\", 1),\n",
        "                \"entropy\": getattr(tree, \"entropy\", 0.5),\n",
        "                \"branching_factor\": getattr(tree, \"branching_factor\", 1),\n",
        "                \"weak_leaves\": getattr(tree, \"weak_leaves\", 0),\n",
        "            }\n",
        "        return snap\n",
        "\n",
        "\n",
        "# lower threshold so clear patterns don't spam follow-ups\n",
        "CLARITY_THRESHOLD = 5  # 0..10\n",
        "\n",
        "\n",
        "class ChatbotInterface:\n",
        "    def __init__(\n",
        "        self,\n",
        "        device: str = \"cpu\",\n",
        "        token_embedder: Optional[TokenEmbedding] = None,\n",
        "        builder: Optional[TreeBuilderV2] = None,\n",
        "        tlite: Optional[TLiteV6] = None,\n",
        "        decoder: Optional[DecoderV1] = None,\n",
        "        env: Optional[Phase2Env] = None,\n",
        "    ):\n",
        "        self.device = device\n",
        "        self.dim50 = 50\n",
        "        self.dim48 = 48\n",
        "\n",
        "        # Follow-up state\n",
        "        self.waiting_for_followup: bool = False\n",
        "        self.pending_query: Optional[str] = None\n",
        "        self.pending_followup_type: Optional[str] = None  # \"sleep\"|\"appetite\"|\"weather\"|None\n",
        "\n",
        "        # Context collected from follow-up answers\n",
        "        self.context: Dict[str, Optional[str]] = {\n",
        "            \"sleep\": None,\n",
        "            \"appetite\": None,\n",
        "            \"weather\": None,\n",
        "        }\n",
        "\n",
        "        self.token_embedder = token_embedder or TokenEmbedding(vocab=[], dim=self.dim50, device=self.device)\n",
        "        self.builder = builder or TreeBuilderV2(mode=\"binary\", dim=self.dim50, device=self.device)\n",
        "        self.decoder = decoder or DecoderV1()\n",
        "        self.env = env or LightChatEnv()\n",
        "\n",
        "        self.tlite = tlite or TLiteV6(dim=self.dim48, device=self.device)\n",
        "        self.proj50to48 = _Project50to48(in_dim=self.dim50, out_dim=self.dim48, device=self.device)\n",
        "\n",
        "        self.tlite.eval()\n",
        "        self.proj50to48.eval()\n",
        "\n",
        "    # ------------------- EMBEDDING / TREE -------------------\n",
        "    def _embed_query_tokens(self, tokens: List[str]) -> List[Tuple[str, torch.Tensor]]:\n",
        "        pairs = []\n",
        "        for t in tokens:\n",
        "            vec = self.token_embedder.lookup(t)\n",
        "            vec50 = _ensure_tensor_50(vec)\n",
        "            if vec50.shape[0] != self.dim50:\n",
        "                if vec50.shape[0] < self.dim50:\n",
        "                    pad = torch.zeros(self.dim50 - vec50.shape[0])\n",
        "                    vec50 = torch.cat([vec50, pad], dim=0)\n",
        "                else:\n",
        "                    vec50 = vec50[:self.dim50]\n",
        "            pairs.append((t, vec50))\n",
        "        return pairs\n",
        "\n",
        "    def _initial_tree_from_query(self, query: str):\n",
        "        toks = universal_tokenizer(query)\n",
        "        if not toks:\n",
        "            return None, []\n",
        "        vec_pairs = self._embed_query_tokens(toks)\n",
        "        vec_pairs = pairs_to_tree_input(vec_pairs)\n",
        "        root = self.builder.build_tree(vec_pairs, sample_id=\"query_T0\")\n",
        "        return root, vec_pairs\n",
        "\n",
        "    # ------------------- RETRIEVAL -------------------\n",
        "    def _anchor_and_retrieve(self, root, combined_query: str, k: int = 6):\n",
        "        try:\n",
        "            anchors = extract_anchors(root)\n",
        "        except Exception:\n",
        "            anchors = []\n",
        "\n",
        "        anchor_terms = []\n",
        "        for a in anchors:\n",
        "            anchor_terms.append(a.get(\"anchor_type\") or a.get(\"token\") or \"\")\n",
        "\n",
        "        final_query = \" \".join([combined_query] + anchor_terms).strip()\n",
        "\n",
        "        if _HAS_STRUCTURED and structured_retrieve is not None:\n",
        "            try:\n",
        "                raw = structured_retrieve(final_query, k=k, k_candidates=40, debug=False)\n",
        "                rets = []\n",
        "                if isinstance(raw, dict):\n",
        "                    raw_list = raw.get(\"results\") or raw.get(\"items\") or []\n",
        "                else:\n",
        "                    raw_list = raw\n",
        "                for item in raw_list:\n",
        "                    if not item:\n",
        "                        continue\n",
        "                    text = (\n",
        "                        item.get(\"text\")\n",
        "                        or item.get(\"fragmenttext\")\n",
        "                        or item.get(\"fragment_text\")\n",
        "                        or item.get(\"fragmenttext\", \"\")\n",
        "                    )\n",
        "                    out = {\"fragmenttext\": text}\n",
        "                    if \"type\" in item:\n",
        "                        out[\"type\"] = item[\"type\"]\n",
        "                    if \"final_score\" in item:\n",
        "                        out[\"final_score\"] = item[\"final_score\"]\n",
        "                    if \"matched_attrs\" in item:\n",
        "                        out[\"matched_attrs\"] = item[\"matched_attrs\"]\n",
        "                    rets.append(out)\n",
        "            except Exception:\n",
        "                try:\n",
        "                    base_rets = retrieve(final_query, k=k)\n",
        "                    rets = [\n",
        "                        {\n",
        "                            \"fragmenttext\": r.get(\"fragmenttext\") or r.get(\"text\") or \"\",\n",
        "                            **({} if not isinstance(r, dict) else r),\n",
        "                        }\n",
        "                        for r in base_rets\n",
        "                    ]\n",
        "                except Exception:\n",
        "                    rets = []\n",
        "        else:\n",
        "            try:\n",
        "                base_rets = retrieve(final_query, k=k)\n",
        "                rets = [\n",
        "                    {\n",
        "                        \"fragmenttext\": r.get(\"fragmenttext\") or r.get(\"text\") or \"\",\n",
        "                        **({} if not isinstance(r, dict) else r),\n",
        "                    }\n",
        "                    for r in base_rets\n",
        "                ]\n",
        "            except Exception:\n",
        "                rets = []\n",
        "\n",
        "        return anchors, rets\n",
        "\n",
        "    def _build_enhanced_tree(self, original_pairs, retrieved_fragments):\n",
        "        combined_pairs = list(original_pairs)\n",
        "        for frag in retrieved_fragments:\n",
        "            text = frag.get(\"fragmenttext\", \"\")\n",
        "            toks = universal_tokenizer(text)[:24]\n",
        "            for t in toks:\n",
        "                vec = self.token_embedder.lookup(t)\n",
        "                vec50 = _ensure_tensor_50(vec)\n",
        "                if vec50.shape[0] != self.dim50:\n",
        "                    if vec50.shape[0] < self.dim50:\n",
        "                        pad = torch.zeros(self.dim50 - vec50.shape[0])\n",
        "                        vec50 = torch.cat([vec50, pad], dim=0)\n",
        "                    else:\n",
        "                        vec50 = vec50[:self.dim50]\n",
        "                combined_pairs.append((t, vec50))\n",
        "\n",
        "        seen = set()\n",
        "        dedup = []\n",
        "        for t, v in combined_pairs:\n",
        "            if t in seen:\n",
        "                continue\n",
        "            seen.add(t)\n",
        "            dedup.append((t, v))\n",
        "        dedup = pairs_to_tree_input(dedup)\n",
        "        new_root = self.builder.build_tree(dedup, sample_id=\"query_T1\")\n",
        "        return new_root\n",
        "\n",
        "    # ------------------- SNAPSHOT + CONFIDENCE -------------------\n",
        "    def _get_snapshot(self, root):\n",
        "        try:\n",
        "            snap = TreeSnapshot(root).to_dict()\n",
        "        except Exception:\n",
        "            snap = {\n",
        "                \"depth\": getattr(root, \"depth\", 1),\n",
        "                \"entropy\": getattr(root, \"entropy\", 0.5),\n",
        "                \"branching_factor\": getattr(root, \"branching_factor\", 1),\n",
        "                \"weak_leaves\": getattr(root, \"weak_leaves\", 0),\n",
        "            }\n",
        "        return snap\n",
        "\n",
        "    def _neural_confidence(self, root):\n",
        "        node_vecs = []\n",
        "\n",
        "        def _walk(n):\n",
        "            if not n:\n",
        "                return\n",
        "            if getattr(n, \"cached_vector\", None) is not None:\n",
        "                node_vecs.append(_ensure_tensor_50(n.cached_vector))\n",
        "            for ch in getattr(n, \"children\", []) or []:\n",
        "                _walk(ch)\n",
        "\n",
        "        try:\n",
        "            _walk(root)\n",
        "        except Exception:\n",
        "            node_vecs = []\n",
        "\n",
        "        if not node_vecs:\n",
        "            pool = torch.randn(self.dim50)\n",
        "        else:\n",
        "            pool = torch.stack(node_vecs, dim=0).mean(dim=0)\n",
        "\n",
        "        pool48 = self.proj50to48(pool.to(self.device))\n",
        "        with torch.no_grad():\n",
        "            score = float(self.tlite(pool48.to(self.device)).squeeze().item())\n",
        "        return max(0.0, min(1.0, score))\n",
        "\n",
        "    # ------------------- FOLLOW-UP LOGIC HELPERS -------------------\n",
        "    def _looks_like_new_symptom(self, toks: List[str]) -> bool:\n",
        "        \"\"\"\n",
        "        A reply is treated as a NEW symptom description only if:\n",
        "        - It is long (>= 8 tokens), AND\n",
        "        - It includes any known symptom words.\n",
        "\n",
        "        Short replies like \"strong appetite\", \"light sleep\", \"yes\", \"no\"\n",
        "        should NOT reset the topic. They are treated as follow-up answers.\n",
        "        \"\"\"\n",
        "        if len(toks) < 8:\n",
        "            return False  # short -> assume follow-up answer\n",
        "\n",
        "        SYMPTOM_WORDS = {\n",
        "            \"skin\", \"dry\", \"dryness\", \"itch\", \"itchy\",\n",
        "            \"joint\", \"joints\", \"pain\",\n",
        "            \"gas\", \"bloating\", \"acidity\", \"burning\", \"fever\", \"heat\",\n",
        "            \"heavy\", \"heaviness\", \"slow\", \"digestion\",\n",
        "            \"constipation\", \"constipated\",\n",
        "            \"anxious\", \"anxiety\", \"sleepy\", \"tired\", \"fatigue\",\n",
        "            \"nausea\", \"vomit\", \"dizzy\", \"weakness\",\n",
        "        }\n",
        "\n",
        "        for t in toks:\n",
        "            if t in SYMPTOM_WORDS:\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _update_context_from_followup(self, reply: str, followup_type: Optional[str]):\n",
        "        \"\"\"Parse short reply like 'light', 'normal', 'yes cold' into context fields.\"\"\"\n",
        "        txt = reply.lower()\n",
        "        toks = txt.split()\n",
        "\n",
        "        if followup_type == \"sleep\":\n",
        "            if any(w in toks for w in [\"light\", \"shallow\", \"broken\", \"disturbed\"]):\n",
        "                self.context[\"sleep\"] = \"light\"\n",
        "            elif any(w in toks for w in [\"deep\", \"sound\", \"good\"]):\n",
        "                self.context[\"sleep\"] = \"deep\"\n",
        "            elif \"normal\" in toks or \"ok\" in toks or \"okay\" in toks:\n",
        "                self.context[\"sleep\"] = \"normal\"\n",
        "\n",
        "        elif followup_type == \"appetite\":\n",
        "            if any(w in toks for w in [\"low\", \"less\", \"poor\", \"weak\"]):\n",
        "                self.context[\"appetite\"] = \"low\"\n",
        "            elif any(w in toks for w in [\"strong\", \"high\", \"sharp\"]):\n",
        "                self.context[\"appetite\"] = \"strong\"\n",
        "            elif \"normal\" in toks or \"ok\" in toks or \"okay\" in toks:\n",
        "                self.context[\"appetite\"] = \"normal\"\n",
        "\n",
        "        elif followup_type == \"weather\":\n",
        "            if \"cold\" in toks or \"winter\" in toks:\n",
        "                self.context[\"weather\"] = \"worse_cold\"\n",
        "            elif \"heat\" in toks or \"hot\" in toks or \"summer\" in toks:\n",
        "                self.context[\"weather\"] = \"worse_heat\"\n",
        "            elif \"damp\" in toks or \"rain\" in toks or \"humid\" in toks or \"humidity\" in toks:\n",
        "                self.context[\"weather\"] = \"worse_damp\"\n",
        "            elif \"no\" in toks or \"none\" in toks:\n",
        "                self.context[\"weather\"] = \"none\"\n",
        "\n",
        "    def _pick_followup_type(self, tokens: List[str]) -> str:\n",
        "        \"\"\"\n",
        "        Decide which follow-up to ask first based on message content.\n",
        "        \"\"\"\n",
        "        tset = set(tokens)\n",
        "        if any(\n",
        "            w in tset\n",
        "            for w in [\n",
        "                \"digestion\",\n",
        "                \"digest\",\n",
        "                \"stomach\",\n",
        "                \"appetite\",\n",
        "                \"hungry\",\n",
        "                \"bloating\",\n",
        "                \"gas\",\n",
        "                \"constipation\",\n",
        "            ]\n",
        "        ):\n",
        "            return \"appetite\"\n",
        "        # could add weather logic later\n",
        "        return \"sleep\"\n",
        "\n",
        "    def _followup_question_text(self, followup_type: str) -> str:\n",
        "        if followup_type == \"sleep\":\n",
        "            return \"How is your sleep—light, disturbed, or deep?\"\n",
        "        if followup_type == \"appetite\":\n",
        "            return \"How is your appetite—strong, normal, or low?\"\n",
        "        if followup_type == \"weather\":\n",
        "            return \"Do your symptoms feel worse in cold, heat, or damp weather?\"\n",
        "        # fallback\n",
        "        return \"How is your sleep—light, disturbed, or deep?\"\n",
        "\n",
        "    def _reset_state(self):\n",
        "        self.waiting_for_followup = False\n",
        "        self.pending_query = None\n",
        "        self.pending_followup_type = None\n",
        "        self.context = {\"sleep\": None, \"appetite\": None, \"weather\": None}\n",
        "\n",
        "    # ------------------- MAIN RESPOND -------------------\n",
        "    def respond(self, query: str, retrieval_k: int = 6, use_anchor_matching: bool = True) -> Dict[str, Any]:\n",
        "        query = (query or \"\").strip()\n",
        "        if not query:\n",
        "            return {\n",
        "                \"answer\": \"I didn't understand that.\",\n",
        "                \"card\": None,\n",
        "                \"need_followup\": False,\n",
        "                \"confidence\": 0.0,\n",
        "                \"mode\": \"empty\",\n",
        "                \"used_retrieval\": None,\n",
        "                \"snapshot\": {},\n",
        "            }\n",
        "\n",
        "        tokens = universal_tokenizer(query)\n",
        "        tok_lower = [t.lower() for t in tokens]\n",
        "\n",
        "        # Decide if this is follow-up answer or fresh symptom description\n",
        "        is_followup_round = False\n",
        "\n",
        "        if self.waiting_for_followup and self.pending_query:\n",
        "            # If it looks like a brand-new symptom description, treat as new topic\n",
        "            if self._looks_like_new_symptom(tok_lower):\n",
        "                # New topic -> reset everything\n",
        "                self._reset_state()\n",
        "                base_query = query\n",
        "                is_followup_round = False\n",
        "            else:\n",
        "                # True follow-up short answer\n",
        "                is_followup_round = True\n",
        "                # update context according to pending_followup_type\n",
        "                self._update_context_from_followup(query, self.pending_followup_type)\n",
        "                base_query = self.pending_query + \". \" + query\n",
        "                # after this turn we will give final summary and reset\n",
        "        else:\n",
        "            # No follow-up pending -> fresh independent message\n",
        "            self._reset_state()\n",
        "            base_query = query\n",
        "            is_followup_round = False\n",
        "\n",
        "        # Build tree\n",
        "        root0, pairs0 = self._initial_tree_from_query(base_query)\n",
        "        if root0 is None:\n",
        "            return {\n",
        "                \"answer\": \"I couldn't understand that description clearly.\",\n",
        "                \"card\": None,\n",
        "                \"need_followup\": False,\n",
        "                \"confidence\": 0.0,\n",
        "                \"mode\": \"empty\",\n",
        "                \"used_retrieval\": None,\n",
        "                \"snapshot\": {},\n",
        "            }\n",
        "\n",
        "        # Retrieval\n",
        "        retrieved = []\n",
        "        if use_anchor_matching:\n",
        "            anchors, retrieved = self._anchor_and_retrieve(root0, base_query, k=retrieval_k)\n",
        "        else:\n",
        "            anchors = []\n",
        "\n",
        "        if retrieved:\n",
        "            root1 = self._build_enhanced_tree(pairs0, retrieved)\n",
        "            mode = \"enhanced\"\n",
        "            used_retrieval = retrieved[0].get(\"fragmenttext\") if isinstance(retrieved, list) and retrieved else None\n",
        "        else:\n",
        "            root1 = root0\n",
        "            mode = \"fallback\" if not retrieved else \"reference\"\n",
        "            used_retrieval = None\n",
        "\n",
        "        # Env + snapshot + confidence\n",
        "        if self.env is not None:\n",
        "            try:\n",
        "                self.env.load_tree(root1)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        snapshot = self._get_snapshot(root1)\n",
        "        conf = self._neural_confidence(root1)\n",
        "\n",
        "        # Decode with context-aware decoder\n",
        "        try:\n",
        "            text, meta = self.decoder._analyze_tree(root1, context=self.context)\n",
        "        except Exception:\n",
        "            try:\n",
        "                t0 = self.decoder.decode_tree(root1)\n",
        "            except Exception:\n",
        "                t0 = \"I built a reasoning tree but couldn't convert it to text.\"\n",
        "            meta = {\n",
        "                \"primary_dosha\": None,\n",
        "                \"secondary_dosha\": None,\n",
        "                \"severity\": \"Unknown\",\n",
        "                \"symptoms\": [],\n",
        "                \"micro_suggestions\": [],\n",
        "                \"followups\": [],\n",
        "                \"clarity_score\": 0,\n",
        "            }\n",
        "            text = t0\n",
        "\n",
        "        clarity = meta.get(\"clarity_score\", 0)\n",
        "\n",
        "        text_clean = \" \".join(str(text).split()).strip()\n",
        "        if not text_clean.endswith(\".\"):\n",
        "            text_clean += \".\"\n",
        "\n",
        "        need_followup = False\n",
        "        card = None\n",
        "\n",
        "        # --------- Decision: follow-up vs final summary ----------\n",
        "        if (not is_followup_round) and (clarity < CLARITY_THRESHOLD):\n",
        "            # FIRST pass + low clarity -> ask ONE follow-up, no card\n",
        "            need_followup = True\n",
        "            self.waiting_for_followup = True\n",
        "            self.pending_query = base_query\n",
        "\n",
        "            ftype = self._pick_followup_type(tok_lower)\n",
        "            self.pending_followup_type = ftype\n",
        "            q = self._followup_question_text(ftype)\n",
        "\n",
        "            answer = (\n",
        "                text_clean.rstrip(\".\")\n",
        "                + \". \"\n",
        "                + \"To understand your pattern better: \"\n",
        "                + q.rstrip(\"?\")\n",
        "                + \"?\"\n",
        "            )\n",
        "            card = None\n",
        "\n",
        "        else:\n",
        "            # Either clarity is good, or this is after follow-up answer.\n",
        "            need_followup = False\n",
        "\n",
        "            # Build summary card\n",
        "            card = {\n",
        "                \"primary_dosha\": meta.get(\"primary_dosha\"),\n",
        "                \"secondary_dosha\": meta.get(\"secondary_dosha\"),\n",
        "                \"severity\": meta.get(\"severity\"),\n",
        "                \"symptoms\": meta.get(\"symptoms\", []),\n",
        "                \"micro_suggestions\": meta.get(\"micro_suggestions\", []),\n",
        "                \"followups\": [],\n",
        "            }\n",
        "            answer = text_clean\n",
        "\n",
        "            # After final card, reset memory for next topic\n",
        "            self._reset_state()\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"card\": card,\n",
        "            \"need_followup\": need_followup,\n",
        "            \"confidence\": float(conf),\n",
        "            \"mode\": mode,\n",
        "            \"used_retrieval\": used_retrieval,\n",
        "            \"snapshot\": snapshot,\n",
        "        }\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Chaturya ChatbotInterface (local) — type exit to quit.\")\n",
        "    inst = ChatbotInterface()\n",
        "    while True:\n",
        "        q = input(\"You: \").strip()\n",
        "        if q.lower() in (\"exit\", \"quit\"):\n",
        "            print(\"Bye.\")\n",
        "            break\n",
        "        res = inst.respond(q)\n",
        "        print(\"Chaturya:\", res[\"answer\"])\n",
        "        print(\"Card:\", res.get(\"card\", {}))\n",
        "        print(\"need_followup:\", res.get(\"need_followup\"))\n",
        "        print(f\" (confidence={res['confidence']:.2f}, mode={res['mode']})\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so3FEMKqkSTl",
        "outputId": "f029f563-8cbb-400c-84b1-0045957ee512"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Collecting flask-cors\n",
            "  Downloading flask_cors-6.0.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask) (8.3.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Downloading flask_cors-6.0.1-py3-none-any.whl (13 kB)\n",
            "Downloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pyngrok, flask-cors\n",
            "Successfully installed flask-cors-6.0.1 pyngrok-7.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install flask flask-cors pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAq0Xbw_nFoD",
        "outputId": "e0a04b92-4a0f-4d8c-cd56-40b382df9115"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-03 06:49:12,256 - INFO - Initialized TreeEncoderWithAttention dim=48, heads=4 on cpu\n",
            "INFO:TLiteComponents:Initialized TreeEncoderWithAttention dim=48, heads=4 on cpu\n",
            "2025-12-03 06:49:12,280 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 06:49:12,290 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 06:49:12,300 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 06:49:12,306 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 06:49:12,312 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 06:49:12,317 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 06:49:12,322 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 06:49:12,325 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 06:49:12,330 - INFO - Initialized TLiteRouter num_experts=8, top_k=2 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteRouter num_experts=8, top_k=2 on cpu\n",
            "2025-12-03 06:49:12,334 - INFO - Initialized TLiteV6 num_experts=8 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteV6 num_experts=8 on cpu\n",
            "2025-12-03 06:49:12,337 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 06:49:12,343 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 06:49:12,349 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 06:49:12,359 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 06:49:12,369 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 06:49:12,378 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 06:49:12,386 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 06:49:12,398 - INFO - Initialized TLiteExpert dim=48 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteExpert dim=48 on cpu\n",
            "2025-12-03 06:49:12,405 - INFO - Initialized TLiteRouter num_experts=8, top_k=2 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteRouter num_experts=8, top_k=2 on cpu\n",
            "2025-12-03 06:49:12,411 - INFO - Initialized TLiteV6 num_experts=8 on cpu\n",
            "INFO:TLiteComponents:Initialized TLiteV6 num_experts=8 on cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Chaturya API URL: https://makenzie-suspensible-unmellifluously.ngrok-free.dev\n",
            "➡ POST https://makenzie-suspensible-unmellifluously.ngrok-free.dev/chat\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [03/Dec/2025 06:52:13] \"OPTIONS /chat HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [03/Dec/2025 06:52:13] \"POST /chat HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [03/Dec/2025 06:52:27] \"OPTIONS /chat HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [03/Dec/2025 06:52:28] \"POST /chat HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Authenticate ngrok\n",
        "!ngrok config add-authtoken 36ETm42WJCIylXKijBJK0WItnXr_cGjBqerC2687yY3gAHaS\n",
        "\n",
        "bot = ChatbotInterface()\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "@app.route(\"/chat\", methods=[\"POST\"])\n",
        "def chat():\n",
        "    data = request.get_json() or {}\n",
        "    user_msg = data.get(\"message\", \"\").strip()\n",
        "\n",
        "    if not user_msg:\n",
        "        return jsonify({\"answer\": \"Please type something.\", \"card\": None, \"need_followup\": False})\n",
        "\n",
        "    try:\n",
        "        res = bot.respond(user_msg)\n",
        "\n",
        "        ans = (res.get(\"answer\") or \"\").strip()\n",
        "        card = res.get(\"card\")\n",
        "        need_followup = bool(res.get(\"need_followup\", False))\n",
        "\n",
        "        ans = ans.replace(\"\\n\", \" \").strip()\n",
        "        ans = \" \".join(ans.split())\n",
        "        if not ans.endswith(\".\"):\n",
        "            ans = ans.rstrip(\".\") + \".\"\n",
        "\n",
        "        return jsonify({\n",
        "            \"answer\": ans,\n",
        "            \"card\": card,\n",
        "            \"need_followup\": need_followup\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\n",
        "            \"answer\": f\"Error: {e}\",\n",
        "            \"card\": None,\n",
        "            \"need_followup\": False\n",
        "        })\n",
        "\n",
        "\n",
        "port = 5000\n",
        "for t in ngrok.get_tunnels():\n",
        "    ngrok.disconnect(t.public_url)\n",
        "\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(\"✅ Chaturya API URL:\", public_url)\n",
        "print(\"➡ POST\", public_url + \"/chat\")\n",
        "\n",
        "app.run(port=port)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVNVqdiPnJWR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "nJMeoV8nxVxU",
        "VDl5kvukjz_z",
        "sxdHOYSbXt33",
        "Lm9FLOywmpMR",
        "qgXUGmBbbTAx",
        "Y60sPq2KTCSN",
        "760mL3YvZfUQ",
        "biiT7vIke7Q5",
        "MMn0CziK8QiT",
        "iiUfgej-pWeT",
        "-IkebNOxKEsQ",
        "vkXE4slRkeHv",
        "RV6P07mTvz3o",
        "dqQnVSBNW97C",
        "kUQF8zOKas_M"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}